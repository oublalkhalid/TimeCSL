\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    linkcolor= crefcolor, %{blue!50!black}, %{green}
    citecolor= crefcolor, %{cyan!60}, %{green}
    urlcolor={magenta!95}, %{crefcolor},%{blue!50!black}%{magenta!95}
}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands}
\usepackage{enumitem}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{wrapfig}
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{rotating}
\usepackage{xspace}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{cite}
\definecolor{cite_color}{HTML}{2267eb} % pure blue: {0,0,128}, light blue:{80,135,208}
\definecolor{magenta_color}{HTML}{ff617b}
\usepackage{tabularx}

\usepackage{algorithm,algorithmic} 

% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}
% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{aliascnt}
\usepackage{thm-restate}
\usepackage{bbm}

\definecolor{darkblue}{rgb}{0.0, 0.0, 0.7}

% if you use cleveref..
\usepackage[capitalize,noabbrev,nameinlink]{cleveref}
%\usepackage{natbib}
%%%%
%TOC
\usepackage{minitoc}
\setcounter{parttocdepth}{3}
\setcounter{secnumdepth}{3}
% Make the "Part I" text invisible
\renewcommand \thepart{}
\renewcommand \partname{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}


\def\vphi{{\bm{\phi}}}



\newaliascnt{model}{equation}
\aliascntresetthe{model}
\creflabelformat{model}{#2\textup{(#1)}#3}
\makeatletter
\def\model{$$\refstepcounter{model}}
\def\endmodel{\eqno \hbox{\@eqnnum}$$\@ignoretrue}
\makeatother
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\definecolor{ourmethod}{gray}{0.93}
\definecolor{priormethod}{gray}{1} %0.93}
\newcommand{\bcmark}{\ding{51}}%
\newcommand{\bxmark}{\ding{55}}%
% Checkmarks and crosses.
\definecolor{myredcolor}{RGB}{215,48,39}
\definecolor{mygreencolor}{RGB}{26,152,80}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\textcolor{blue}{\ding{51}}}
\newcommand{\xmark}{\textcolor{black}{\ding{55}}}    
\newcommand{\mygreen}[1]{\textcolor{mygreencolor}{#1}}
\newcommand{\myred}[1]{\textcolor{myredcolor}{#1}}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\newcommand\FBox[1]{{\setlength{\fboxsep}{-0.5pt}\setlength{\fboxrule}{0.5pt}\frame{#1}}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{minitoc}
\usepackage{soul}
%\usepackage{tikz}
% Make the "Part I" text invisible
\renewcommand \thepart{}
\renewcommand \partname{}
% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}

% Define the \reviewed command
\newcommand{\rebutall}[1]{\textcolor{red}{#1}}
\newcommand{\david}[1]{\textcolor{red}{"david" #1}}
% to compile a camera-ready version, add the [final] option, e.g.


\title{Identifiability Guarantees In Time Series Representation via Contrastive Sparsity-inducing}

%\title{Provable Identifiability and Generalization Guarantees For Time Series Representation via Sparsity-inducing Contrastive}

%\title{Identifiability Guarantees For Time Series Representation via Contrastive Sparsity-inducing}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumitem}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{problem}{Challenge}[section]
\newtheorem{example}{Example}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\usepackage{thm-restate}
\usepackage{url}
\usepackage{multirow}
\usepackage{appendix}
\definecolor{crefcolor}{RGB}{0, 92, 255}
\newcommand{\first}{\bf \cellcolor{gray!25}}
\newcommand{\second}{\cellcolor{gray!10}}

\usepackage{amsthm}
\usepackage{tcolorbox}
\usepackage{bm}
\usepackage{amsthm}
\tcolorboxenvironment{theorem}{
  colback=white!10!white,
  colframe=white,
  boxrule=0pt,
  boxsep=1pt,
  left=2pt,right=2pt,top=2pt,bottom=2pt,
  oversize=2pt,
  sharp corners,
  before skip=\topsep,
  after skip=\topsep,
}
\tcolorboxenvironment{definition}{
  colback=white!10!white,
  colframe=white,
  boxrule=0pt,
  boxsep=0pt,
  left=2pt,
  right=2pt,
  top=2pt,
  bottom=2pt,
  oversize=2pt,
  sharp corners,
  before skip=\topsep,
  after skip=\topsep,
}
%\newtheorem{Constraint}{Constraint}[section]
%\newtheorem{assumption}{Assumption}[section]
\tcolorboxenvironment{assumption}{
  colback=white!10!white,
  colframe=white,
  boxrule=2pt,
  boxsep=0pt,
  left=2pt,
  right=2pt,
  top=2pt,
  bottom=2pt,
  oversize=2pt,
  sharp corners,
  before skip=\topsep,
  after skip=\topsep,
}
\tcolorboxenvironment{example}{
  colback=white!10!white,
  colframe=white,
  boxrule=2pt,
  boxsep=0pt,
  left=2pt,
  right=2pt,
  top=2pt,
  bottom=2pt,
  oversize=2pt,
  sharp corners,
  before skip=\topsep,
  after skip=\topsep,
}

\tcolorboxenvironment{lemma}{
  colback=white!10!white,
  colframe=white,
  boxrule=2pt,
  boxsep=0pt,
  left=2pt,
  right=2pt,
  top=2pt,
  bottom=2pt,
  oversize=2pt,
  sharp corners,
  before skip=\topsep,
  after skip=\topsep,
}

\tcolorboxenvironment{proposition}{
  colback=white!10!white,
  colframe=white,
  boxrule=2pt,
  boxsep=0pt,
  left=2pt,
  right=2pt,
  top=2pt,
  bottom=2pt,
  oversize=2pt,
  sharp corners,
  before skip=\topsep,
  after skip=\topsep,
}

\tcolorboxenvironment{restatable}{
  colback=white!10!white,
  colframe=white,
  boxrule=0pt,
  boxsep=0pt,
  left=2pt,
  right=0pt,
  top=0pt,
  bottom=0pt,
  %oversize=2pt,
  %sharp corners,
  %before skip=\topsep,
  %after skip=\topsep,
}

%% TEXT FUNCTIONS
\crefname{section}{\S}{\S\S}
\crefname{subsection}{\S}{\S\S}
\crefname{subsubsection}{\S}{\S\S}
\crefname{algorithm}{Algorithm}{Algorithms}
\creflabelformat{pb_multiline}{#2{\upshape(#1)}#3}
\crefname{figure}{Fig.}{Figs.}
\crefname{table}{Tab.}{Tabs.}
\crefname{proposition}{Prop.}{Props.}
\crefname{definition}{Def.}{Defs.}
\crefname{equation}{Eq.}{Eqs.}
\crefname{Equation}{Eq.}{Eqs.}
\crefname{corollary}{Cor.}{Cors.} 
\crefname{proposition}{Proposition}{Propositions}
\crefname{theorem}{Thm.}{Thms.}
\crefname{appendix}{App.}{Apps.}
\crefname{remark}{Remark}{Remarks}
\crefname{principle}{Principle}{Principles}
\crefname{example}{Ex.}{Ex.}
\crefname{lemma}{Lemma}{Lemmas}
\crefname{claim}{Claim}{Claims}
\crefname{assumption}{Asm}{Asms}
\numberwithin{equation}{section}

\begin{document}
\doparttoc % Tell to minitoc to generate a toc for the parts
\faketableofcontents

\maketitle

%producing undefined representations and, consequently unstable model and  weaker predictions.

\begin{abstract}
Time series representations learned from high-dimensional data, sometimes called ”disentanglement” are generally expected to be more robust and better at generalizing to new and potentially out-of-distribution (OOD) scenarios. Yet, this is not always the case, as variations in unseen data or prior assumptions may insufficiently constrain the posterior probability distribution, leading to an unstable model and non-disentangled representations, which in turn less generalization and prediction accuracy. While identifiability and disentangled representations for time series are often said to be beneficial for generalizing downstream tasks, the current empirical and theoretical understanding remains limited. In this work, we provide results on identifiability that guarantee complete disentangled representations via Contrastive Sparsity-inducing Learning, which improves generalization and interpretability. Motivated by this result, we propose the~\TimeCSL~framework to learn a disentangled representation that generalizes and maintains compositionality. We conduct a large-scale study on time series source separation, investigating whether sufficiently disentangled representations enhance the ability to generalize to OOD downstream tasks. Our results show that sufficient identifiability in time series representations leads to improved performance under shifted distributions. Our code is available at \url{https://anonymous.4open.science/r/TimeCSL-4320}.

% %result in an unstable model and indeterminate representation (not disentangled), ultimately, a less generalization and a far less accurate prediction.

%We conduct a large-scale study on time series source separation and we investigate whether a sufficient disentangled representations are more suitable for downstream tasks generalization-the ability to generalize to seen during training, and the ability to generalize to source combinations not seen during training, we observe that sufficient identifiability for time series representations do in fact lead to better downstream performance under shifted distribution. 

\end{abstract}



\section{Introduction}\label{sec:introduction}
\begin{wrapfigure}{r}{0.40\textwidth}
    \centering
    \vspace{-0.5cm}
    \includegraphics[clip, trim=0.0cm 0.5cm .0cm .0cm, width=0.40\textwidth]{figures/cover_space.pdf}
    \vspace{-0.8cm}
    \caption{Recovered 5 slots latents for 4 runs of \TimeCSL on UKDALE dataset.}
    \label{fig:identifiability_presentations}
    \vspace{-0.31cm}
\end{wrapfigure}
Time series representation learning has been proposed as a solution to the lack of robustness, transferability, systematic generalization, and interpretability of current downstream task methods. However, the problem of learning meaningful representation for time series, is still open. This problem is strongly related to learning \emph{disentangled} representations pointed by \citet{bengio2013representation}. Informally, a representation is considered \emph{disentangled} when its components are in \emph{one-to-one} correspondence with natural and interpretable factors of variations. However, a large body of work have investigated theoretically under which conditions disentanglement is possible through the lens of identifiability originated in works on non-linear independent analysis (ICA)~\citep{comon1994independent, hyvarinen2017nonlinear, hyvarinen2019nonlinear, khemakhem2020variational}, which aims to recover independent latent factors from mixed observations. It has been found in \citep{locatello2019challenging, van2008visualizing, dittadi2021sim2real_dis, montero2021disGen, lachapelle2022disentanglement} that without exploiting an appropriate class of assumptions in
estimation, the latent variables are not identifiable in the most general case. \textcolor{black}{Existing methods like Generalized Contrastive Learning (GCL) via an auxiliary variable~\citep{hyvarinen2019nonlinear},  HM-NLICA~\citep{halva2020hidden}, PCL~\citep{PCL17}, and SlowVAE~\citep{klindt2020towards} rely on the assumption of mutually independent sources in the data generation process. However, this assumption breaks down for time-lagged or dependent latent variables, distorting identifiability. SlowVAE assumes linear relationships, while TDRL optimizes mutual information between input and latent factors, penalizing static-dynamic interactions, and assumes only time-lagged influences. This requires matching the temporal resolution of observations and latent variables~\citep{yao2022temporally}. A more flexible framework is needed to deal with real-world time series (\eg energy separation), where sources are often dependent, may be correlated in a general nonstationary environments with time-varying relations.}
%\textcolor{red}{Existing methods, like PCL [7], GCL [8], HM-NLICA [11], and SlowVAE [12] , enforce functional and distributional assumptions, such as mutually-independent sources in the data generating process. However, this assumption may severely distort the identifiability if the latent variables have time-delayed. SlowVAE assume linear relations; The TDRL use mutual information between the input and the latent factors, while penalizes the mutual information between the static and dynamic factors;  that assumed to have no  instantaneous dependency but only timedelayed influences are require, this possible only if the time resolution of the observed time series is the same as the time-varying latent~\citep{yao2022temporally}.}
%In fact, many real applications, particularly those involving source separation,  
Prior work on sparsity through convex optimization with sparsity-inducing norms~\citep{bach2011convex} and recent findings in disentanglement using sparse task predictors~\citep{lachapelle2023synergies, lachapelle2022disentanglement} show impressive results empirically. A key question is whether these sparsity can guarantee identifiability, and resulting in disentangled representations that capture meaningful features and remain stable under distribution shifts?  Indeed, without identifiability, the representation of a model can be unstable and not consistent~\citep{locatello2019challenging,lenc2015understanding}, in the sense that retraining the same model under small perturbations of the data or hyperparameters may result in wildly different representations.
More formally, identifiability means that the parametrization of the model is injective~\citep{roeder2020linear, khemakhem2020variational}. %

%The identifiable model we analyze are closely related to deep architectures that have been widely used in practice \citep{li_generative_2023, yao2022temporally, hyvarinen2017nonlinear}.
\begin{figure}
    \centering
    \vspace{-0.5cm}
    \includegraphics[clip, trim=0cm 0.0cm 0.0cm 0.0cm, width=0.99\textwidth]{figures/overview_2.pdf}
    \vspace{-0.2cm}
    \caption{\textbf{Multi-view motivating setting for the \emph{energy time series representation}.} \textbf{Left:} We consider $\{\textcolor{magenta!50!blue}{\bf 1},\textcolor{orange}{\bf 2}, \textcolor{black}{\bf 3}, \textcolor{green!60!black}{\bf 4}\}$ sources and $\{\textcolor{gray}{\bf 5}\}$ representing measurement \textcolor{gray}{\textit{\bf noise}} or other irrelevant sources. The mixed observation at different time are: $\rvx$ includes $\{\textcolor{magenta!50!blue}{\bf 1},\textcolor{orange}{\bf 2}, \textcolor{green!60!black}{\bf 4}, \textcolor{gray}{\bf 5}\}$, and $\prvx$ includes $\{\textcolor{orange}{\bf 2}, \textcolor{black}{\bf 3}, \textcolor{green!60!black}{\bf 4}, \textcolor{gray}{\bf 5}\}$. \textbf{Center:} Training distribution combinations. \textbf{Right:} compositional consistency for OOD based recombining inferred latent slots $(\hrvz, \hprvz)$ allows for generalization, thus improving downstream tasks. \label{fig:overview}}
    %\caption{Multi-view motivating example for the \emph{energy time series representation} setting. We consider $\rvx$ and $\prvx$ at different time. In $\rvx$ only sources $\{\bf \textcolor{red}{1}, \textcolor{blue}{2}, \textcolor{gray}{4}, \textcolor{orange}{5}\}$ are contributing, while in $\prvx$ we have $\{\bf \textcolor{blue}{2}, \textcolor{green!60!black}{3}, \textcolor{gray}{4}, \textcolor{orange}{5}\}$. Left, seen combinations under training distribution, and right the compositional consistency generalization for OOD. Recombining slots of the inferred latent $(\rvz, \prvz)$, and the sources are not observed, but only through $\rvx$. When the model is identifiable and generalized the combination of slots latent produces unseen views, thereby improving downstream task performance. \label{fig:overview}}
    \vspace{-0.6cm}
\end{figure}
In this work, we establish that achieving Identifiability for time series representation up to affine transformations—essentially, disentanglement—is possible for time series through Contrastive Sparsity-inducing Learning (\TimeCSL) (see \Cref{fig:identifiability_presentations}, across 4 runs, the latents are recovered, providing evidence of the latent space recovery up to the affine transformations). Importantly, this can be achieved with commonly adopted weaker assumptions. Specifically, we allow for statistically dependent latent factors, with empirical evidence indicating that relaxing independence improves OOD generalization~\citep{roth_disentanglement_2023, oublal2024disentangling}. Moreover, it doesn't require complete information on auxiliary variables.  Two key strengths stand out: first, it accommodates nonlinear task-specific predictors and unknown latent relationships, expanding its applicability to time series. Second, \TimeCSL reduces reliance on fully labeled data via contrastive learning, offering greater flexibility across time series datasets. Our contributions include:

\vspace{-0.21cm}
%At the heart of our contributions is the assumption that only a small subset of all factors of variations are useful for each downstream task, and this subset might change from one task to another
% This contrasts a recent line of work that has established fundamental new results regarding
% the identifiability of model that requires conditioning on an compelet auxiliary variable that renders each latent
% dimension conditionally independent ~\citep{iVAEkhemakhem20a, pmlr-v119-locatello20a}.  For instance, to effectively leverage nonstationary time series for estimating independent components, \citet{TCL2016} pointed that Time Contrastive Learning (TCL)~necessit indices corresponding to different latent data generative states. However, since these states are typically unobserved in practice, the default approach involves manual data segmentation. Additionally, the method considers independence components.

%Informally, a representation is considered \emph{disentangled} when its components are in one-to-one correspondence with natural and interpretable factors of variations, such as object positions, colors or shapes.
% Another line of work has argued for the integration of ideas from causality to make progress towards more robust and transferable machine learning systems~\citep{Pearl2018TheSP,scholkopf2019causality,InducGoyal2021}. \textit{Causal representation learning} has emerged recently as a field aiming to define and learn representations suited for causal reasoning~\citep{scholkopf2021causal}. This set of ideas is strongly related to learning \emph{\emph{disentangled} representations}~\citep{bengio2013representation}. 
% \begin{example}[\textbf{Insufficient Data}]
% Consider a time series $\rvx \in \mathcal{X}$, where a model $h$ learns representations $h(\rvx)$. Non-identifiability occurs when small changes occur $\rvx'$ not likely observed in $\mathcal{X}$ then $h$ has not fully capture, even when $\rvx$ and $\rvx'$ share similar patterns. This compromises predictive performance, especially in tasks like source separation and forecasting.
% \end{example}
% \begin{example}[\textbf{Non-Identifiability due to insufficient data}]\label{exp_1}
% Let $\rvx \in \mathcal{X}$ be a time series and $\rvh$ be a model learning representations $\rvh(\rvx)$. Non-identifiability arises when model fails to fully capture small changes (\ie still sharing similar patterns) that have not been observed or are less likely in $\mathcal{X}$. This limit the predictor performance, particularly in tasks such as source separation and forecasting.
% \end{example}
% \begin{example}[\textbf{Non-Identifiability due to Prior Assumptions}]\label{exp_2}
% Let $\rvx = \sum_{m}y_m + \epsilon$, where a model predict sources $\hat{y}_{m}$ from $\rvx$. However, despite training on different sources $y_m$, the model may struggle to learn all necessary combinations of ${y}_{m}$ for accurate reconstruction. This arises from prior independence assumptions, which may not fully capture the complexity of the data distribution.
% \end{example}


% \begin{problem}[Identifiability and Disentanglement]\label{pb:Identifiability}
%     Identifying the ground-truth latent variables on the slot-supported subset $\gZ$ presents a significant challenge. Identifiability is notoriously difficult, often requiring specific assumptions on the generative model. Previous work has relied on statistical independence assumptions on the latent distribution $p({\rvz})$. However, in our scenario, $p({\rvz})$ is only supported on to $\gZ$, which might introduce dependencies among individual latent variables. However, the latent variables within slots are permutationally identifiable, meaning they are \emph{disentangled} from one another. Thus, it's more natural to place constraints on the generator $\gtheta$ to address this challenge.
% \end{problem}

% \begin{problem}[Generalization in Out-of-Distribution.]\label{pb:Generalization_uncommon_correlation}
% Even if an autoencoder can identify the ground-truth latents within the distribution, generalizing to out-of-distribution cases remains a hurdle. Existing research has shown that simply identifying latents within the distribution does not guarantee successful generalization. Theoretical considerations suggest that out-of-distribution generalization implies the behavior of the generator on the full latent space is solely determined by its behavior on $\gZ$, which may not hold for arbitrary functions. Thus, constraining the function class of $\gtheta$ becomes crucial for ensuring generalization.
% \end{problem}



%\subsection{Contributions}
%\setlist{nolistsep}
\begin{enumerate}[label={[{\color{gray!100}\arabic*}]}, leftmargin=*]
    \item We rely on the sparsity assumption of time series representation, and provide theoretical insight and empirical arguments on how, and under which condition, identifiability up to affine transformation  is preserved. We show that \TimeCSL outperform an affine transformation \eg~permutation and element-wise transformation.
    \item Unlike many existing identifiability results, we allow for arbitrary dependencies without parametric assumptions, achieving slot latent disentanglement through \emph{Partial Selective Pairing}. This approach is particularly suitable for time series, where obtaining fully labeled data is challenging.
    \item Building on this result, we propose generalization consistency for uncommon OOD correlations as in \Cref{fig:overview}. We validate it by showing that \TimeCSL effectively disentangles latent slots in real-world source separation tasks (\eg energy disaggregation). Notably, existing architectures (\eg~D3VAE, RNN-VAE) improve by {\bf +11\%} \RMSE in downstream tasks with disentangled representations. We also release over 221 trained models as baselines for future research\footnote{Pretrained models and usage guidelines: \url{https://anonymous.4open.science/r/TimeCSL-4320}}.
    % \item Building on this result, we propose a generalization consistency to uncommon correlations OOD. We validate our insights by showing that \TimeCSL effectively help disentangling latent slots in real-world time series separation source (\ie energy disaggregation datasets). Interestingly, we find that existing models architectures (\eg~D3VAE, RNN-VAE,..etc) can be enhanced in average by {\bf +11\%} \RMSE downstream task if the representations is \emph{disentangled}. We also release more than 221 trained models which can be used as baselines for future research\footnote{All pretrained models are accessible along with guidelines on their utilization at: \url{https://anonymous.4open.science/r/TimeCSL-4320}}. 
    \vspace{-0.3cm}
    % \item A large scale experiment, and dataset for time series disentanglement with variants factors labels are made open source. As reproducing our results requires substantial computational effort, we also release more than 221 trained models which can be used as baselines for future research\footnote{All pretrained models are accessible along with comprehensive guidelines on their utilization at: \url{https://anonymous.4open.science/r/TimeCSL-4320}}
    %We release disentanglement_lib2, a new library to train and evaluate disentangled representations. As reproducing our results requires substantial computational effort, we also release more than 10 000 trained models which can be used as baselines for future research
    % \item Finally, we establish a connection between this bi-level optimization problem and formulations from the meta-learning literature.
\end{enumerate}

\paragraph{Notation} Vectors and vector-valued functions are denoted by bold letters. Vectors with factorized dimensionality, such as the latent variable $\rvz \in \mathbb{R}^{\dimz}$, where the latent space $\mathcal{Z}$ has dimension $\dimz = d \times n$, or functions with factorized outputs, like the encoder $\fphi\colon \xfancy \to \mathbb{R}^{2\dimz}$, where $\fphi(\rvx) = \begin{bmatrix} \muphi{\rvx}, \sigmaphi{\rvx} \end{bmatrix}^\top$, are used in this context. We refer to $(\fphi, \gthetaseul)$ as the ground truth encoder-decoder, and $(\fphihat, \gthetaseulhat)$ as the learned encoder-decoder, and $\hrvz:=\{\hvz_1, \dots, \hvz_n\}$ is the learned representation of $\rvz:=\{\vz_1, \dots, \vz_n\}$. When indexing with $k $, we refer to the $k $-th contiguous sub-vector, such as the learned slot latent $\hvz_k := \hmuphik{\rvx} + \hsigmaphik{\rvx} \odot \boldsymbol{\epsilon} $, where $\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})$, and both $\hmuphik{\rvx}$, $\hsigmaphik{\rvx} \in \mathbb{R}^{d}$. Additionally, for a positive integer $n $, we denote the set $\{1, \ldots, n\} $ as $[n] $. 

%%Finally, let $\hvz_k $ represent the learnable  corresponding to the ground truth slot latent $\vz_k $.

% \paragraph{Notation} Vectors or vector-valued functions are denoted by bold letters. For vectors with factorized dimensionality (\eg, $\rvz$ usually from $\sR^{d \times n}$) or functions with factorized output (usually $\fphihat$ mapping to $\real^{2\dimz}$, 

% where 
% $\fphihat(\rvx) = \begin{bmatrix} \hmuphi{\rvx}, \hsigmaphi{\rvx} \end{bmatrix}^\top$) and $\dimz$ represents the total dimensionality, we indexing with $k$ denotes the $k$-th contiguous sub-vector (\ie $\hvz_{k} := \hmuphik{\rvx} + \hsigmaphik{\rvx} \odot \boldsymbol{\epsilon}$ where $\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})$, and $\hmuphik{\rvx} \in \sR^d$). Additionally, for a positive integer $n $, we denote the set $\{1, \ldots, n\}$ as $[n]$. Furthermore, let $\hvz_k$ represent the learnable variable of the ground truth $\vz_k$.

%\textbf{Paper Organization.} Sec. \Cref{sec: Problem set up} covers background and preliminaries of identifiability in time series. Sec. \Cref{sec:proposed_method} provides theoretical guarantees for element-wise identifiability using contrastive sparsity-inducing methods. Sec. \Cref{sec:experiments} offers real-world evaluations and validates improvements in downstream tasks with \emph{disentangled} representations.\par

% \textbf{Notation.} For clarity, we denote the set of integers from $1$ to $n$ as $[n]$.
% The Euclidean norm for vectors and the Frobenius norm for matrices are represented by $\norm{\cdot}$.
% %For a matrix $\rvz \in \mathbb{R}^{k \times m}$, $\norm{\rvz}_{2, 1}$ sums the norms of its columns, while $\norm{\rvz}_{2, 0}$ counts the number of non-zero columns using the indicator function $\mathbbm{1}$.
% The ground-truth auto-encoder function is referred to as $(\fphi, \gtheta)$, and the learned auto-encoder is denoted by $(\hatfphi, \gthetaseulhat)$. 
% %This notation is consistently applied throughout the paper. \Cref{tab:TableOfNotation} in \Cref{app:proofs} overview of all notation used.


\section{Background and Preliminaries}\label{sec: Problem set up}

We formalize our setting for time series representation learning, in which we have a set of high-dimensional time series observations $\rvx$ as $C$-variate time series observed at times
$t=1,\dots, T$. We denote by
$\rvx \in \mathbb{R}^{C \times T}$ resulting matrix with rows denoted by $\vx_1,\dots,\vx_C$. Each row can be seen as a univariate time series in $\sR^{T}$. Without loss of generality, we consider the case where $C = 1$. In the source separation problem, the observed signal $\rvx \in \mathcal{X}$ is assumed to be a mixture of $n$ sources, denoted as $\mathbf{y} := \{\vy_1, \dots, \vy_n\} \in \mathcal{Y}$, where each $\vy_k \in \mathbb{R}^{T}$, with additive independent noise $\xi \in \mathbb{R}^{T}$: $\rvx = \sum_{k=1}^{n} \vy_k + \xi$. The space $\mathcal{Y}$ representing the individual source signals, satisfies $\mathcal{Y} \subseteq \mathcal{X}$~\footnote{When $\rvx$ is sparse, it may equal a single source $\vy_1$, so $\mathcal{Y} \subseteq \mathcal{X}$.}. Given a data set of $N$ samples,  denoted as$\{\rvx_i, \rvy_i\}_{i=1}^{N}$, the goal is to recover $\rvy$ from $\rvx$. Although the observed signal is a sum of sources, the mixing process is inherently \textit{non-linear} due to interactions from multi-state appliances, power distortions, and continuously fluctuating power in NILM~\citep{yue_bert4nilm_2020}, similar to harmonic distortions and reverberations in audio~\citep{lu2021nonlinear}.


%Although the observed signal is expressed as a sum of sources, the mixing process is inherently \textit{non-linear}. This \textit{non-linearity} arises from complex interactions between the sources, such as harmonic distortions and power fluctuations in energy separation source or NILM~\citep{yue_bert4nilm_2020}, or reverberation and distortion in audio~\citep{lu2021nonlinear}.


%In the source separation problem, the observed signal $\rvx$ is assumed to be a mixture of $n$ sources $\rvy := \{\vy_1, \dots, \vy_n\}$, where $\vy_k \in \sR^{T}$, with additive independent noise $\xi \in \sR^{T}$. Thus, the mixing observation $\rvx$ is given by $\sum_{k=1}^{n} \vy_k + \xi$.  

%In the source separation problem, the observed signal $\rvx \in \mathcal{X}$ is assumed to be a mixture of $n$ sources $\rvy := \{\vy_1, \dots, \vy_n\}$, where $\vy_k \in \mathbb{R}^{T}$, with additive independent noise $\xi \in \sR^{T}$. The set $\mathcal{X}$ spans a space that includes the possible configurations of the sources in $\mathcal{Y}$, but transformed non-linearly into the aggregate signal. Although the observed signal is expressed as a sum of sources, the mixing process is inherently \textit{non-linear}. This \textit{non-linearity} arises from complex interactions between the sources, such as harmonic distortions and power fluctuations in energy separation source or NILM~\citep{yue_bert4nilm_2020}, or reverberation and distortion in audio~\citep{lu2021nonlinear}.\par

To formalize this idea, we consider a Euclidean observation space $\xfancy$, and denote by $\mprob{\xfancy}$ the set of probability measures on $\xfancy$. %The latent space is represented as $\zfancy = \mathbb{R}^{\dimz}$
The standard framework for learning representations typically relies on VAEs \citep{Kingma2014}, which consist of two main components: \textcolor{blue}{i)} the encoder network with parameters $\phi$, and \textcolor{blue}{ii)} the decoder network with parameters $\theta$. The encoder parameterized a distribution $\qphi{\zgivenx}$ over the latent space $\zfancy=\mathbb{R}^{\dimz}$, with $\dimz=d \times n$ representing the dimensionality, serves as a variational approximation of the Bayesian posterior $\ptheta{\zgivenx}$. The likelihood $\ptheta{\xgivenz}$ is parameterized by the decoder network. In standard setup, we assume a standard Gaussian prior $p(\rvz)=\normal{\zeromatrix, \idmatrix}$ on $\zfancy$ and Gaussian distributions $\qphi{\zgivenx}$. More precisely, for any $\rvx \in \xfancy$, the distribution $\qphi{\zgivenx}$ is a Gaussian distribution with a diagonal covariance matrix $\normal{\muphi{\rvx}, \diag{\sigmaphisq{\rvx}}}$, where $\muphiseul: \xfancy \rightarrow \zfancy$ and $\sigmaphiseul : \xfancy \rightarrow \zfancy_{\geq 0}$. %Note that $\diag{\vsigma}$ denotes the diagonal matrix whose main diagonal is the vector $\vsigma$. 
In order to simplify some of the expressions below, it may be useful to express the encoder network as a function 
$\encfuncseul : \xfancy \rightarrow \real^{2\dimz}, \quad \text{where } 
\encfunc{\rvx} = \begin{bmatrix}
\muphi{\rvx}, \sigmaphi{\rvx}\end{bmatrix}^\top$ and the decoder is a compositional function $\gthetaseul: \sR^{\dimz} \to \sR^{T \times n}$, defined as $\gthetaseul(\rvz) = \sum_{k=1}^{n}\gthetaseulk(\rvz)$, where each $\gthetaseulk: \sR^{\dimz} \to \sR^{T \times 1}$, mainly, $\vy_k = \gthetaseulk(\rvz)$. 
%For any $\rvx \in \gX$, upon receiving $\rvz \sampled \qphi{\zgivenx}$, the decoder $\gthetaseul$ is a compositional of functions $g_{\theta}(z)$ that maps the latent vector $\rvz \in \sR^{d\times n}$ to an output $\ry_k \in \sR^{T}$. 
The encoder and decoder networks are jointly trained on data set of $N$ samples by minimizing the following objective: 
\begin{equation}\label{eq-loss-vae}
\lfancy_\text{VAE}(\phi,\theta) = \unsur{N} \somme{i=1}{N}{  \left[   \expect{\rvz \sampled \qphi{\zgivenx_i}}{\log \ptheta{\rvx_i| \rvz}} - \beta \kl{\qphi{\zgivenx_i}}{p(\rvz)}   \right]  },
\end{equation}
where the first part of \Cref{eq-loss-vae} is the reconstruction loss and the second part is the KL-divergence between the latent distributions (associated to the training samples) and the prior over the latent space, weighted by a hyper-parameter $\beta > 0$ \citep{higgins_beta-vae_2016}. 
The reconstruction loss measures the similarity between the true source measurements $\rvy = \{\vy_1, \dots, \vy_n\}$ and its reconstruction given by a multi-output decoder $\gthetaseul(\rvz)=:\{\gthetaseulindice{1}(\rvz),\dots, \gthetaseulindice{n}(\rvz)\}$, and can be defined in many ways.  With a Gaussian likelihood, the reconstruction loss is the squared $L_{2}$ norm: $\norme{\sum_{k=1}^{n} \left(\vy_k - \gthetaseulk(\rvz)\right)}^2$, or in an unsupervised fashion, when the label source $\rvy$, \ie when the label source $\rvy$ is absent, the reconstruction loss becomes $\norme{\rvx - \gthetaseul(\rvz)}^2$. After training, the VAE defines a generative model using the prior $p(\rvz)$ and the decoder $\gthetaseul$. The VAE's generated distribution denote by $\pushf\gthetaseul p(\rvz) \in \mprob{\xfancy}$ allows one to generate new samples by first sampling a latent vector from the prior, then passing it through the decoder. We further assume the following:
% \begin{assumption}
% \label{assm:mixture}
% The density of the latent variable $\rvz $ follows a (possibly degenerate) Gaussian mixture model with an unknown number of components $K_G \ge 1 $, given by:
% \begin{equation}
% p(\rvz) = \sum_{j=1}^{K_G} \Pi_j \psi(\rvz; \mu_j, \Sigma_j),
% \quad \sum_{j=1}^{K_G} \Pi_j = 1,
% \quad \Pi_j > 0,
% \end{equation}
% where $p(\rvz) $ represents the density of $\rvz $ with respect to some base measure, and $\psi(\rvz; \mu_j, \Sigma_j) $ is the Gaussian density function with mean $\mu_j $ and covariance $\Sigma_j $. with $K_{G} =1$ for an isotropic Gaussian prior (\eg Vanilla VAE \citep{Kingma2014}).
% \end{assumption}
\begin{assumption}\label{assump:injective_piecewise}
The decoder $\gthetaseul$ is a piecewise affine function, such as a multilayer perceptron with ReLU (or leaky ReLU) activations. 
\end{assumption}
A special case of this model is well-studied in theory and applications and in deep generative models literature~\citep{3dshapes18, ahuja2022sparse}. We consider the following generative process:
\vspace{-0.3cm}
\paragraph{Data-generating process.} We assume \Cref{assump:injective_piecewise},  and we consider the following generative model for observations $\rvx$:
% We assume that each column $\vz_k $ of the matrix $\rvz $ is drawn from the same Gaussian Mixture Model (GMM) in $\mathbb{R}^d $. Specifically, we have:
\begin{align}\label{eq:generative_model}
\rvx = \sum_{k=1}^{n} \gthetaseulk(\rvz) + \xi, \quad
\rvz = (\vz_{1},\ldots,\vz_{n}) \in \sR^{d \times n},\, \mathrm{vec}(\rvz) \sim \sum_{j=1}^J \omega_j \mathcal{N}(\mathrm{vec}(\boldsymbol{\mu}_j), \boldsymbol{\Sigma}_j),
\end{align} 
where $\xi \in \sR^{T}$, denote independent random noise. Our results include the noiseless case $\xi=0$ as a special case (\ie when all sources are well-known). The notation $\mathrm{vec}(\mathbf{\rvz}) \in \sR^{d\cdot n }$ denotes the vectorization~\footnote{The vectorization of $\rvz$ (\ie stacks the columns of $\rvz$ in a single column vector), following a multivariate Gaussian mixture model, is equivalent to $\rvz$ following a Matrix Gaussian mixture, as shown in ~\Cref{app:equivalent_GMM}.}
of $\rvz$ that follows a multivariate Gaussian Mixture Model (GMM), and $\omega_j$ are the mixture weights (with $\sum_{j=1}^J \omega_j = 1 $), with mean $\mathrm{vec}(\boldsymbol{\mu}_j) \in \sR^{d\cdot n } $ and $\boldsymbol\Sigma_j = \boldsymbol\Sigma_d \otimes \boldsymbol\Sigma_n$ with $\boldsymbol\Sigma_d$ being the $d \times d$ covariance and $\boldsymbol\Sigma_n$ the $n \times n$ covariance between sub-components \ie $\vz_k$. Here, \( \otimes \) denotes the Kronecker product. The GMM prior assumption can be generalized to exponential family mixtures~\citep{kivva2022identifiability}, provided the prior is analytic and affine-closed. Additionally, GMMs can approximate complex distributions~\citep{nguyen2019approximations}. This maintains the flexibility and generalization of \Cref{eq:generative_model}, and we impose no constraints on:  \textcolor{blue}{1)} ReLU architectures, \textcolor{blue}{2)} independence of $\rvz$, or \textcolor{blue}{3)} the complexity of the mixture model or neural network.

%The assumption of a GMM prior can be replaced with more general exponential family mixtures \citep{kivva2022identifiability}, as long as the prior is analytic and the family is closed under affine transformations. This does not limit the representational capacity in \Cref{eq:generative_model} because we make no assumptions about: \textcolor{blue}{1)} prior model knowledge, futhermore GMM can approximate complex distribution~\citep{nguyen2019approximations}, \textcolor{blue}{2)} ReLU network architecture, or \textcolor{blue}{3)} the independence of $\rvz$ components. Both the mixture model and neural network can be complex, and the latent space may be high-dimensional and dependent.

% Thus, as far as disentanglement is considered to mean finding the original components s in a nonlinear mixing such as Eq. (3), the very problem seems to be ill-defined. This is a fundamental problem which is receiving increasing attention in the deep learning community, and forms the basic motivation for nonlinear ICA theory

\paragraph{Objective.} Our goal is to identify the latent variables $\rvz$ from a set of observations $\rvx$ that lead to better reconstruction of true sources $\vy_{k} = \gthetaseulk(\rvz)$, thus $\rvy$, which meaning recovering $\rvx$ up to an additive error $\xi$. Thus, as far as disentanglement is considered to mean finding the original components $\rvz$ in a nonlinear mixing such \Cref{eq:generative_model}, the very problem seems to be ill-defined. This is a fundamental problem which is receiving increasing attention in the deep learning community, and forms the basic motivation for nonlinear ICA theory~\citep{HYVARINEN1999429}.
%In representation learning, we usually cannot recover the exact value of the latent variables, but we can only identify them up to some transformation. 
%Similar notions of identifiability were used in previous works; In classical Linear ICA~\citep{comon1994independent}, the observed $\rvx = \boldsymbol{A}\rvz$, assumes independent components in $\rvz$ and a linear mixing matrix $\boldsymbol{A}$. Compared to \Cref{eq:generative_model}, this simplifies to a linear $\gthetaseul$ with $\epsilon = 0$, 
Unlike~\citep{Hyvrinen2023NonlinearIC}, our setting via \Cref{eq:generative_model} does not require $\vz_k$ to be independent, recognizing the interdependencies in real-world data, and instead imposes structure on the nonlinear mixing~\Cref{assump:injective_piecewise}. Identifiability here ensures a linear mapping between ground truth and learned variables but does not guarantee disentanglement. Following~\citep{lachapelle2022disentanglement, locatelloObjectCentricLearningSlot2020}, we extend this to define slot identifiability up to element-wise linear transformations below: 
% %In calssical Linear ICA \citep{comon1994independent}, we observe $\rvx = \boldsymbol{A}\rvz$, where $\rvz$ is assumed to have independent components and $\boldsymbol{A}$ is a mixing matrix. Compared to the general model \Cref{eq:generative_model}, this corresponds to the special case where $\gthetaseul$ is linear and $\epsilon = 0$, or from classical nonlinear ICA~\citep{HYVARINEN1999429}. Our problem setting is distinguished from the classical nonlinear ICA
% model via \Cref{eq:generative_model} as we do not require the $\vz_k$ to be mutually independent as real-world data often have complex interdependencies, we impose forme on the nonlinear mixing function~\Cref{assump:injective_piecewise}. This identifiability definition means there exists a linear function between ground truth
% variables and learned variables, but this does not imply that each ground truth latent variable is
% represented in a disentangled way by an estimated latent variable. Following \citep{lachapelle2022disentanglement, locatelloObjectCentricLearningSlot2020}, in order to define a disentangled version of identifiability, we now define slot identifiability up to element-wise linear transformation:
\begin{definition}[\bf Slot Identifiability and Disentangled Representation]\label{def:disentanglement}
An autoencoder $\gthetaseulhat, \fphihat $ \textit{slot-identifies} $\rvz $ on $\gZ $ w.r.t.~the true decoder $\gthetaseul $ if $\hat{\rvz} = \fphihat(\gthetaseul(\rvz)) $ minimizes the reconstruction loss in \Cref{eq-loss-vae} (first term), and there exists an invertible transformations $\rvh:=\{\vh_1,\vh_2 \dots, \vh_n\}$, with $\vh_{k} \in \sR^{d}$, such that $\hvz_k = \vh_k(\vz_{k})\, \forall k \in [n],$ ensuring a one-to-one mapping. The learned representation $\hat{\rvz}$ identified \emph{up to permutation, scaling, and element wise linear transformation} $\rvz$, if there exist a permutation matrix $\mathbf{\Pi}$ of $[n]$, an invertible diagonal matrix $\boldsymbol{\Lambda}$ constructed from the scaling factors of $\rvh$, and an offset $\mathbf{b}$, such that $\hat{\rvz} = \boldsymbol{\Lambda} \mathbf{\Pi} \rvz + \mathbf{b}$. 
% $\mathbf{\bf\Pi}\rvz = [\vz_{\Pi(1)}, \vz_{\Pi(2)}, \dots, \vz_{\Pi(n)}]^\top$
%Otherwise $\hrvz$ is said to be \emph{entangled}.
%there exists an invertible diagonal matrix $\boldsymbol{\Lambda} $ and offset $\mathbf{b} $ such that 
\end{definition}

\section{Related Work}\label{sec:related_work}
\paragraph{On the Nonlinear ICA for Time Series Representation Learning.} Recent advances in nonlinear ICA has increasingly focused on utilizing temporal structures and nonstationarities for identifiability. \citep{TCL2016} introduced Time-Contrastive Learning (TCL), which assumes independent sources and leverages variance differences across data segments. Similarly, Permutation-based Contrastive Learning (PCL) identifies independent sources under the assumption of uniform dependency. i-VAE~\citep{iVAEkhemakhem20a} extended this by using VAEs to approximate joint distributions in nonstationary regimes, relaxing the independence assumption with promising results. Further, \citep{roth_disentanglement_2023} and \citep{oublal2024disentangling} explored using contrastive learning for latent space recovery without assuming source independence. LEAPS~\citep{yao2021learning} proposed a nonparametric approach for causal discovery in motion, but is limited by assumptions of no instantaneous causal influence and causal constancy. Work by \citep{lachapelle2022disentanglement}, and \citep{klindt2020towards} also requires source independence or some intervention \citep{ahuja_interventional_2023} for identifiability. In contrast, our work extends identifiability theory by relaxing the independence assumption. We impose no constraints on \( p(\rvz) \) beyond its definition in \Cref{eq:generative_model}, offering a more flexible framework. Recent studies have explored structural assumptions like orthogonality~\citep{gresele2021independent, zheng2022on} or fixed sparsity~\citep{moran2022identifiable}, but our approach generalizes these further. Our intuitive arguments for the reason of that sparsity and contrastive learning may be
complementary to each other, and combining them could potentially gain better disentanglement.

%While the latter on sparsity relates to our definition in this paper,  we crucially allow the sparsity pattern  on $\Jb\gthetaseul$ to vary with $\rvz$ which is in line with the basic notion of time series, i.e. sources are not fixed in time, and impose sparsity with respect to block latent rather than individual latent.% Secondly, existing work typically aims to identify individual latent components $\rvz_k$ up to permutations (or linear transformations). %However, this is inappropriate for time series representation learning, where we aim to capture and isolate the subsets of latent corresponding to each source in well-defined slots.

\textbf{Time Series Representation with Out-Of-Distribution.} 
Recent studies in time series representation learning include methods like RNNVAE~\citep{chung_recurrent_2015}, SlowVAE~\citep{klindt2020towards}, and D3VAE~\citep{li_generative_2023}. Other approaches, such as CoTS~\citep{woo_cost_2022}, and CDSVAE~\citep{bai_contrastively_nodate} focus on sequential data with contrasive disentanglement. Transformer-based models, such as Transformer~\citep{zerveas2021transformer}, TimesNet~\citep{timesnet}, Autoformer~\citep{autoformer}, and Informer~\citep{zhou2021informer}, are designed to capture long-term dependencies but do not focus on identifiability or disentanglement. Understanding whether they preserve disentanglement presentation across runs is crucial for robust representation learning.
%Models like TimesNet~\citep{timesnet}, Autoformer~\citep{autoformer}, and FEDformer~\citep{fedformer} address long-term dependencies but don’t focus on generalization in terms of identifiability. 
Inspired by OOD generalization frameworks in object-centric models~\citep{zhao2022toward, netanyahuLearningExtrapolateTransductive2023}, this ideas can be extend to time series. OOD generalization has been demonstrated in additive models~\citep{dongFirstStepsUnderstanding2022} and slot-wise functions with nonlinearity~\citep{wiedemer2023compositional}, assuming identifiability for images. Work by \citep{lachapelle2023additive} and \citep{wiedemer2023provable} shows that additivity of the decoder (see \Cref{sec: Problem set up}) ensures identifiability and decoder generalization under certain assumptions, which we apply to time series for enhanced generalization. Finding generalization with less data has been a major focus, with SSL methods moving beyond negative pairs in CL to alignment with regularization to avoid collapsed representations. Methods like {BYOL}\citep{grill2020bootstrap} and {SimSiam}\citep{chen2021exploring} achieve this via moving-average updates or stop-gradients, while {BarlowTwins}~\citep{zbontar_barlow_2021} aligns cross-correlation with the identity matrix to reduce redundancy. Extending these to time series, we demonstrate promising generalization by employing a decoder satisfying \Cref{assump:injective_piecewise}.


%However, they do not allow slots to interact during rendering and thus cannot adequately model general time series, that what \TimeCSL what allow for natural depency occur but the element of the slots can still dependent with each others.

%On eo fTraditional methods for time series disentanglement often emphasize enforcing statistical independence among representation dimensions \citep{do_theory_2021, klindt_towards_2021}, even when dealing with highly correlated data. In representation learning, identifiability is mostly examined through nonlinear ICA~\citep{jutten2010nonlinear} to find independent latent, usually allowing permutations or element-wise transformations. However, a significant finding shows that without additional assumptions, achieving this is fundamentally impossible with i.i.d. data. There has been an exploration of using auxiliary information to improve identifiability, moving away from the assumption of statistical independence~\citep{trauble_disentangled_nodate, roth_disentanglement_2023, oublal2024disentangling}.  
%In our unique approach, we tackle the time series disentanglement and identifiability without explicit auxiliary variables or prior models.  Instead, we achieve pairwise factorized support through contrastive learning, departing from the traditional independence assumption. 

\vspace{-0.1cm}
\section{Identifiability Guarantees via Contrastive Sparsity-Inducing}\label{sec:proposedmethod}
\vspace{-0.1cm}
 In this section, we begin with the intuition behind the proposed approach, which leverages sparsity in the mixing process to achieve identifiability. We propose an approach leveraging sparsity in the mixing process to achieve identifiability. Previous methods relying on independence or non-Gaussian priors for identifiability often fail in nonlinear cases, as marginal transformations can preserve independence without revealing true structure \citep{HYVARINEN1999429, HyvarinenST19}. We build on the insight that any alternative solution introducing indeterminacy, beyond permutations or component-wise transformations, would result in a denser structure. Rather than constraining functional forms \citep{TalebJutten1999, ahuja_interventional_2023} or relying on auxiliary variables \citep{iVAEkhemakhem20a}, we assume Partial Contrastive Sparsity for time series. This enables learning identifiable and disentangled representations without requiring independence or parametric assumptions on $p(\rvz)$.
 
 %As highlighted in previous works \citep{HYVARINEN1999429}, there are infinitely many ways to preserve the independence of variables after mixing, such as through Darmois construction \citep{Darmois1953AnalyseGD} or measure-preserving automorphisms (MPA) \citep{HYVARINEN1999429}. Moreover, sources with non-Gaussian priors can be transformed into marginally independent Gaussian distributions using simple point-wise transformations \citep{HyvarinenST19}. Consequently, existing strategies that rely on independence or non-Gaussianity for identifiability break down in nonlinear cases. % However, if the true underlying structure is sufficiently sparse, any alternative solution introducing indeterminateness beyond component-wise transformations and permutations (e.g., mixtures or rotations) would correspond to a denser structure. Building on this insight, rather than constraining the functional class (e.g., post-nonlinear models \citep{TalebJutten1999} or conformal maps \citep{HYVARINEN1999429}), or causal intervention~\citep{ahuja_interventional_2023}, or auxiliary variables to enforce structural dependencies among latent sources \citep{iVAEkhemakhem20a}, or the sparsity pattern in the Jacobian of the mixing function~\citep{brady2023provably} as the key to identifying the sources. Instead, first use assume a Partial Contrastive Sparsity for time series. secondly, we show how contrastive sparsity-inducing enable learning identifiability that guarantee disentangled representations without parametric assumptions on $p(\rvz)$ (not require independce).

%We show how the ground-truth generator $\gthetaseul$ and autoencoder $(\fphihat, \gthetaseulhat)$ can be constrained to address both \emph{identifiability} and \emph{generalization}, thereby facilitating compositional generalization of time series representations. Following this, We then define Partial Contrastive Pairing, detailing the conditions for reliable latent structure identification. Finally, we examine generalization as regularization, demonstrating how it ensures identifiability and fosters robust, compositional generalization for OOD time series representations. 
% \subsection{Time Series Identifiability with Contrastive Sparsity-inducing}

%\paragraph{Partial Selective Pairing} When aiming to learn identifiable representations, the importance of contrasting features becomes apparent. More recently, self-supervised learning (SSL) methods have moved away from using negative pairs, as in contrastive learning (CL), and instead focus on alignment with various forms of regularization to prevent collapsed representations. For example, {BYOL}~\citep{grill2020bootstrap} and {SimSiam}~\citep{chen2021exploring} use architectural regularization with moving-average updates for a separate \emph{target} network ({BYOL} only) or a stop-gradient operation (for both). Meanwhile, {BarlowTwins}~\citep{zbontar_barlow_2021} promotes redundancy reduction and alignment by optimizing the cross-correlation between $\rvz$ and $\prvz$ to match the identity matrix, ensuring zero off-diagonals and ones on the diagonal. We can interpret positive augmentation as a modified representation $\prvz$ that is connected to the original $\rvz$ through a conditional distribution $p(\prvz\!\mid\!\rvz)$. This implies that the augmented observation $\prvx$ shares similar information with the anchor observation $\rvx$, and is generated by applying the same mixing function $\gthetaseul$ as defined in data-generating process \Cref{eq:generative_model}. However, in real scenarios, views are not identical. 




% condition, as follows:
% \begin{equation}
% \exists \Ib \subseteq \{1, 2, \ldots, n\} \text{ such that } \Ib \neq \emptyset \text{ and } \Ib \subseteq \{k \mid k \in \mathcal{S}(\rvx) \cap \mathcal{S}(\prvx)\},
% \end{equation}
% where $\Ib$ is the pairing index subset of $\rvx $ and $\prvx $, denoted by $\Ib(\rvx, \prvx) $. 
%Notably, $\Ib(\rvx, \prvx) = \emptyset $ corresponds to a negative anchor (no similarity), while $\Ib(\rvx, \prvx) = [n] $ indicates a positive anchor (strong similarity).  This work addresses identifiability and generalization in time series representations for source separation in real-world scenarios. It aligns with the concept of compositional generalization—particularly, the model’s ability to generalize across both in-distribution (ID) and out-of-distribution (OOD) settings, as discussed in~\citep{brady2023provably}—and emphasizes common sequence-to-sequence time series representations.

% Let $k $ denote the $k $-th source (which could represent any factor) that contributes to the observation $\rvx$. The support $\mathcal{S}$ represents the set of all possible sources that can be actively contributing to the observed $\rvx$. For an observation $\rvx$, the support is defined as follows: $\mathcal{S}(\rvx) := \{k \mid p(s_k, \rvx) > 0, \, k = 1, 2, \dots, n \}$. We define the \emph{Partial Selective Pairing}  condition between two observations $\rvx$ and $\prvx$ based on the \emph{shared support}:
% \begin{equation}
% \exists \mathbf{i} \subseteq \{1, 2, \dots, n\} \text{ such that } \mathbf{i} \neq \emptyset \text{ and } \mathbf{i} \subseteq \{k \midk \in \mathcal{S}(\rvx) \cap \mathcal{S}(\prvx)\},
% \end{equation}
% we say that $\mathbf{i}$ is a pairing index subset of $\rvx$ and $\prvx$, denoted by $\mathbf{i}(\rvx, \prvx)$. We note that anchor is a specific case of our proposed setting: $\mathbf{i}(\rvx, \prvx) = \emptyset$ corresponds to negative anchor (i.e., when there is no similarity between $\rvx$ and $\prvx$), while $\mathbf{i}(\rvx, \prvx) = [n]$ represents positive anchor (i.e., when there is strong similarity between $\rvx$ and $\prvx$). orover, our work adopts a definition of compositional generalization 
% similar~\citep{brady2023provably} to these studies but diverges by centering on common sequence-to-sequence time series representation as the main subject of interest.





% Now, we start by introducing formally the notion of entangled and disentangled representations. First, we assume the existence of some ground-truth representation maped by an encoder $\fphi$ observations $\rvx \in \gX \subseteq \sR^{d}$, \eg time series, to its corresponding interpretable and usually lower dimensional representation. The learned encoder $\hatfphi$ should cover that ground-truth latent.

%
% Intuitively, a representation is disentangled when there is a one-to-one correspondence between its components and those of the ground-truth representation, up to rescaling. When an encoder $\vf_{\hat\vphi}$ is not disentangled, we say it is \emph{entangled}. Note that there exist less stringent notions of disentanglement which allow for component-wise nonlinear invertible transformations of the factors~\citep{PCL17,HyvarinenST19}.


%Contrary to this assumption, appliances are not operated independently; rather, they could be used simultaneously, and their profiles may be correlated (though less likely), thereby challenging the validity of Independent Factorization.thereby challenging the validity of Independent Factorization.

% \subs{Partial Selective Pairing for Time Series Identifiability} 
\vspace{-0.2cm}
\paragraph{\raisebox{1pt}{\textcircled{\scriptsize 1}}\, Partial Contrastive Pairing for Time Series}
For instance, in multiview object-centric settings~\citep{Bengio2020A} or time series (see \Cref{fig:overview}), a view $\rvx$ and its augmentation $\prvx$ typically share limited information rather than complete overlap. To address this, we propose a more general case, \emph{Partial Selective Pairing}, which allows pairs to share only a subset of relevant factors, serving as a relaxation of \emph{Selective Pairing} in SSL. Assuming the data process generating~\Cref{eq:generative_model}, we define the shared support indices $\mathcal{S} $ of all sources that actively contribute to $\rvx$ as $\mathcal{S}(\rvx) := \{ k \mid \vy_{k} \neq 0, \, k = 1, 2, \ldots, n \} $. The \emph{Partial Selective Pairing} between observations $\rvx $ and $\prvx $ is based on \emph{shared support}  $\Ib(\rvx, \prvx) :=\mathcal{S}(\rvx) \cap \mathcal{S}(\prvx)$.
%\begin{assumption}[\bf Sufficient Partial Selective Pairing] {assumptionsufficientsparsity}

\begin{restatable}[\bf Sufficient Partial Selective Pairing]{assumption}{assumptionsufficientsparsity}\label{assump:sufficient_partial_pairing}
For each factor $ k \in [n] $, there exist observations $(\rvx, \prvx) \in \gX$ such that the union of the shared support indices $\rvi = \Ib(\rvx, \prvx)$ that do not include $ k $ must cover all other factors. Formally:
\begin{equation}\label{eq-sufficient_partial_pairing}
\bigcup_{\mathbf{i} \in \mathcal{I} \mid k \notin \mathbf{i}} \mathbf{i} = [n] \setminus \{k\} \quad, \quad \mathcal{I} := \{ \mathbf{i} \subseteq [n] \mid p(\mathbf{i}) > 0 \}
\end{equation}
where \( \mathcal{I} \) is the set of shared support indices and \( p(\mathbf{i}) := \frac{1}{\# \gX} \cdot \# \left\{ \gS(\rvx) = \mathbf{i}, \, \rvx \in \gX \right\} \) gives the probability that the factors indexed by \( \rvi \) are active, with \( k \notin \mathbf{i} \) inactive.
\end{restatable}



%where \( \mathcal{I} \) is the set of shared support indices, and the probability mass function $p(\mathbf{i}) := \frac{1}{\# \gD} \cdot \# \left\{ \gS(\rvx) = \mathbf{i} ; \rvx \in \gD \right\}$, gives the probability that the factors indexed by $\rvi$ are actively contributing and all other index factors $ k \notin \mathbf{i} $ are not.

%Under \cref{assump:sufficient_partial_pairing}, we  
%Given the example motivating in \Cref{fig:overview}, for $n=5$ factors (including ir), the 2-element subsets are $\mathcal{I} = \{ \textcolor{black!60!gray}{(1, 2)}, \textcolor{black!60!gray}{(1, 3)}, \textcolor{black!60!gray}{(1, 4)}, \textcolor{black}{(2, 3)}, \textcolor{black}{(2, 4)}, \textcolor{black}{(3, 4)},...(5,4)\}$. This satisfies \Cref{assump:sufficient_partial_pairing} since, for each \( k \), for example $k=1$, the union of subsets $\bigcup_{\mathbf{i} \in \mathcal{I} \mid (k=1) \notin \mathbf{i}} \mathbf{i}$ not containing \( k \) covers \( [n=4] \setminus \{k=1\} = \{2, 3, 4\} \).
% that every pair of sources should influence more than one different observed variable.

In nonlinear ICA, sufficient variability assumes the auxiliary variable diversely affects source distributions \citep{TCL2016, hyvarinen2019nonlinear}, while \citep{lachapelle2023synergies} adapted this concept for task supports. Similarly, Structural Variability \citep{ng2023identifiability} ensures each pair of sources influences distinct observed variables. However, overlapping influences often occur in real-world time series, posing practical challenges. Instead, our Partial Selective Pairing assumption \Cref{eq-sufficient_partial_pairing} permits some overlap, provided the union of shared support indices (excluding the specific source) spans all sources, enabling flexible modeling of source dependencies.

%%% last
%The assumption above prevent situations in which two latent variables are absent at the same time, as this would complicate their identification from the observations in different scenarios. Notably, in the field of nonlinear ICA with auxiliary variable~\citep{TCL2016}, \citep{hyvarinen2019nonlinear} have adopted the assumption of sufficient variability which requires that the auxiliary variable has a sufficiently diverse effect on the distributions of sources; specifically, the conditional distributions of the sources given the auxiliary variable must vary sufficiently. Similarly~\citep{lachapelle2023synergies} addapted sufficient variability seen in task supports. Furthermore, Structural Variability \citep{ng2023identifiability} requires that each pair of sources influences sufficiently distinct sets of observed variables, minimizing overlap. influences frequently occur, as discussed in \Cref{app:assumption_validation}. This is a \emph{practical limitation} in real-world time series data, where overlapping influences are common. In contrast, our assumption \Cref{eq-sufficient_partial_pairing} of Partial selective Pairing allows some overlap of influences between sources as long as the union of shared support indices (without the specific source) covers all other sources, allowing for the modeling of various dependencies among sources or factors. %Since the latent variables are not directly observable, we only require sufficient partial selectivity from \( \Ib(\rvx, \prvx) \) to recover all factor combinations.

%Second, 

%\rebutall{The rationale of this requirement is that we do not need to know the latent not observed, but we do need to be able to separate observations that are generated by different distributions of $\rvz$. A relaxation of this assumption could involve considering a group of factors $K$ instead of all factors individually. This approach can also be applied to many other time series.}. In a nutshell, this is different from the variability seen in task supports \citep{lachapelle2023synergies}. First,  is designed to be more flexible with contrastive instance induced .  %This induces a natural sparsity, which is crucial for ensuring identifiability.

%With this foundation, we are ready to present our main result, with a detailed analysis provided in \Cref{app:app_def_theorems_proofs}.
%The key idea of Contrastive Sparsity-inducing is similar to the context of sparse learning multitask highlighted in~\citep{lachapelle2022disentanglement}. We leverage these concepts in the context of identifiability through partial selective pairing. 

%Intuitively, this allows us to identify the latent factors $\rvz$ through \emph{sparsity constraint} on the learned representations, given that we are able to partition the data according to the unknown value latent, or in other words, given the index support $\Ib(\rvx, \prvx)$ \ie the sharing index that is contributing to both and $\rvx, \prvx$. The rationale behind this is that we do not need to know the value of the latent unmeasured. Still, we do need to be able to separate observations that are generated by different distributions of $\rvz$, so we can effectively enforce the Gaussianity constraint.

%In this setting, we consider the pairs $(\rvx, \prvx)$ and their corresponding partial selective pairing shared shared support indices set $\Ib(\rvx, \prvx)$, and the latent variables $\rvz \sim \mathcal{N}(\vmu_{\phi}(\rvx), \vsigma_{\phi}(\rvx))$ (\resp $\prvz \sim \mathcal{N}(\vmu_{\phi}(\prvx), \vsigma_{\phi}(\prvx))$), where $\vmu_{\phi} = [\vmu_{\phi\,1}, \ldots, \vmu_{\phi\,n}]^\top$ and $\vsigma_{\phi}$ is a block diagonal matrix of $\vsigma_{\phi,j} \in \mathbb{R}^{d \times d}$. $\Ib(\rvx, \prvx)$ represents the indices of the non-zero components of $\hrvz$ and $\hprvz$, indicating that if, then $\vz_k$ takes a value $\latentk{\rvx} > 1$ (\resp $\hprvz_k$ takes $\latentk{\prvx} > 1$). Conversely, when $\norm{\vy_k}_0 = 0$, we treat $\vz_k$ as unmeasured, assigning it a value $\latentk{\rvx} \leq 1$ that is small enough to indicate that $\vz_k$ is considered noisy. The objective is to separate time series representations so that each inferred slot corresponds to {\it one and only one} ground-truth slot. Specifically, an inference model $\fphihat: \gX \rightarrow \gZ$ \emph{slot-identifies} $\rvz = \fphi(\rvx)$ via $\hrvz = \fphihat(\rvx) = \fphihat(\gthetaseul(\rvz))$ if, for every $k \in [n]$, there exists a unique $j \in [n]$ and a diffeomorphism $\vh_{k}: \gZ_{k} \rightarrow \gZ_{j}$ such that $\hvz_j = \vh_{k}(\vz_k)$ for all $\rvz \in \gZ$.

%The sparsity given $\mathbf{i}$ encourages some components to be zero, under sufficient partial selective pairing ensures that the non-zero components are essential and may be interdependent. This prevents the model from finding trivial solutions where latent variables might be independent but non-informative. Furthermore, it does not rely on any parametric assumptions regarding the distribution of $\rvz$, thereby permitting arbitrary statistical dependence among the latent variables. The less stringent assumption here pertains to sufficient partial selective pairing assumption, which we will describe in depth below through slot latent variables masking.\par
% To satisfy the conditions outlined in \Cref{assump:sufficient_partial_pairing}, a training data space $\gD^{S}$ is constructed from $\gX \times \gY$, incorporating positive pairs as defined in \Cref{sec: Problem set up}. This space includes every configuration that meets the criteria of \Cref{assump:sufficient_partial_pairing} for each factor (e.g., appliance), although it does not encompass all possible combinations of factors.
% p(\mathbf{i}) := \mathbb{P}\left(\bigwedge_{j \in \mathbf{i}} p(\rvs_j) > 0 \land \bigwedge_{j \notin \mathbf{i}} p(\rvs_j) = 0 \right),$ 
%\textcolor{blue}{In a nutshell, this means that sparsiry introduced, a permutation can always be found such that the reconstruction of any given variable depends on the variable itself. Moreover, by enforcing sparsity on the transformed variables $\rvh(\rvz)$, the corresponding transformation becomes an element-wise linear function.}
% Assuming \cref{assump:sufficient_partial_pairing} prevents scenarios where two latent slots are consistently absent simultaneously, which would hinder their simultaneous identification from observations. This contrasts with the variability of task supports (\textcolor{crefcolor}{Asm-3.2}, \citep{lachapelle2023synergies}). Our approach is more generalized and does not restrict to linear tasks. It remains effective even if each source maintains a constant value for most of the time (not necessarily zero) and only varies sparsely.\par
% As an intermediate step, we first prove an important lemma that shows that by enforcing sparsity of the transformed variables, the corresponding transformation is an element-wise linear function. Intuitively, these transformed variables will be the reconstructed masked latent variables $\rvz$.
% \begin{restatable}[Element-wise Identifiability with Sparsity-inducing]{proposition}{propositionelementwisesparsity}\label{propo:elementwisesparsity}
%\begin{lemma}[Element-wise Identifiability for Linear Transformation] 
% Assume that the latent variables $\rvz$ with support $\gZ$ follow the data generating process in Sec.~\ref{sec: Problem set up} and Ass.~\ref{assump:sufficient_partial_pairing} holds. Let the function $\rvh: \mathcal{Z} \rightarrow \mathbb{R}^{d_{\mathcal{Z}}}$ be invertible and linear on $\mathcal{Z}$, and
% \begin{equation}
% \mathbb{E}\norm{\rvh(\rvz)}_0 \leq \mathbb{E}\norm{\rvz}_0.
% \label{equ:sparsity_constraint}
% \end{equation}
% Then $\rvh$ is a permutation composed with an element-wise invertible linear transformation on $\mathcal{Z}$.
% \label{proposition:element_wise_h}
% \end{restatable}
\vspace{-0.1cm}
\paragraph{\raisebox{1pt}{\textcircled{\scriptsize 2}}\ Identifiability via Contrastive Sparsity-inducing.}
According to~\Cref{assump:sufficient_partial_pairing}, the sparsity-inducing nature arises from the existence of a source $k \notin \rvi$. However, this source is still well-defined within the support indicating that existing source $k$ remains inactive in either $\rvx$ or $\prvx$. The use of a sparsity constraint or regularization is inspired by prior work~\citep{ahuja_interventional_2023, lachapelle2023synergies} in the context of sparse multitask learning. The loss of zero reconstruction ensures that the encoding \( \fphi(\rvx) \) retains all information, implying that \( (\hrvz, \hprvz) \) achieves sparsity comparable to the ground truth \( (\rvz, \prvz) \). This sparsity in a latent representation \( \hrvz \), means only a subset of latent variables are active for a given input \(\mathbf{x}\). If  $\frac{\left|\hat{\boldsymbol\mu}_{k,\phi}(\mathbf{x})\right|}{\hat{\boldsymbol\sigma}_{k,\phi}(\mathbf{x})}  \text{ is small (e.g., close to zero)}$, it suggests the \(k\)-th latent variable is not contributing, thus making it inactive $\vy_k=0$. However, when $\frac{\left|\hat{\boldsymbol\mu}_{k,\phi}(\mathbf{x})\right|}{\hat{\boldsymbol\sigma}_{k,\phi}(\mathbf{x})}  \text{ is large (e.g., }\geq 1\text{)}$, it implies the source $k$ contribute to $\rvx$. Bounding this ratio ensures that only the most relevant latent variables remain active, indirectly enforcing sparsity by limiting the number of significant variables. This raises the question of whether minimizing the \( l_0 \)-norm of the learned latents, with sufficient partial pairing, can identify \( \rvz \) through \( \gthetaseulhat^{-1}(\rvx) \) up to permutation and element-wise linear transformations. While \( \gthetaseul \) is non-linear, sparsity alone only valid for the linear case \citep{lachapelle2022disentanglement} which is a strong assumption and may not be sufficient to resolve the ambiguities introduced by the non-linearity in many real-world cases. Our results shows that, sparsity without additional constraints, does not guarantee identifiability in practice, as for  \( \rvh = \gthetaseulhat^{-1} \circ \gthetaseul \) can depends on multiple components of \( \rvz \). Building on this insight, we extend the concept of sparsity to contrastive sparsity by assuming \Cref{assump:injective_piecewise}, without requiring bijectivity, and provide conditions under which \( \rvz \) can be identified up to permutation and element-wise transformations.

%One may ask if the sufficient partial paring with constraint that minimizing the $l_0$-norm of the learned latent latent, can identified $\rvz$ by $\gthetaseulhat^{-1}(\rvx)$ up to a permutation and element-wise linear transformations ~\Cref{def:disentanglement}, \ie $\gthetaseulhat^{-1}\circ\gthetaseulhat$ is a permutation composed with element-wise invertible linear transformations. However, as we do not consider $\gthetaseul$ Linear mixing as such case of $\gthetaseul$ linear is a strong assumption and not applicable in many real-world cases, sparsity alone cannot resolve the ambiguity introduced by the piecewise nature of the function. 

%As a first step towards a more general setting, we consider piecewise linear mixing functions $\gthetaseul$ \ie \Cref{assump:injective_piecewise}. 

%This sparsity constraint aligns with the framework for linear identifiability, as discussed in \citep{ahuja2022sparse}. Specifically, if an invertible function \( \rvh: \mathbb{R}^{d \times n}  \rightarrow \mathbb{R}^{d \times n} \) satisfies the sparsity constraint, it ensures that the latent variables can be identified uniquely in a way that maintains the structure of the data. The sparsity of \( \vz_k \) not only limits the solution space but also guarantees that the latent representations are distinguishable and identifiable under the linear mixing model.

%Given that the latent variable $\rvz$ is degenerate, we apply the result \citep{ahuja2022sparse} that if an invertible function $\rvh: \mathcal{Z} \rightarrow \mathbb{R}^{d_{\mathcal{Z}}}$ satisfies the sparsity constraint:
% \begin{lemma}[Element-wise Identifiability for Linear Transformation] Assume that the masked latent variables $\rvz$ with support $\gZ$ follow the data generating process in Sec.~\ref{sec:  Problem set up} and Ass.~\ref{assump: sufficient support} holds. Let the function $\rvh: \gZ \rightarrow \sR^{d \times n}$ be invertible and linear on $\gZ$, and
% \begin{equation}
% \mathbb{E}\norm{\rvh(\rvz)}_0 \leq \mathbb{E}\norm{\rvz}_0.
% \label{eq:sparsity_constraint}
% \end{equation}
% then $\rvh$ must be a permutation combined with an element-wise invertible linear (see \Cref{def:disentanglement}) transformation on $\mathcal{Z}$.
% \end{lemma}

%The sparsity constraint ensures that  $\frac{|\hmuphik{\rvx}|}{\hsigmaphik{\rvx}} \leq 1$ for $\hvz_k$ and $\frac{|\hmuphik{\hprvx}|}{\hsigmaphik{\hprvx}} \leq 1$ for $\hpvz_k$, respectively.


% Theoretically, $\rvh$ involves a permutation $\mathbf{\Pi}: d_{\gZ} \rightarrow d_{\gZ}$ such that, for each index $k$, $N_k = \{\Pi(k)\}$ represents the set of dependent inputs. If there is exactly one latent slot $\rvz_j$ such that $(\rvz_j, \rvz_{-j}) \in \gZ$, then any function defined on $\gZ$ remains constant for that index, ensuring $N_{\Pi(k)} = \{k\}$. This guarantees that the permutation preserves the latent slots' sparsity and structure.\par
% We build on the result of sparsity, noting that since the ground truth latent structure is unknown, the sparsity indicated in \Cref{eq:sparsity_constraint} will merely serve as a hyper-parameter. Based on the data partition concerning the Partial Selective Pairing, the indices defined in $\Ib(\rvx, \prvx)$ will enforce similarity between the latent variables. This, in turn, will encourage alignment of the latent representations. We consider the indices $k$ for the inactive factors in $\hrvx$ and $p$ in $\hprvx$. This alignment resolves indeterminacies caused by rotations in the latent space.\par

% Given that the latent variable $\rvz$ is degenerate, we leverage the known result that if an invertible function $\rvh: \mathcal{Z} \rightarrow \mathbb{R}^{d_{\mathcal{Z}}}$ satisfies that the sparsity of the estimated representation cannot exceed that of the ground truth, i.e.,
% \begin{equation}
% \mathbb{E}\norm{\rvh(\rvz)}_0 \leq \mathbb{E}\norm{\rvz}_0.
% \label{eq:sparsity_constraint}
% \end{equation}
% then $\rvh$ is a permutation composed with an element-wise invertible linear transformation on $\mathcal{Z}$.

% Moreover, there exists a permutation $\Pi: d_{\mathcal{Z}} \rightarrow d_{\mathcal{Z}}$ such that for each column index $k$ of $\rvz$, the set $N_k \subseteq [n]$ consists only of the permutation of $k$, denoted as $\Pi(k)$. If only one slot latent $\rvz_j$ exists such that $(\rvz_j, \rvz_{-j}) \in \mathcal{Z}$ (with $\rvz_{-j}$ representing all other components except $\rvz_j$), then any function on $\mathcal{Z}$ remains constant for $j$, implying that $N_{\Pi(k)} = \{k\}$.



% Meanwhile, the $\rvh$ is a permutation composed with an \emph{element-wise} invertible linear transformation on $\gZ$, as there exist a permutation $\Pi:d_{\gZ} \rightarrow d_{\gZ}$ such that, for each columns index $k$ of $\rvz$, the set $N_{k} \subseteq [n]$ is composed solely of the permutation of $k$, denoted as $\Pi(k)$. Here, $N_{k}$ denotes the set of dependent inputs for which, if there exists only one slot latent $\rvz_j$ such that $(\rvz_j, \rvz_{-j}) \in \gZ$ for an appropriate $\rvz_{-j}$ \ie all components except $\rvz_j$, then any function defined on $\gZ$ remains constant for that index, leading to the exclusion of $j$ from $N_{k}$ implies that $N_{\Pi(k)}=\{k\}$. 



%\textcolor{red}{sparsity is not enough}
%Our framework assumes stronger sparsity-inducing via \Cref{assump:sufficient_partial_pairing} that help to identify representation up to affine transformations, as defined in~\Cref{def:disentanglement}.

%To achieve~\Cref{propo:elementwisesparsity}, assuming~\Cref{assump:sufficient_partial_pairing}, we use the fact that $\rvh$ is a diffeomorphism and its Jacobian $\mathbf{J}\rvh = \{\frac{\partial \vh_{k}}{\partial z_j}(\rvz)\}_{k, j \in [n]}$ is invertible everywhere. Since $\mathbf{J}\rvh(\rvz)$ is invertible, its determinant is non-zero. This implies that for all $k \in [n]$, $\frac{\partial \rvh_{\Pi(k)}}{\partial \rvz_k}(\rvz) \neq 0$, meaning $\rvh_{\Pi(k)}$ is not constant with respect to $\rvz_k$ in $\rvz$.  !!  In this setting, $\rvh$ is a diffeomorphism and its Jacobian $\rvh = \{\frac{\partial \vh_{k}}{\partial z_j}(\rvz)\}_{k, j \in [n]}$ is invertible everywhere. Since $\mathbf{J}\rvh(\rvz)$ is invertible, its determinant is non-zero. This implies that for all $k \in [n]$, $\frac{\partial \rvh_{\Pi(k)}}{\partial \rvz_k}(\rvz) \neq 0$, meaning $\rvh_{\Pi(k)}$ is not constant with respect to $\rvz_k$ in $\rvz$.

% \begin{restatable}[Linear Identifiability for possibly degenerate multivariate distribution with Piecewise Affine $\gthetaseul$ adapted from \citet{kivva2022identifiability}]{theorem}{thmpcaffine}\label{thm:disentanglement_piecewise}
% Assume the observation $\mathbf{x}$ follows the data-generating process, and $p({\rvz|\rvm})$ is multivariate normal distribution. Assume further that
% \begin{itemize}
%     \item Let $\fphi: \gX \rightarrow \sR^{d_z}$ be a continuous invertible piecewise linear function. Let $\gthetaseul: \gZ \to \gX$ is an injective continuous piecewise linear function and let $\gthetaseulhat: \sR^{d_z} \rightarrow \sR^{d}$ be a continuous invertible piecewise linear function onto its image;
%     \item Contrastive Sparcity-inducing under assumption of sufficient masking: \begin{align}
%     &\mathbb{E}_{(\rvx,\prvx) \sim p({\rvx,\prvx})}\left[\norm{\fphi(\prvx) - \fphi(\mathbf{x})}^2_2\right] = 0 \quad \text{s.t.} \quad \mathbb{E}_{\rvx \sim p({\rvx,\prvx})}\left[\norm{\fphi(\mathbf{x})}_1\right] \leq \mathbb{E}\left[\norm{\rvz}_1\right] \, %\nonumber %\text{and} 
%     %\fphi(\rvx)~|~\rvm \sim p_{\rvm},
%     \end{align}
% \end{itemize}
% %$n_a$ ($1\leq n_a <n$), a 
% If, for a given model generative  %$(\hat{p({\rvz}), 
% $(\hatfphi, \gthetaseulhat)$ assumes the same generative process, satisfies the above assumptions, and matches the data training likelihood \ie $\forall (\rvx,\prvx)\in \gX\times\gX,$ we have $p(\rvx,\prvx)=\hat{p}(\rvx,\prvx)$, then the ground-truth slots latent $\rvz$ is identified by $\fphi = \gthetaseulhat^{-1}$ \ie $\gthetaseulhat^{-1} \circ \gthetaseul$ is a permutation composed with element-wise invertible linear transformations.
% \end{restatable}

% \begin{restatable}[Element-wise Identifiability via under Sparsity-inducing for $\gthetaseul$]{theorem}{thmpiecewiselinear}
% 
% Assume observations $\rvx$ and $\prvx$ follow the process in Sec~\ref{sec: Problem set up}, with pairing index subset $\rvi$ under Ass.~\ref{assump:sufficient_partial_pairing}, with $\gthetaseul: \gZ \to \gX$ satisfying Ass.~\ref{assump:injective_piecewise}. Let $\fphi: \gX \rightarrow \gZ$ be a continuous invertible piecewise linear function, and $\gthetaseulhat: \gZ \rightarrow \gX$ be continuous invertible piecewise linear functions nto its image. 


\begin{restatable}[Element-wise Identifiability given index support $\rvi$ for Piecewise Linear $\gthetaseul$]{theorem}{propositionpiecewiselinearsupportindex}
\label{thm:Elementwise_Identifiability_piecewise}
Let $\fphi: \sR^{d \times n} \rightarrow \sR^{T \times n}$ be a continuous invertible piecewise linear function and $\gthetaseulhat: \sR^{d \times n} \rightarrow \sR^{T \times n}$ be a continuous invertible piecewise linear function onto its image. Assume that~\Cref{assump:sufficient_partial_pairing},  ~\Cref{assump:injective_piecewise} holds, and the mixed observations $(\rvx, \prvx) \overset{\scriptstyle \text{i.i.d.}}{\sim} \gX$, follows the data-generating process~\Cref{eq:generative_model}. The learnable latent \( \hrvz \) (resp. \( \hprvz \))  of \( \rvz \) (resp. \( \prvz \)). If all following conditions hold:  
\begin{align}\label{eq:align}
    \mathbb{E}\|\hrvz\|_0 \leq \mathbb{E}\|\rvz\|_0 \quad \text{and} \quad \mathbb{E}\|\hprvz\|_0 \leq \mathbb{E}\|\prvz\|_0 \text{, and,} \\
    \gR_{alig}(\hrvz, \hprvz, \mathbf{i}) := \sum_{i \in \rvi} \left| \frac{\hvz_i^{\prime\top} \hat{\vz}_i}{\|\hvz'_i\|_2 \|\hvz_i\|_2} - 1 \right| = 0.
\end{align}
then $\rvz$ is identified by $\rvh:=\gthetaseulhat^{-1}(\rvx)$, i.e., $\gthetaseulhat^{-1} \circ \gthetaseul$ is a permutation composed with element-wise invertible linear transformations (Def.~\ref{def:disentanglement}).  
\end{restatable}

\textbf{\it Proof Sketch.} The complet proof are given in~\Cref{app:propositionpiecewiselinearsupportindex}. Intuitively, based result \citep{kivva2022identifiability} combined with contrastivity between tow latent based on their shared support indices $\rvi$. This means that for the data that satisfy \Cref{assump:sufficient_partial_pairing}, $\gthetaseul(\rvz)$ and $\gthetaseulhat(\hrvz)$ are equally distributed, then there exists an invertible affine transformation such that $\rvh(\rvz)=\prvz$. 
Second, we use the strategy of linear identifiability \citep{lachapelle2022partial} to obtain element wise identifiabiltiy. \par
This approach is similar to SparseVAE~\citep{moran2022identifiable}, which enforces constraints using Spike-and-Slab Lasso. However, our method ensures slot identifiability through Partial Selective Pairing, without requiring strong assumptions or extra constraints on $\gZ$. In contrast, SparseVAE uses separate decoders for each feature. Another line of work can dive to constrains the generator $\gthetaseul$ via its Jacobian $\Jb\gthetaseul(\rvz)$, known as compositionality and irreducibility~\citep{vonkugelgen2021selfsupervised,brady2023provably}. Definitions are provided in \Cref{app:app_comp_irr}. Within our framework, compositionality means that each high-dimensional source is controlled by only one latent slot $\vz_k$, enforcing local sparsity. However, minimizing compositionality in $\gthetaseulhat$ on $\gZ$ is computationally infeasible~\footnote{For a CNN with 1 million parameters and a batch size of 32, at least 250GB of GPU memory is required.}.
%This approach aligns well with prior work, such as SparseVAE~\citep{moran2022identifiable}, which imposes additional constraints on the latent variables via Spike-and-Slab Lasso. However, in our case, sparsity-inducing through Partial Selective Pairing ensure slot identifiability without relying on strong assumptions about $\gZ$ or additional constraints related to prior optimization. Coversarly, SparseVAE requires a distinct decoder for each feature or factor. Another line of work can dive to the \Cref{thm:Elementwise_Identifiability_piecewise}, is by imposing the the constraint on the generator $\gthetaseul$ based on its Jacobian $\Jb\gthetaseul(\rvz)$, termed compositionality and irreducibility~\citep{vonkugelgen2021selfsupervised,brady2023provably}. We give in the \Cref{app:app_comp_irr} the formal definition of irreducibility and compositionality. Within our framework, compositionality means that each high-dimensional source is controlled by at most one latent slot $\vz_k$, imposing local sparsity in the Jacobian matrix by minimizing compositionality of $\gthetaseulhat$ on $\gZ$ which is  computationally infeasible to optimize in paracitcs~\footnote{For a CNN with 1 million parameters and a batch size of 32, at least 250GB of GPU memory is required.}.

%More specifically, assumes the decoder $\gthetaseulhat$ satisfies compositionality on $\gZ$, but does not provide a practical enforcement method.
% \begin{equation}\label{eq:compositional}
%     \frac{\partial \gthetaseulk}{\partial \rvz_{i}}(\rvz) \neq 0 \implies \frac{\partial \gthetaseulk}{\partial \rvz_{j}}(\rvz) = 0, \quad\text{for any $i,j \in [n]$, $i\neq j$ and any $k \in [n]$.}
% \end{equation}
%\citep{brady2023provably} proposed a regularizer that enforces compositionality if minimized, but their objective is computationally infeasible to optimize for time series models (\ie large models), thus limiting its practical use for time series.

% % yields $p(\hrvz~|\prvz \sim \mathcal{N}(\vmu_{\phi}, \vsigma_{\phi})$, where $\vmu_{\phi} = [\vmu_{\phi\,1}, \ldots, \vmu_{\phi\,n}]^\top$ and $\vsigma_{\phi}$ is a block diagonal matrix of $\vsigma_{\phi,j} \in \mathbb{R}^{d \times d}$. 
% Thus, the ground truth $\rvz$ is identified by $\gthetaseulhat^{-1}(\rvx)$, implying that $\gthetaseulhat^{-1} \circ \gthetaseul$ is a permutation combined with element-wise invertible transformations in the sense of \Cref{def:disentanglement}. 
% % \end{restatable}
%To prove \Cref{thm:disentanglement_piecewise}, we now first, show that given the information of $\rvi$, we can identify the latent factors $\rvz$ up to an affine transformation (Def.~\ref{def:disentanglement}). \ref{eq:sparsity_constraint1} under the inequality, suggests that the expected sparsity of the learned latent variable (left side) will not exceed the expected sparsity of the ground truth latent variable (right side).

% We first provide some results for a weaker notion of identifiability: \emph{identifiability up to affine transformations}~(\Cref{def:disentanglement}). To this end, we first extend a theorem by \citet{kivva2022identifiability} to take on consideration case where a shared shared support indices is available. We then show that given the information of the subset index $\rvi$ according to \Cref{assump:sufficient_partial_pairing}, we can identify the true latent up to an affine function denotes $\vh_{k}$
% (\Cref{proposition:element_wise_h}), this as \ref{eq:sparsity_constraint1} under the inequality, suggests that the expected sparsity of the learned latent variable (left side) will not exceed the expected sparsity of the ground truth latent variable (right side).. Finlay, we demonstrate that all of these affine functions $\vh_{k}$ can be represented by a single affine function $\rvh:= \gthetaseulhat^{-1}(\gthetahat{\rvz})$ defined on $\gZ$.\par
\vspace{-0.1cm}
\paragraph{\raisebox{1pt}{\textcircled{\scriptsize 3}}\ Invariance for Compositional Generalization Representation}
From~\Cref{thm:Elementwise_Identifiability_piecewise}, it follows that \(\gthetaseulhat\) faithfully maps each inferred slot \(\vh_{k}(\vz_{\pi(k)})\) to its corresponding source in \(\rvx\) for all possible values of \(\vz_{\pi(k)}\), ensuring identifiability (ID). We extend this to ensuring OOD scenarios by simply composing the latents from the training set and applying a stop gradient to prevent the gradients from flowing back into the recomposed latent during training (see \Cref{fig:overview}). During training, simultaneously, we perform ID and OOD, ensuring that the combined latent remains consistent \ie compositional with the original latent, allowing the model to generalize OOD samples while retaining the ID.
%\begin{restatable}[Compositional Generalization Consistency via Invariance slots]{corollary}{corollarycompositionalgeneralizationconsistency}\label{coro:compositional}
Assuming the conditions stated in \Cref{thm:Elementwise_Identifiability_piecewise} are satisfied, this implies the existence of transformations \(\rvh\), along with a permutation \(\pi\), that enable the slot-identification $\rvz$ for any composition of slots, whether ID or OOD, over $\gZ$, as given by
\begin{equation}\label{eq:support_permutation}
    \rvz_{c} = \fphi( \vh_{1}(\vz_{\pi(1)}), \ldots, \vh_{n}(\vz_{\pi(n)}) \big),\, \text{and }\,
    \gZ_c = \fphi( \vh_{1}(\gZ_{\pi(1)})\times \cdots \times \vh_{n}(\gZ_{\pi(n)}) \big).
\end{equation}
The compositional generalization consistency on $\gZ'$, holds, \ie $\gthetaseulhat^{-1}(\gthetaseul(\rvz)) = \rvz_{c} $ and $\gthetaseulhat(\rvz_{c})=\gthetaseul(\rvz)$, if and only if $\rvz_{c}$ minimizes the invariance such that,
\begin{equation}\label{coro:compositional}
%\sum_{i \in \rvi}
\gR_{inv}(\rvz_{c}) :=  \sum_{i \neq k} \left( \frac{\vz^{\top}_{c\,i} \vz_{c\,k}}{\|\vz_{c\,i}\|_2 \|\vz_{c\,k}\|_2} \right)^2, \text{ for some $\gamma_{inv}>0$, $\gamma_{inv}\gR_{inv}(\rvz_{c})=0$.}
\end{equation}
%\end{restatable}
% \label{coro:compositional}
The condition in \Cref{coro:compositional} ensure that $\fphihat$ inverts $\gthetaseulhat$ on ID and OOD by re-encoding the latent from inferred ones (see \Cref{fig:model}). Implementation details and sampling process of $\rvz_{c}$ for this regularization is discussed in \Cref{sec:implementation_loss}. To validate \Cref{coro:compositional}, we have just to verify the compositional consistency error \ie $\gthetaseulhat^{-1}(\gthetaseulhat(\rvz_{c})= \rvz_{c}$ over $\forall\,\rvz_{c}\in\gZ_c$. Formally:
\begin{equation}\label{eq:compositional_consistency}
    \mathcal{L}_{cons} := \sE_{\rvz_{c} \sim q_{\phi}(\rvz_c)}[||\fphihat(\gthetaseulhat(\rvz_{c}) - \rvz_{c}||]=0,\, \textit{where, $ \textit{supp}(q_{\phi}(\rvz_c))=\gZ'$} \cref{eq:support_permutation}.
\end{equation}

%The reformulation highlights that we can construct OOD samples in the consistency regularization from ID observations by randomly shuffling the slots of two inferred ID latent. We can interpret the decoder $\gthetaseulhatk$ as an additive mechanism, where each slot $\hrvz_{k}$ is transformed into an intermediate image via \emph{slot functions} $\gthetaseulhatk$. These intermediate images are then summed to produce the final output. This approach allows the decoder to act as a time series \emph{compositional generator}. As a result, $\gthetaseulhat$ becomes injective on $\gZ'$, yielding the result ${\gthetaseulhat(\gZ') = \gthetaseul(\gZ) = \gX}$.

%We denote by $\gR_{inv}^{o/w}(\hrvz)$ the case invariancce temre without any combination (non-additive decoder). The reformulation highlights that we can construct OOD samples in the consistency regularization from ID observations by randomly shuffling the slots of two inferred ID latents. We can view the decoder $\gthetaseulhat$ as an additive mechanism that transform's each slot $\hrvz_{k}$ into an intermediate image through \emph{slot functions} $\gthetaseulhatk$. These intermediate images are then summed to form the final output. This approach allows the decoder to serve as a time series \emph{compositional generator}, effectively removing interactions among the slots. Consequently, $\gthetaseulhat$ becomes injective on $\gZ'$, yielding the result ${\gthetaseulhat(\gZ') = \gthetaseul(\gZ) = \gX}$.

% \paragraph{Does Encoder Generalize ?} 

\begin{figure}
    \centering
    \vspace{-0.8cm}
    \includegraphics[clip, trim=0cm 0.cm 0.0cm 0.0cm, width=0.84\textwidth]{figures/partial_pairing.pdf}
    \caption{\textbf{Overview of \TimeCSL framework using ResTimeCSL Architecture.} After linearly projecting the time series patches into high dimensional embeddings the ResTimeCSL is affine. \label{fig:model}}
    \vspace{-0.4cm}
\end{figure}

\subsection{Putting it All Together in Practice}\label{sec:implementation_loss}

\textbf{On the Possibility of Sufficient Partial Pairing} In \Cref{thm:Elementwise_Identifiability_piecewise}, we demonstrated how slot identifiability can be achieved on $\gZ$ and OOD $\gZ_c$ under the compositionality condition in \Cref{eq:compositional_consistency}. A key insight is the sufficient partial pairing for contrastive learning (\Cref{assump:sufficient_partial_pairing}). This assumption can be relaxed to factor groups when the dataset is complex enough to discern varying features (e.g., in weather time series). For such cases, grouping factors avoids assumption violations. We validated our results on synthetic time series data (assumptions fully satisfied) and energy separation tasks, were used to relax assumptions via grouping factors. Data was prepared in pairs $(\rvx, \prvx)$, with additional samples generated as needed to cover all factors.\par

\textbf{Conditions on the Network.} We proposed ResTimeCSL (see \Cref{fig:model}), an efficient architecture for time series modeling that doesn't violate \Cref{assump:injective_piecewise}. It projects time series patches into high-dimensional embeddings and processes them sequentially using a cross-patch linear sublayer and a cross-channel two-layer MLP, similar to the Transformer’s FCN sublayer. Each sublayer includes residual connections, two affine element-wise transformations, and uses ReLU or LeakyReLU activations. For training, we leverage a VAE model with a mixture of Gaussians~\citep{jiang2016variational} for a fixed latent dimension by $n$ and $d$, optimizing the objective $\mathcal{L}_{\textsc{VAE}}$. We sample i.i.d. pairs \((\rvx, \prvx) \in \gX\). Using a learnable encoder \(\fphihat\), \(\rvx\) (resp. \(\prvx\)) is encoded into \([\hmuphik{\rvx}, \hsigmaphik{\rvx}]^{\top}\) (resp. \([\hmuphik{\prvx},  \hsigmaphik{\prvx}]^{\top}\)) with reparameterization noise terms~\citep{kingma_auto-encoding_2022-1}. The inferred latents are \((\hrvz, \hprvz)\). A learnable decoder \(\gthetaseulhat\) maps \(\hrvz\) (resp. \(\hprvz\)) to single-source outputs \(\hat{\vy}_k = \gthetaseulhat_{k}(\hrvz)\) (resp. \(\hat{\vy}^{\prime}_k = \gthetaseulhat_{k}(\hprvz)\)) for \(k = 1, \cdots, n\). Summing over these outputs reconstructs the mixed signals \(\hrvx\) (resp. \(\hprvx\)). In practice, the sparsity of the ground truth variables $\rvz$ is unknown, so we instead set a hyperparameters $\eta$ for the sparsity constraint, furthermore for more stability, instead of $\mathbb{E}\norm{\rvz}_{0} \leq \eta$ we consider $\| \mathbf{v} \|_{s,\text{norm}} = \frac{1}{d_{z}} \sum_{i=1}^{d_{z}} \sum_{j=1}^{n_a + 1} |v_{ij}|$. The \TimeCSL objective serves then as a regularization term for the loss $\mathcal{L}^{*}_{\textsc{VAE}}$, that denote the sum of $\mathcal{L}_{\textsc{VAE}}$ computed for time series $\rvx $ and $\prvx$. Thus, the final objective can be expressed as follows: 
\begin{align}\label{equ:TimeCSL}
\mathcal{L}_{\TimeCSL}(\phi, \theta; \mathcal{B}) &= \lfancy^{*}_\text{VAE}(\phi,\theta; \mathcal{B}) + \mathbb{E}_{\mathcal{B}}[\gamma_{alig}\gR_{alig}(\rvz, \prvz, \mathbf{i})] + \mathbb{E}_{\mathcal{B}} [\gamma_{inv}\gR_{inv}(\rvz_{c}, \mathbf{i})] \\
&+ \mathbb{E}_{\mathcal{B}}\|\max(0, \|\hrvz\|_s - \eta) + \max(0, \|\hprvz\|_s - \eta)\|\nonumber,
\end{align}
where $\mathcal{B}$ is a batch of data. The alignment term $\gR_{alig}$ penalizes deviations from cosine similarity between corresponding latents, scaled by $\gamma_{alig}$. The invariance term $\gR_{inv}$, scaled by $\gamma_{inv}$, reduce invariance of the latent composed $\rvz_{c}$ from $\hrvz$ and $\hprvz$. In our experiments, we use $\eta=0.01$ or $0.001$.   %We refer to loss $\gL(\phi,\theta; \cdot)$ as main objective reconstruction and regularization, while regularization introduced with sparsity as $\gL_{\text{cons}}(\phi,\theta; \cdot)$



% \textbf{Discussion.}
% \cref{thm:disentanglement_piecewise} assumes that the number of slots latent variables is known,
% and that there is a positive probability that each \textit{slot} and sufficient sparsity. In this case, training a generative model of the form specified in~\cref{sec: Problem set up}  by maximum likelihood on pairs $(\rvx,\prvx)$ will asymptotically recover the true slot latent. \Cref{thm:disentanglement_piecewise} assumes that the number of known $n_{a}$ slots latent is fixed. Ensuring the likelihood matching condition guarantees that the encoding $\fphi(\rvx)$ retains all information from $\rvx$, and \Cref{eq:disentanglement_piecewise} implying that $\fphi(\rvx)$ is not more sparse than $\rvx$. This property persists even under small translations, as the term addressing invariance is minimized. Additionally,  the condition~\Cref{eq:disentanglement_piecewise} ensure the gaussianity of the representation $\fphi(\rvx)$.  This ensures that the composition of two piecewise linear functions $\fphi$ and $\gtheta$ remains affine on $\gZ$, we highlight that $\rvz$ is potentially a degenerate multivariate normal, but $\fphi$ and $\gtheta$ still affine on the support of $\gZ$. \Cref{eq:disentanglement_piecewise} enforces the encoder $\fphi$ to be \emph{\textcolor{gray!60!black}{\it aligned}} only with source that are augmented, while ensuring sparsity and \emph{\textcolor{gray!60!black}{likelihood}} matching. The first term of our ~\cref{eq:disentanglement_piecewise} is similar to the \emph{alignment} term in BarlowTwins~\citep{zbontar_barlow_2021}, however the limitation of such objective is already explored as it find trivial representation. However, the effectiveness of this approach is limited by the stringent constraints it imposes on the encoder, making it challenging to scale these methods up to high-dimensional settings. As discussed in~\cref{sec: Problem set up}, modern self-supervised learning (SSL) methods, including those mentioned, opt against using invertible encoders. Instead, they mitigate the risk of collapsed representations resulting from naively optimizing~\eqref{eq:disentanglement_piecewise} with arbitrary, non-invertible $\fphi$ through various regularization techniques. In contrast, our objective~\eqref{eq:disentanglement_piecewise} with $\ell_1$ regularization ensures that collapse does not occur, shedding light on why some SSL methods excel in time series representation learning when sparsity is maintained.

\section{Experiments}
\label{sec:experiments}
%\paragraph{Datasets.}
%Due to page limitations, we present further detailed results in Appendix~\Cref{app:other_datasets} regarding other real-world time series as well as synthetic data \Cref{app:synthetic}. Notably, we found that regularization is crucial for maintaining identifiability in forecasting.
\subsection{Validation of The Theory}
\paragraph{Datasets and Evaluation Setup.}\label{subsec:datasets}
We conducted experiments for time series representation with separation task on three public \textbf{real datasets:} UK-DALE~\citep{kelly_uk-dale_2015}, REDD~\citep{kolter_redd_2011}, and REFIT~\citep{murray2017electrical} providing power measurements from multiple homes. The 70\% of the data is used for training (including 10\% of data augmentation), while the remaining 40\% of real data is evenly divided between validation and testing. Inputs are zero-mean normalized, we consider $T=256$, $C=1$ and number factors/sources $n=5$: \text{Fridge (FR)}, \text{Dishwasher (DW)}, \text{Washing Machine (WM)}, \text{Heater (HTR)}, and \text{Lighting (LT)}. The mixed observation may include unlabeled noise factors. \textbf{Synthetic Dataset:} we generate a nonlinear mixing observations with $n=3$, from ground truth available signals of~\SynFactors~from UK-DALE, REDD, and REFIT with adding some Gaussian noise. To generate OOD scenarios \Cref{tab:impact_sparsity} \ie strong correlation between factors, we adopt the methodology outlined in \citep{trauble_disentangled_nodate} where $p(y_{1}, y_{2}) \propto \exp\left(-||y_{1} - \alpha y_{2}||^2/2\sigma^2\right)$, we are modifying the parameter $\sigma$ to adjust the correlation. \par
\vspace{-0.1cm}
\paragraph{Metrics.} To assess slot identifiability, we follow \citep{locatelloObjectCentricLearningSlot2020} by fitting nonlinear regressors to predict each ground-truth slot \(\rvz_k\) from inferred slots \(\hrvz_j\), evaluating the fit with the \RSquare score. Slot assignments are optimized via the Hungarian algorithm~\citep{kuhnHungarianMethodAssignment1955}, and we report the average \RSquare over matched slots. Additionally, we use the Mean Correlation Coefficient (\MCC) metric~\citep{iVAEkhemakhem20a}, reporting both \emph{strong} \MCC (before affine alignment) and \emph{weak} \MCC (after alignment). All MCCs are computed out-of-sample: the affine map \(\Gamma\) is fitted on one half of the data and evaluated on the other. \RMIG (Robust Mutual Information GAP)~\citep{do_theory_2020}, and \DCI (Disentanglement, Completeness and Informativeness)~\citep{eastwood2018framework} adapted for time series are used to evaluate the disentanglement of factors \ie sources. We provide in-depth details of metrics and their implementation in \Cref{appendix:TDS_metric}.

\begin{figure}
    \centering
    \vspace{-0.3cm}
    \includegraphics[clip, trim=0cm 0cm 0.0cm 0.0cm, width=.64\linewidth]{figures/result_synt_2.pdf}
    \includegraphics[width=.35\linewidth]{figures/result_synt_MMC.pdf}
    \vspace{-0.6cm}
    \caption{\textbf{Identifiability Validation.} \MCC for factors $\SynFactors$ on synthetic data; \textbf{Left:} Weak \MCC for TimeCSL, SparseVAE, and TDRL. \textbf{Right:} Baseline comparisons over training steps.}
    \label{fig:validation_mcc_synthetic}
    \vspace{-0.5cm}
\end{figure}

% To measure a decoder's compositionality (\Cref{eq:compositional}), we rely on the compositional contrast regularizer from~\citep{brady2023provably} (we provide more details in \Cref{app:comp_contrast}), which was proven to be zero if a $\gthetaseulhat$ is compositional.
\vspace{-0.1cm}
\paragraph{Contrastive Partial Selective Pairing Pipeline.}\label{exp:partial_selective} Four augmentations were sequentially applied to all contrastive methods' pipeline branches. The parameters from the random search are: \textcolor{blue}{1}) \textbf{Crop and delay:} applied with a $0.5$ probability and a minimum size of $50\%$ of the initial sequence. \textcolor{blue}{2}) \textbf{Cutout or Masking:} time cutout of $5$ steps with a $0.8$ probability. \textcolor{blue}{3}) \textbf{Channel Masks powers:} each time series is randomly masked out with a $0.4$ probability. \textcolor{blue}{4}) \textbf{Gaussian noise:} random Gaussian noise is added to window input $\mathbf{x}$ with a standard deviation form $0.1$ to $0.3$. Further details in \Cref{app:augmentations}.

\textbf{Baselines \& Implementations.} Nonlinear ICA methods are used;$\beta$-VAE, iVAE and TCL which leverage nonstationarity establish identifiability but assumes independent factors, and SlowVAE/SlowVAE which
exploit temporal constraints but assume independent sources. We provide also variant $\beta$-TC/Factor/-VAE such as D3VAE and CDSVAE implemented for time series sequence modeling. We compare \TimeCSL~with downstream task models in energy disaggregation, BertNILM~\citep{yue_bert4nilm_2020} and S2S~\citep{chen_convolutional_2018} as a baseline, for those models, we keep the same configuration as the original implementation. We run experiments with 5 seeds, reporting average results and standard deviations, using 8 NVIDIA A100 GPUs. Hyperparameters and training details are in \Cref{appendix:setup}. % $8 \times$NVIDIA A100 GPUs.
 




% \begin{figure}
%     \centering
%     \vspace{-0.8cm}
%     \includegraphics[width=0.99\textwidth]{figures/mimi.png}
%     \caption{\textbf{Overview of \TimeCSL framework.} In addition to the reconstruction objective, $\mathcal{L}_{cons}$, is minimized on recombined latents. Recombining slots of the inferred latents of $\hrvz$ and $\hprvz$, which can rendered to an OOD sample decoder by $\gthetaseulhat$ generalizing OOD. The $\gthetaseulhat$ is optimized to invert the $\fphihat$. \label{fig:model}}
%     \vspace{-0.4cm}
% \end{figure}

%\paragraph{Implementation Details.} All representation learning methods, including CoST \citep{woo_cost_2022}, employ a Temporal Convolutional Network (TCN) as the backbone encoder, similar to TS2Vec \citep{yue2021ts2vec}, unless specified otherwise. The representation dimensionality is fixed at 320, and standard hyperparameters are used across all datasets: a batch size of 512, learning rate of $1\mathrm{E}{-3}$, momentum of 0.85, and weight decay of $1\mathrm{e}{-5}$ with the Adam optimizer and cosine annealing. For the time domain contrastive loss, a queue size of 256, momentum of 0.999, and temperature of 0.07 are applied. Training lasts for 200 epochs for datasets with fewer than 100,000 samples and 600 iterations otherwise.

\textbf{Results.} \Cref{fig:validation_mcc_synthetic} shows that standard nonlinear ICA models like $\beta$-VAE/C-DSVAE, and SlowVAE struggle with identifiability, while SparseVAE and iVAE perform comparatively better on synthetic data. \TimeCSL with strong sparsity ($\eta=0.01$) achieves the best identifiability. \Cref{fig:scatterplots} provides convincing probes of the compositional generalization consistency condition~ \Cref{coro:compositional}, where minimizing $\gR_{alig}$ and $\gR_{inv}$, both with and without sparsity, aligns with the predictions of \Cref{thm:Elementwise_Identifiability_piecewise}. Slot identifiability improves as reconstruction error decreases, with similar trends observed for $\mathcal{L}_{KL}$. Additionally, \Cref{fig:scatterplots} (Left) illustrates a reduction in compositional error as $\gR_{inv}$ is minimized, confirming the compositional nature of the decoder as predicted by \Cref{coro:compositional}. Empirically, \Cref{tab:results} summarizes the performance of different models as data complexity increases, controlled by correlation levels. The findings show that \TimeCSL surpasses SparseVAE, demonstrating better disentanglement and reconstruction. However, at higher correlation levels, models without tailored designs for identifiability and disentanglement face challenges, underscoring potential limitations in real-world applications.\par

\begin{figure}
    \centering
    \includegraphics[width=.66\linewidth]{figures/sparsity_vs_without.pdf}
    \includegraphics[width=.33\linewidth]{figures/compositionality.pdf}
    \vspace{-9mm}
    \caption{
        \textbf{Experimental validation.}
        \textbf{Left}: As predicted by \Cref{eq:align},  inducing sparsity in models that minimize $\gR_{alig}$ and $\gR_{inv}$ results in representations that are slot-identifiable both in ID and OOD, provided the reconstruction loss $\mathcal{L}_\text{rec}$ (as in \Cref{eq-loss-vae}) is also minimized  (see heat-map). A similar trend is observed for the $\mathcal{L}_\text{KL}$.
        \textbf{Right}: Compositional error~\Cref{eq:compositional_consistency} decreases throughout training, indicating that the decoder is implicitly optimized to be compositional, then validating \Cref{coro:compositional}.
        \vspace{-0.5cm}
    }\label{fig:scatterplots}
\end{figure}




\begin{table}
\centering
\caption{Average performance, considering factors~\RealFactors~with 5 seed on real datasets REFIT and REDD. Metrics reported are: \DCI, \RMIG and \RMSE. Lower values are better for all metrics. ($\downarrow$ lower is better, $\uparrow$ higher is worse  {\small \colorbox{gray!25}{Top-1}, \colorbox{gray!10}{Top-2}}). \label{tab:results}}
\resizebox{\textwidth}{!}{
\begin{tabular}{p{.4cm}p{3.5cm}|rrr|rrr|rrr|rrr}\toprule
\bf Sc. & \bf Methods &\multicolumn{3}{c}{$\boldsymbol\sigma=\infty$} &\multicolumn{3}{c}{ $\boldsymbol\sigma=0.3$} & \multicolumn{3}{c}{ $\boldsymbol\sigma=0.8$} \\ \midrule
& \bf Metrics $\Rightarrow$ & \bf \DCI $\downarrow$ & \bf \RMIG$\downarrow$ & \bf \RMSE $\downarrow$ & \bf \DCI $\downarrow$ & \bf \RMIG$\downarrow$ & \bf \RMSE $\downarrow$ & \bf \DCI $\downarrow$ & \bf \RMIG$\downarrow$ & \bf \RMSE $\downarrow$ \\ \midrule
\multirow{7}{*}{\textbf{\rotatebox{90}{\shortstack{{REFIT}}}}}& \BertNILMcolor BertNILM &- &- &56.4  $\pm$ 2.58 &- &- &70.2  $\pm$  1.45 &- &- &70.92  $\pm$  1.15 \\
& \StoScolor S2S &- &- &54.3  $\pm$ 3.12 &- &- &69.5 $\pm$ 3.56 &- &- &69.95 $\pm$ 3.26 \\
& \Autoformercolor Autoformer &- &- & 49.7  $\pm$  0.81 &- &- & 50.5  $\pm$  2.15 &- &- & 52.95  $\pm$  1.63 \\
& \Informercolor Informer &- &- & 50.3  $\pm$  2.41 &- &- & 53.5  $\pm$  1.98 &- &- & 58.95  $\pm$  1.89 \\
&  \TimesNetcolor TimesNet &- &- & 49.24  $\pm$  2.87 &- &- & 51.10  $\pm$  2.64 &- &- & 54.91  $\pm$  2.31 \\
&  \CoSTcolor CoST & 68.4 $\pm$ 2.41 &0.94 $\pm$ 0.03 &47.7  $\pm$ 1.35 & 73.7 $\pm$ 2.41 & 0.98 $\pm$ 0.27 &53.2  $\pm$ 1.02 & 71.95 $\pm$ 1.63 & 1.00 $\pm$ 0.02 & 58.45 $\pm$ 0.82 \\
&  \SVAEcolor SlowVAE & 78.0 $\pm$ 1.09 &0.94 $\pm$ 0.13 &43.2  $\pm$ 2.23 & 81.0 $\pm$ 1.82 & 0.94 $\pm$ 0.13 & 49.2  $\pm$ 1.13 &79.74 $\pm$ 0.84 &1.07 $\pm$ 0.11 &54.65 $\pm$ 1.43 \\
&  \SVAEHDFcolor SlowVAE+HDF & 79.8  $\pm$  0.10 & 0.64  $\pm$  0.05 &57.2  $\pm$  2.15 & 81.1  $\pm$  0.34 & 0.71  $\pm$  0.14 & 59.3  $\pm$  1.82 &80.37 $\pm$ 0.05 &0.72 $\pm$ 0.03 & 61.64 $\pm$ 1.52 \\
& \TDRLcolor TDRL & 64.85 $\pm$ 1.48 & 0.42 $\pm$ 0.12 & 28.56 $\pm$ 2.15 & 76.23 $\pm$ 1.32 & 0.48 $\pm$ 0.02 & 26.33 $\pm$ 1.97 & 77.13 $\pm$ 1.00 & 0.58 $\pm$ 0.24 & 31.99 $\pm$ 1.64 \\
& \CDSVAEcolor D3VAE & 63.12 $\pm$ 2.84 & 0.40 $\pm$ 0.14 & 42.28 $\pm$ 2.13 & 63.66 $\pm$ 1.31 & 0.51 $\pm$ 0.38 & 46.11 $\pm$ 1.58 & 66.73 $\pm$ 1.88 & 0.67 $\pm$ 0.08 & 50.10 $\pm$ 0.74 \\
&  \CDSVAEcolor C-DSVAE & 72.42 $\pm$ 3.10 & 0.91 $\pm$ 0.15 & 48.6  $\pm$ 2.32 &73.12 $\pm$ 1.43 & 0.95 $\pm$ 0.41 & 52.9  $\pm$ 1.71 & 76.29 $\pm$ 2.04 &1.08 $\pm$ 0.09 & 57.45 $\pm$ 0.81 \\
& \CDSVAEHDFcolor C-DSVAE + HDF  & 67.80 $\pm$ 2.91 & 0.85 $\pm$ 0.14 & 45.45 $\pm$ 2.18 & 68.76 $\pm$ 1.34 & 0.90 $\pm$ 0.39 & 49.69 $\pm$ 1.60 & 71.50 $\pm$ 1.92 & 1.01 $\pm$ 0.08 & 53.85 $\pm$ 0.76 \\
&  \SparseVAEcolor SparseVAE &\second 61.51 $\pm$ 1.31 &\second 0.39 $\pm$ 0.13 &\second 21.01 $\pm$ 1.89 &\second 67.29 $\pm$ 1.17 &\second 0.43 $\pm$ 0.62 &\second 22.71 $\pm$ 1.73 &\second 68.19 $\pm$ 0.88 &\second 0.51 $\pm$ 0.21 &\second 28.91 $\pm$ 1.89 \\
&  \TimeCSLcolor \TimeCSL &\first 59.71 $\pm$ 1.27 &\first 0.36 $\pm$ 0.11 &\first 18.44 $\pm$ 1.84 &\first 65.22 $\pm$ 1.13 &\first 0.41 $\pm$ 0.23 &\first 19.11 $\pm$ 1.69 &\first 66.01 $\pm$ 0.86 &\first 0.48 $\pm$ 0.08 &\first 22.21 $\pm$ 1.41 \\ \cmidrule{2-11}
 & \bf Avg.  & 69.74 $\pm$ 1.95 & 0.80 $\pm$ 0.10 & 47.3 $\pm$ 1.92 & 73.4 $\pm$ 1.22 & 0.90 $\pm$ 0.17 & 52.25 $\pm$ 1.47 & 74.98 $\pm$ 1.38 & 1.00 $\pm$ 0.08 & 54.9 $\pm$ 1.25 \\ \cmidrule{1-11}
\multirow{7}{*}{\textbf{\rotatebox{90}{\shortstack{{REDD}}}}} & \BertNILMcolor BertNILM & - & - & 61.42  $\pm$  3.47 & - & - & 67.61  $\pm$  1.95 & - & - & 69.06  $\pm$  1.43 \\
& \StoScolor S2S & - & - & 59.08  $\pm$  4.15 & - & - & 68.60 $\pm$  3.91 & - & - & 70.68 $\pm$  3.25 \\
& \Autoformercolor Autoformer & - & - & 49.87  $\pm$  0.92 & - & - & 51.53  $\pm$  1.48 & - & - & 51.88  $\pm$  1.34 \\
& \Informercolor Informer &- &- & 54.61  $\pm$  1.41 &- &- & 58.13  $\pm$ 0.67 &- &- & 62.45  $\pm$  1.76 \\
&  \TimesNetcolor TimesNet & - & - & 51.37  $\pm$  2.41 & - & - & 55.35  $\pm$  2.23 & - & - & 58.47  $\pm$  2.21 \\
& \CoSTcolor CoST & 62.60 $\pm$ 2.20 & 0.86 $\pm$ 0.03 & 43.53 $\pm$ 1.23 & 67.51 $\pm$ 2.11 & 0.89 $\pm$ 0.25 & 48.71 $\pm$ 0.94 & 65.98 $\pm$ 1.50 & 0.92 $\pm$ 0.02 & 53.32 $\pm$ 0.75 \\
& \SVAEcolor SlowVAE & 71.14 $\pm$ 0.96 & 0.86 $\pm$ 0.12 & 39.46 $\pm$ 2.05 & 74.34 $\pm$ 1.60 & 0.86 $\pm$ 0.12 & 45.02 $\pm$ 1.04 & 73.19 $\pm$ 0.77 & 0.98 $\pm$ 0.10 & 49.94 $\pm$ 1.31 \\
& \SVAEHDFcolor SlowVAE+HDF & 73.12 $\pm$ 0.09 & 0.59 $\pm$ 0.05 & 52.34 $\pm$ 1.97 & 74.40 $\pm$ 0.31 & 0.65 $\pm$ 0.13 & 54.48 $\pm$ 1.67 & 73.75 $\pm$ 0.05 & 0.66 $\pm$ 0.03 & 56.28 $\pm$ 1.40 \\
& \TDRLcolor TDRL & 59.39 $\pm$ 1.31 & 0.38 $\pm$ 0.11 & 26.12 $\pm$ 1.97 & 69.82 $\pm$ 1.19 & \second 0.44 $\pm$ 0.02 & 24.10 $\pm$ 1.78 & 70.82 $\pm$ 0.91 & 0.53 $\pm$ 0.22 & 29.27 $\pm$ 1.51 \\
& \CDSVAEcolor D3VAE & 59.39 $\pm$ 2.56 & 0.74 $\pm$ 0.13 & 39.56 $\pm$ 1.92 & 59.65 $\pm$ 1.17 & 0.78 $\pm$ 0.34 & 43.13 $\pm$ 1.42 & 62.62 $\pm$ 1.69 & 0.89 $\pm$ 0.07 & 47.07 $\pm$ 0.66 \\
& \CDSVAEcolor C-DSVAE & 66.44 $\pm$ 2.84 & 0.83 $\pm$ 0.14 & 44.51 $\pm$ 2.13 & 67.06 $\pm$ 1.31 & 0.87 $\pm$ 0.38 & 48.48 $\pm$ 1.58 & 70.24 $\pm$ 1.88 & 0.99 $\pm$ 0.08 & 52.74 $\pm$ 0.74 \\
& \CDSVAEHDFcolor C-DSVAE + HDF & 62.20 $\pm$ 2.67 & 0.78 $\pm$ 0.13 & 41.65 $\pm$ 2.01 & 63.23 $\pm$ 1.24 & 0.83 $\pm$ 0.36 & 45.71 $\pm$ 1.48 & 65.73 $\pm$ 1.77 & 0.93 $\pm$ 0.07 & 49.54 $\pm$ 0.70 \\
& \SparseVAEcolor SparseVAE &\second 56.39 $\pm$ 1.21 &\second 0.36 $\pm$ 0.12 &\second 19.21 $\pm$ 1.74 &\second 61.60 $\pm$ 1.07 & 0.45 $\pm$ 0.57 &\second 20.81 $\pm$ 1.60 & \second 62.65 $\pm$ 0.81 &\second 0.47 $\pm$ 0.19 &\second 26.42 $\pm$ 1.74 \\
& \TimeCSLcolor \TimeCSL &\first 54.74 $\pm$ 1.17 &\first 0.33 $\pm$ 0.10 &\first 16.93 $\pm$ 1.70 &\first 60.10 $\pm$ 1.04 &\first 0.38 $\pm$ 0.21 &\first 17.50 $\pm$ 1.56 &\first 60.31 $\pm$ 0.79 &\first 0.44 $\pm$ 0.07 &\first 20.39 $\pm$ 1.30 \\ \cmidrule{2-11}
& \bf Avg. & 69.25 $\pm$ 1.87 & 0.67 $\pm$ 0.09 & 47.4 $\pm$ 1.83 & 74.2 $\pm$ 1.36 & 0.73 $\pm$ 0.10 & 53.16 $\pm$ 1.55 & 75.55 $\pm$ 1.23 & 0.80 $\pm$ 0.08 & 56.31 $\pm$ 1.48 \\
%\worstResult
\bottomrule
\end{tabular}
}
\vspace{-0.4cm}
\label{tab:results}
\end{table}


% \begin{wraptable}[14]{r}{0.5\textwidth}
% \vspace{-0.7cm}
% \caption{Average \RSquare, and weaker/strong \MCC scores on UK-DALE dataset with factors~\RealFactors. ($\downarrow$ lower is better, $\uparrow$ higher is worse  {\small \colorbox{gray!25}{Top-1}, \colorbox{gray!10}{Top-2}}).}
% \resizebox{0.5\textwidth}{!}{
% %\vspace{-1.8cm}
% \centering
% \begin{tabular}{llcccc} % Added column for strong MCC
% \toprule
% \bf Method & \bf Activation & \bf Sparsity ($\lambda$) & \bf  $\boldsymbol{R^{2}}$ $\uparrow$ & \bf \Delta \MCC $\downarrow$ & \bf strong \MCC $\uparrow$ \\ % Added strong MCC
% \toprule
% CoST & ReLU &  - & 0.165 & 0.405 & 0.195 \\ % Inversely proportional to weak MCC
% \midrule
% RNN-VAE (baseline) & LeakyReLU & - & 0.065 & 0.360 & 0.540 \\ % Inversely proportional to weak MCC
% RNN-VAE+TimeCSL & LeakyReLU & - & \underline{0.169} & 0.362 & 0.538 \\ % Inversely proportional to weak MCC
% C-DSVAE & ReLU & - & 0.127 & 0.415 & 0.585 \\ % Inversely proportional to weak MCC
% C-DSVAE+TimeCSL & ReLU & - & \underline{0.167} & 0.411 & 0.689 \\ % Inversely proportional to weak MCC
% SlowVAE & LeakyReLU & - & 0.263 & 0.409 & 0.691 \\ % Inversely proportional to weak MCC
% SlowVAE+TimeCSL & LeakyReLU & - & \underline{0.272} & 0.413 & 0.587 \\ % Inversely proportional to weak MCC
% DIOSC & Softmax & - & 0.271 & 0.356 & 0.744 \\ % Inversely 
% TDRL & LeakyReLU & - & 0.271 & 0.356 & 0.804 \\ % Inversely proportional to weak MCC
% TimeCSL w/o Sparsity & Softmax & 0 & 0.292 & 0.371 & 0.749 \\ % Inversely proportional to weak MCC
% TimeCSL & ReLU & 0.8 & 0.268 & 0.362 & 0.838 \\ % Inversely proportional to weak MCC
% TimeCSL ReLU & ReLU & 1 & \bf \first 0.305 & 0.367 & 0.813 \\ % Inversely proportional to weak MCC
% TimeCSL+self-attention & Softmax & 1 & \bf \first 0.305 & 0.367 & 0.873 \\ % Inversely proportional to weak MCC
% \hline
% \end{tabular}}
% \label{tab:impact_sparsity}
% \end{wraptable}
% \begin{wraptable}[14]{r}{0.5\textwidth}
% \vspace{-0.7cm}
% \caption{Average \RSquare, and weaker/strong \MCC scores on UK-DALE dataset with factors~\RealFactors. ($\downarrow$ lower is better, $\uparrow$ higher is worse  {\small \colorbox{gray!25}{Top-1}, \colorbox{gray!10}{Top-2}}).}
% \resizebox{0.5\textwidth}{!}{
% %\vspace{-1.8cm}
% \centering
% \begin{tabular}{llcccc} % Added column for weak MCC
% \toprule
% \bf Method & \bf Activation & \bf  $\boldsymbol{R^{2}}$ $\uparrow$ & \bf \Delta \MCC $\downarrow$ & \bf strong \MCC $\uparrow$ & \bf weak \MCC $\uparrow$ \\ % Added weak MCC column
% \toprule
% CoST & ReLU & 0.165 & 0.405 & 0.595 & 0.190 \\ % weak MCC calculated as abs(0.405 - 0.595)
% \midrule
% RNN-VAE (baseline) & LeakyReLU & 0.065 & 0.460 & 0.540 & 0.080 \\ % weak MCC calculated as abs(0.460 - 0.540)
% RNN-VAE+TimeCSL & LeakyReLU & \underline{0.169} & 0.362 & 0.638 & 0.276 \\ % weak MCC calculated as abs(0.362 - 0.638)
% C-DSVAE & ReLU & 0.127 & 0.415 & 0.585 & 0.170 \\ % weak MCC calculated as abs(0.415 - 0.585)
% C-DSVAE+TimeCSL & ReLU & \underline{0.167} & 0.411 & 0.689 & 0.278 \\ % weak MCC calculated as abs(0.411 - 0.689)
% SlowVAE & LeakyReLU & 0.263 & 0.409 & 0.591 & 0.182 \\ % weak MCC calculated as abs(0.409 - 0.591)
% SlowVAE+TimeCSL & LeakyReLU & \underline{0.272} & 0.413 & 0.587 & 0.174 \\ % weak MCC calculated as abs(0.413 - 0.587)
% DIOSC & Softmax & 0.271 & 0.356 & 0.644 & 0.288 \\ % weak MCC calculated as abs(0.356 - 0.644)
% TDRL & LeakyReLU & 0.271 & 0.356 & 0.804 & 0.448 \\ % weak MCC calculated as abs(0.356 - 0.804)
% TimeCSL w/o Sparsity & Softmax & 0.292 & 0.371 & 0.629 & 0.258 \\ % weak MCC calculated as abs(0.371 - 0.629)
% TimeCSL & ReLU & 0.268 & 0.362 & 0.638 & 0.276 \\ % weak MCC calculated as abs(0.362 - 0.638)
% TimeCSL ReLU & ReLU & \bf 0.305 & 0.367 & 0.633 & 0.266 \\ % weak MCC calculated as abs(0.367 - 0.633)
% TimeCSL+self-attention & Softmax & 0.205 & 0.267 & 0.373 & 0.606 \\ % weak MCC calculated as abs(0.367 - 0.873)
% \hline
% \end{tabular}}
% \label{tab:impact_sparsity}
% \end{wraptable}
%
\newpage
\vspace{-0.4cm}
\subsection{Ablation Studies and Discussion}\label{sec:ablation}
\vspace{-0.2cm}

\begin{wraptable}[14]{r}{0.5\textwidth}
\vspace{-0.8cm}
\caption{Average \RSquare, \RMIG and weaker/strong \MCC scores on UK-DALE dataset with factors~\RealFactors. ($\downarrow$ lower is better, $\uparrow$ higher is better  {\small \colorbox{gray!25}{Top-1}, \colorbox{gray!10}{Top-2}}).$^\dagger$ indicates implemented.}
\resizebox{0.5\textwidth}{!}{
\centering
\begin{tabular}{llcccc}
\toprule
\bf Method & \bf Activation & \bf $\boldsymbol{R^{2}}$ $\uparrow$ &  \bf $\RMIG$ $\downarrow$ & \bf weak \MCC $\uparrow$ & \textcolor{orange!80!black}{\bf strong \MCC} $\uparrow$ \\
\toprule
\TimeCSLPcolor CoST & ReLU & 0.165 & 0.405 & 0.395 &  -0.010 \\
\midrule
\TimeCSLPcolor RNN-VAE (baseline) & LeakyReLU & 0.065 & 0.660 & 0.340 & 0.080 \\
\TimeCSLPcolor RNN-VAE+TimeCSL & LeakyReLU & {0.169} & 0.562 & 0.400 & 0.038 \\
\TimeCSLPcolor C-DSVAE & ReLU & 0.127 & 0.415 & 0.685 & 0.070 \\
\TimeCSLPcolor C-DSVAE+TimeCSL & ReLU & {0.167} & 0.511 & 0.578 & 0.167 \\
\TimeCSLPcolor SlowVAE & LeakyReLU & 0.263 & 0.860 & 0.671 & 0.082 \\
\TimeCSLPcolor SlowVAE+TimeCSL & LeakyReLU & {0.272} & 0.560 & 0.387 & 0.074 \\
\TimeCSLPcolor DIOSC & Softmax & 0.280 & 0.368 & 0.562 & 0.194 \\
\TimeCSLPcolor D3VAE (Diffusion) & Softmax & 0.271 & 0.791 & 0.544 & 0.188 \\
\TimeCSLPcolor D3VAE+TimeCSL (Diffusion) & Softmax & 0.285 & 0.682 & 0.573 & 0.198 \\
\TimeCSLPcolor iVAE & LeakyReLU & 0.230 & 0.408 & 0.479 & 0.177 \\
\TimeCSLPcolor TDRL &LeakyReLU & 0.223 & 0.380 & 0.464 & 0.172 \\
\TimeCSLPcolor TCL & LeakyReLU & 0.115 & 0.748 & 0.448 & 0.165 \\
\TimeCSLPcolor LEAP & LeakyReLU & 0.138 & 0.340 & 0.538 & 0.198 \\
\TimeCSLcolor TimeCSL $\eta=0.001$ & ReLU & \cellcolor{gray!10} 0.292 & \cellcolor{gray!10} 0.330 & \cellcolor{gray!10} 0.629 & \cellcolor{gray!10} 0.258 \\
\TimeCSLcolor TimeCSL$^\dagger$+self-attention & Softmax & 0.231 & 0.478 & \fisrt 0.373 & \fisrt 0.106 \\
%TimeCSL & ReLU & 0.268 & 0.362 & 0.738 & 0.376 \\
\TimeCSLcolor TimeCSL $^\dagger$ $\eta=0.01$ & ReLU & \bf \cellcolor{gray!25} 0.305 & \bf \cellcolor{gray!25} 0.367 & \bf \cellcolor{gray!25} 0.633 & \bf \cellcolor{gray!25} 0.266 \\
\hline
\end{tabular}
}
\label{tab:impact_sparsity}
\end{wraptable}


% \begin{wraptable}[14]{r}{0.5\textwidth}
% \vspace{-0.8cm}
% \caption{Average \RSquare, \RMIG and weaker/strong \MCC scores on UK-DALE dataset with factors~\RealFactors. ($\downarrow$ lower is better, $\uparrow$ higher is better  {\small \colorbox{gray!25}{Top-1}, \colorbox{gray!10}{Top-2}}).$^\dagger$ indicates implemented.}
% \resizebox{0.5\textwidth}{!}{
% \centering
% \begin{tabular}{llcccc}
% \toprule
% \bf Method & \bf Activation & \bf $\boldsymbol{R^{2}}$ $\uparrow$ & \bf $\Delta \MCC$ $\downarrow$ & \bf weak \MCC $\uparrow$ & \bf strong \MCC $\uparrow$ \\
% \toprule
% \CoSTcolor CoST & ReLU & 0.165 & 0.405 & 0.395 &  -0.010 \\
% \midrule
% \SVAEcolor RNN-VAE (baseline) & LeakyReLU & 0.065 & 0.460 & 0.340 & 0.080 \\
% \SVAEcolor RNN-VAE+TimeCSL & LeakyReLU & {0.169} & 0.462 & 0.400 & 0.038 \\
% \CDSVAEcolor C-DSVAE & ReLU & 0.127 & 0.415 & 0.485 & 0.070 \\
% \CDSVAEHDFcolor C-DSVAE+TimeCSL & ReLU & {0.167} & 0.411 & 0.578 & 0.167 \\
% \SVAEcolor SlowVAE & LeakyReLU & 0.263 & 0.409 & 0.491 & 0.082 \\
% \SVAEcolor SlowVAE+TimeCSL & LeakyReLU & {0.272} & 0.413 & 0.387 & 0.074 \\
% \DIOSCcolor DIOSC & Softmax & 0.280 & 0.368 & 0.562 & 0.194 \\
% \DVAE D3VAE (Diffusion) & Softmax & 0.271 & 0.356 & 0.544 & 0.188 \\
% \DVAEcolor D3VAE+TimeCSL (Diffusion) & Softmax & 0.285 & 0.374 & 0.573 & 0.198 \\
% %TDRL & LeakyReLU & 0.271 & 0.356 & 0.564 & 0.208 \\
% iVAE & LeakyReLU & 0.230 & 0.303 & 0.479 & 0.177 \\
% TDRL &LeakyReLU & 0.223 & 0.294 & 0.464 & 0.172 \\
% TCL & LeakyReLU & 0.115 & 0.283 & 0.448 & 0.165 \\
% LEAP & LeakyReLU & 0.138 & 0.340 & 0.538 & 0.198 \\
% \TimeCSLcolor TimeCSL $\eta=0.001$ & ReLU & \cellcolor{gray!10} 0.292 & \cellcolor{gray!10} 0.371 & \cellcolor{gray!10} 0.629 & \cellcolor{gray!10} 0.258 \\
% \TimeCSLcolor $^\dagger$+self-attention & Softmax & 0.205 & 0.267 & \fisrt 0.373 & \fisrt 0.106 \\
% %TimeCSL & ReLU & 0.268 & 0.362 & 0.738 & 0.376 \\
% \TimeCSLcolor TimeCSL$^\dagger$ $\eta=0.01$ & ReLU & \bf \cellcolor{gray!25} 0.305 & \bf \cellcolor{gray!25} 0.367 & \bf \cellcolor{gray!25} 0.633 & \bf \cellcolor{gray!25} 0.266 \\
% \hline
% \end{tabular}
% }
% \label{tab:impact_sparsity}
% \end{wraptable}

% First table:
% \begin{wraptable}[14]{r}{0.5\textwidth}
% \vspace{-0.8cm}
% \caption{Average \RSquare, and weaker/strong \MCC scores on UK-DALE dataset with factors~\RealFactors. ($\downarrow$ lower is better, $\uparrow$ higher is better  {\small \colorbox{gray!25}{Top-1}, \colorbox{gray!10}{Top-2}}).}
% \resizebox{0.5\textwidth}{!}{
% \centering
% \begin{tabular}{llcccc}
% \toprule
% \bf Method & \bf Activation & \bf $\boldsymbol{R^{2}}$ $\uparrow$ & \bf $\Delta \MCC$ $\downarrow$ & \bf weak \MCC $\uparrow$ & \bf strong \MCC $\uparrow$ \\
% \toprule
% CoST & ReLU & 0.165 & 0.405 & 0.395 &  -0.010 \\
% \midrule
% RNN-VAE (baseline) & LeakyReLU & 0.065 & 0.460 & 0.340 & 0.080 \\
% RNN-VAE+TimeCSL & LeakyReLU & \underline{0.169} & 0.462 & 0.400 & 0.038 \\
% C-DSVAE & ReLU & 0.127 & 0.415 & 0.485 & 0.070 \\
% C-DSVAE+TimeCSL & ReLU & \underline{0.167} & 0.411 & 0.578 & 0.167 \\
% SlowVAE & LeakyReLU & 0.263 & 0.409 & 0.491 & 0.082 \\
% SlowVAE+TimeCSL & LeakyReLU & \underline{0.272} & 0.413 & 0.387 & 0.074 \\
% DIOSC & Softmax & 0.271 & 0.356 & 0.544 & 0.188 \\
% D3VAE (Diffusion) & Softmax & 0.271 & 0.356 & 0.544 & 0.188 \\
% D3VAE+TimeCSL (Diffusion) & Softmax & 0.271 & 0.356 & 0.544 & 0.188 \\
% %TDRL & LeakyReLU & 0.271 & 0.356 & 0.564 & 0.208 \\
% iVAE & LeakyReLU & 0.230 & 0.303 & 0.479 & 0.177 \\
% TDRL &LeakyReLU & 0.223 & 0.294 & 0.464 & 0.172 \\
% TCL & LeakyReLU & 0.115 & 0.283 & 0.448 & 0.165 \\
% TimeCSL $\eta=0.001$ & ReLU & \cellcolor{gray!10} 0.292 & \cellcolor{gray!10} 0.371 & \cellcolor{gray!10} 0.629 & \cellcolor{gray!10} 0.258 \\
% TimeCSL+self-attention & Softmax & 0.205 & 0.267 & \fisrt 0.373 & \fisrt 0.106 \\
% %TimeCSL & ReLU & 0.268 & 0.362 & 0.738 & 0.376 \\
% TimeCSL $\eta=0.01$ & ReLU & \bf \cellcolor{gray!25} 0.305 & \bf \cellcolor{gray!25} 0.367 & \bf \cellcolor{gray!25} 0.633 & \bf \cellcolor{gray!25} 0.266 \\
% \hline
% \end{tabular}
% }
% \label{tab:impact_sparsity}
% \end{wraptable}

\paragraph{\bf When and how performing disentanglement?}
In~\Cref{tab:impact_sparsity}, we use as  \TimeCSL~regularizer and we train models only on (REFIT+REDD) and tested on possible OOD dataset \ie UKDALE, and we explore its application with alternative structures tailored for time series with analysis impact of nonlieanrtité of the dedcoder (\Cref{assump:injective_piecewise} not hold), particularly those residual in Difussion based VAE model (D3VAE). We see that the model it generalize after incorporating \TimeCSL~with another method slightly enhances results, as the alternative regularizes assumes independent factorization, potentially compromising the relaxing effect. Secondly, \TimeCSL shows improved performance as sparsity increases, with \RSquare positively correlating with performance. \RMIG further indicates that integrating attention with \TimeCSL~yields well-disentangled representations. The attention mechanism notably enhances overall model performance, including identifiability metrics, suggesting potential for guaranteeing identifiable disentangled representations. 

%Secondly, \TimeCSL~demonstrates improved performance with increasing values of the sparsity, and the \RSquare~correlates positively with performance, while \RMIG~suggests that using~\TimeCSL~with attention leads to well-disentangled representations. Notably, the attention mechanism proves efficient by enhancing the overall model performance even that thos identifiability metrcis. This sugget futher anaylisi weather attention mechnisme can guarentee an  identifiable disentnagled representation. We Give futher results in \Cref{app:experiments}.\par



% \begin{wraptable}[13]{r}{0.5\textwidth}
% \vspace{-0.8cm}
% \caption{\RMIG, \NRMSE, \RSquare, and scaled \MCC scores for Variants of \TimeCSL~with and without Sparsity Level on Uk-dale dataset. ($\downarrow$ lower is better, $\uparrow$ higher is worse  {\small \colorbox{gray!25}{Top-1}, \colorbox{gray!10}{Top-2}}.}
% \resizebox{0.5\textwidth}{!}{
% %\vspace{-1.8cm}
% \centering
% \begin{tabular}{llcc} % Removed \NRMSE and \\RMIGcolumns
% \toprule
% \bf Method & \bf Sparsity ($\lambda$) & \bf  $\boldsymbol{R^{2}}$ $\uparrow$ & \bf weak-\MCC $\downarrow$ \\ % Removed \NRMSE and \RMIG
% \toprule
% CoST &  - & 0.165 & 0.405 \\ % Inverted value
% \midrule
% RNN-VAE (baseline) & - & 0.065 & 0.460 \\ % Inverted value
% RNN-VAE+\TimeCSL & - & \underline{0.169} & 0.362 \\ % Inverted value
% C-DSVAE & - & 0.127 & 0.415 \\ % Inverted value
% C-DSVAE+\TimeCSL & - & \underline{0.167} & 0.411 \\ % Inverted value
% SlowVAE & - & 0.263 & 0.409 \\ % Inverted value
% SlowVAE+\TimeCSL & - & \underline{0.272} & 0.413 \\ % Inverted value
% DIOSC & - & 0.271 & 0.356 \\ % Inverted value
% \TimeCSL w/o Sparsity   & 0 & 0.292 & 0.371 \\ % Inverted value
% \TimeCSL & 0.8 & 0.268 & 0.362 \\ % Inverted value
% \TimeCSL & 1 & \bf \first 0.305 & 0.367 \\ % Inverted value
% \hline
% \end{tabular}}
% \label{tab:impact_sparsity}
% \end{wraptable}
\vspace{-0.3cm}
\paragraph{\bf Is that sparsity enough for Robustness, is donwstream task ?}
We provide evidence that \TimeCSL exhibits robustness across different correlation scenarios as illustrated in \Cref{fig:DIOSC_preserves_its_robustness}. In addition, we conduct experiments using different sate of the art architecture for time series representation. The results presented in, highlight that \TimeCSL  with sparsity $\eta=0.1$ consistently than \TimeCSL  with lower sparsity $\eta=0.01$ outperforms the competing baseline across all three settings, reaffirming its effectiveness and versatility in diverse scenarios where data exibit a strong correlation. Due to limited space, additional results are available in \Cref{app:impact_about_sparsity}.


\begin{figure*}
    \centering
    \vspace{-0.30cm}
    \includegraphics[clip, trim=0cm 4cm .0cm .0cm, width=\textwidth]{figures/performance.pdf}
    \vspace{-0.5cm}
    \caption{Relative~RMSE (\%) improvement over baseline BertNILM~\citealp{yue_bert4nilm_2020} for~\RealFactors~devices, with the amount of labeled training data as a variable parameter.}\label{fig:DIOSC_preserves_its_robustness}
    \vspace{-0.5cm}
\end{figure*}

\vspace{-0.2cm}
\section{Conclusion}
\vspace{-0.1cm}
In this work, we delved into the effectiveness of contrastive sparsity-inducing techniques in attaining both identifiability and generalization. We showcased that disentangled representations, complemented by sparse-inducing methods through contrastive learning, improve generalization, particularly when the downstream task can be tackled using only a portion of the underlying factors of variation. Looking ahead, future investigations could explore leveraging such meaningful representations for downstream tasks, as evidenced by our primary experiments demonstrating performance enhancement. Furthermore, we posit that such representations could prove efficient in scenarios characterized by limited labeled data for time series representation. We have demonstrated generalization through compositional representations. We built on the literature in generative models and non-linear ICA~\citep{kivva2022identifiability, hyvarinen2019nonlinear, lachapelle2022disentanglement} and made two key assumptions: i) partial sufficiency holds, which enables sparsity through contrastive learning, and ii) the decoder $\gthetaseul$ is injective. Our results are a step toward identifiability and disentanglement in time series models.

%We propose that future research should investigate the possibility of relaxing these assumptions. %Future research should investigate the possibility of relaxing these assumptions. \par

\textbf{Limitations \& Future Work}
We recognize that our assumptions regarding time series representation and source separation have potential for extension. Our piecewise injectivity assumption (\Cref{assump:injective_piecewise}), although potentially violated in practice, can be revised to take account of structures such as attention mechanisms or instance normalization. The Sufficient Partial Pairing assumption (\Cref{assump:sufficient_partial_pairing}) requires sufficient data, and as discussed  in \Cref{sec:implementation_loss}, it can be relaxed to group factors as well. 

%While our assumption of injectivity may be violated in practical scenarios, where more complex structures like attention mechanisms or instance normalization might be needed, this assumption could be revisited and adjusted accordingly.
%A discussion of broader impacts is included in \cref{app:broader Impact}.

% \paragraph{Experimental Platform.} 

% \paragraph{Reproducibility Statement.}
% Detailed versions of all assumptions and definitions, as well as the full the results are included in \Cref{app:augmentations}. We attach our codebase to facilitate the reproduction of our experiments. All hyperparameters, model architectures, training regimes, datasets, and evaluation metrics are provided in the codebase. Explanations for design choices are given in \Cref{sec:experiments} in the main text and \Cref{app:experiments}. The implementation of the objective function loss is detailed in \Cref{sec:proposedmethod}.


% \section{Broader Impact}\label{app:broader Impact}
% Our proposed method enables effective representation learning for time series data related to energy load, offering broad applicability across various downstream tasks. In this context, we showcase its efficacy in scenarios characterized by strong correlations. The scalability of our approach, particularly when applied to scaled versions featuring a large number of appliances, facilitates its generalization across domains, establishing foundational models for energy disaggregation. The potential societal benefits, such as enabling household consumption determination, are particularly notable within the context of smart grid systems. This capability not only aids in energy management but also provides users with valuable feedback regarding optimal utilization during off-peak hours, thereby optimizing energy consumption and consequently reducing carbon footprint. Such contributions underscore the significant societal and environmental advantages afforded by AI-driven models.




% \input{sections/1_intro}
% \input{sections/2_background}
% \input{sections/3_proposed_methods}
% \input{sections/2_related_work}
% \input{sections/4_1_experiments}
% \input{sections/4_2_ablation_studies}
% \input{sections/5_conclusion}
% In an attempt to encourage standardized notation, we have included the
% notation file from the textbook, \textit{Deep Learning}
% \cite{goodfellow2016deep} available at
% \url{https://github.com/goodfeli/dlbook_notation/}.  Use of this style
% is not required and can be disabled by commenting out
% \texttt{math\_commands.tex}.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}
\appendix
\newpage
\vspace{-1cm}
\part{\Large Supplementary Material:  } % Start the appendix part
To ensure a comprehensive understanding of our paper and to support reproducibility and reliability, we present additional results and provide complete proofs for the theorems articulated in the main paper. This supplementary material is meticulously organized as follows:
\parttoc % Insert the appendix TOC

\begin{figure}[H]
    \centering
    \vspace{-0.30cm}
    \includegraphics[width=\textwidth]{figures/cover_TimeCSL.pdf}
    \vspace{-1cm}
    \caption{Recovered latent spaces for 4 runs of \TimeCSL on REDD dataset \#1 with 5 latents ($n=5, d=16$).}
    \vspace{-0.5cm}
\end{figure}

\section{Definitions, Propositions, and Proofs Analysis}\label{app:app_def_theorems_proofs}
In this section, we detail the contributions of the paper, including all the details. Although there is no change in their contents, the formulation of some definitions and theorems are slightly altered here to be more precise and cover edge cases omitted in the main text. Hence, the numbering of the restated elements is reminiscent of that used in the main text.


% \subsection{Extended Related Work}\label{sec:related_work}
% \paragraph{On the Nonlinear ICA for Time Series Representation Learning.} 
% Temporal structure and nonstationarities were recently used to achieve identifiability in nonlinear ICA. \citep{TCL2016} propose Time-contrastive learning (TCL) used the independent sources assumption and leveraged sufficient variability in variance terms of different data segments. Permutation-based contrastive (PCL)  proposed a learning framework which discriminates between true independent sources and permuted ones, and identifiable under the uniformly dependent assumption. Their work assumes that the conditional distribution is within exponential families to achieve the identifiability of the latent space. i-VAE~\citep{iVAEkhemakhem20a} introduced VAEs to approximate the true joint distribution over observed and auxiliary nonstationary regimes. The most recent literature on non linear ICA is LEAPS~\citep{yao2021learning} from the lens of causal discovery for motion, proposed a non prametric condition. The main limitations of LEAPS lie on no instantaneous causal influence between latent causal processes, and causal influences do not change across regimes. Both of them may not be true for some specific types of temporal data. The closest work to ours includes~\citep{lachapelle2021disentanglement, klindt2020towards, ahuja_interventional_2023} which require the underlying sources to be mutually independent for identifiability.  Our work extends the theories for identifiability of sources without necessarry assuming independence.
% More specifically, we impose no assumptions on $p(\rvz)$ beyond its definition in \Cref{eq:generative_model}. We note that GMM can approximate complex distribution as already knowns as results ~\citep{nguyen2019approximations}. The assumption of a GMM prior can be replaced with more
% general exponential family mixtures\citep{kivva2022identifiability}, as long as the prior is analytic and the family is closed under affine transformations. Recent works have also leveraged assumptions on $\Jb\gthetaseul$ such as orthogonality~\citep{gresele2021independent,zheng2022on}, or a \textit{fixed} sparsity structure~\citep{moran2022identifiable,lachapelle2022disentanglement}.
% While the latter on sparsity relates to our definition in this paper, 
% we crucially allow the sparsity pattern  on $\Jb\gthetaseul$ to vary with $\rvz$ which is in line with the basic notion of time series, i.e. sources are not fixed in time, and impose sparsity with respect to block latent rather than individual latent. Secondly, existing work typically aims to identify individual latent components $\rvz_k$ up to permutations (or linear transformations). However, this is inappropriate for time series representation learning, where we aim to capture and isolate the subsets of latent corresponding to each source in well-defined slots.

% \paragraph{Time Series Representation with Out-Of-Distribution.} Traditional methods for time series disentanglement often emphasize enforcing statistical independence among representation dimensions \citep{do_theory_2021, klindt_towards_2021}, even when dealing with highly correlated data. In representation learning, identifiability is mostly examined through nonlinear ICA~\citep{jutten2010nonlinear} to find independent latent, usually allowing permutations or element-wise transformations. However, a significant finding shows that without additional assumptions, achieving this is fundamentally impossible with i.i.d. data. There has been an exploration of using auxiliary information to improve identifiability, moving away from the assumption of statistical independence~\citep{trauble_disentangled_nodate, roth_disentanglement_2023, oublal2024disentangling}. Another study by \citep{wang_desiderata_2022} proposes support factorization for disentanglement from a causal perspective, incorporating a Hausdorff objective akin to \citep{roth_disentanglement_2023}. 
% %In our unique approach, we tackle the time series disentanglement and identifiability without explicit auxiliary variables or prior models.  Instead, we achieve pairwise factorized support through contrastive learning, departing from the traditional independence assumption. 
% Recent contributions~\citep{wang_desiderata_2022-2, roth_disentanglement_2023} seek to alleviate this assumption, yet remain disconnected from observational data and grapple with numerical stability. This method pioneers disentanglement in correlated time series by emphasizing independence-of-support through contrastive learning during training. Exploring ways to enhance representation learning for time series has been the focus of recent studies such CoTS~\citep{woo_cost_2022}. However, achieving an informative and disentangled representation remains an open and challenging question. Existing models, like RNNVAE~\citep{chung_recurrent_2015} for sequential data and D3VAE~\citep{li_generative_2023}, or for general sequential data (e.g. audio, video) such CDSVAE~\citep{bai2021contrastively}~and SlowVAE~\citep{klindt2020towards}, assume statistically independent attributes. In recent efforts to address challenges in long-term dependency modeling for time series data, models like TimesNet~\citep{timesnet}, Autoformer~\citep{autoformer}, FEDformer~\citep{fedformer} have emerged.\par



\subsection{Generalization, Compositionality and irreducibility assumptions~}\label{app:app_comp_irr}

\paragraph{Compositional contrast}\label{app:comp_contrast}
In recent work on compositionality \citep{assouel2022objectcentric, zhao2022toward, KurthNelson2022ReplayAC} and its importance in learning models that can generalize well to novel situations, the concept of \emph{compositional contrast} has emerged as a powerful tool for evaluating how well a model separates information into independent, non-interacting components. This concept is particularly relevant in the context of time series analysis or image generation, where the model’s ability to decompose an input into distinct parts, or "slots," can significantly impact the quality of predictions and interpretability. Compositionality ensures that each slot, or latent variable, corresponds to a specific factor or component of the data. In highly compositional models, these components do not interact with each other—each one affects a distinct aspect of the output. In contrast, non-compositional models tend to mix these components, making it harder to disentangle the factors and interpret the model’s output. Evaluating how well a model adheres to compositionality principles can be challenging, as it requires quantifying how independent the slots are in their contribution to the final output. To address this, \citet{brady2023provably} introduced the notion of \emph{compositional contrast}, which measures the extent to which the model’s latent variables (slots) interact when producing the final output. This measure is particularly useful in determining whether a decoder is truly compositional—that is, whether each slot contributes independently of the others, or if there are unwanted interactions between them. Before we introduce the formal definition of compositional contrast, it is important to understand the underlying principle. The intuition behind the compositional contrast is that if a model is fully compositional, each slot should affect only a specific subset of the output (e.g., one region of an image or one time series variable) and have no influence on other components. Conversely, if the model is not compositional, changes in one slot will influence multiple components of the output simultaneously, indicating that the slots are not independent. The compositional contrast function captures this idea by calculating how much the gradients of each slot (with respect to the model’s output) overlap. If the gradients of different slots with respect to the same output component are non-zero, this suggests interaction between the slots, indicating a lack of compositionality. The function sums these interactions across all slots and output components, providing a single value that quantifies the degree of interaction. A lower compositional contrast value suggests higher compositionality, while a higher value indicates more interaction between slots. Formally, the compositional contrast is defined as follows:
\begin{definition}[Compositional Contrast] \label{def:compositional_contrast}
    Let $\gthetaseul: \gZ\to\gX$ be differentiable. The \emph{compositional contrast} of $\gthetaseul$ at $\rvz$ is
    \begin{equation} \label{eq:compositional_contrast}
        {C_{\mathrm{comp}}}(\gthetaseul, \rvz) = \sum\limits_{n=1}^{N} 
        \sum\limits_{\substack{k=1}}^{K}
        \sum\limits_{j=k+1}^{K}
        \left\|\frac{\partial \gthetaseul_{n}}{\partial \rvz_{k}}(\rvz)\right\|
        \left\|\frac{\partial \gthetaseul_{n}}{\partial \rvz_{j}}(\rvz)\right\|
        \,.
    \end{equation}
\end{definition}
This contrast function was proven to be zero if and only if $\gthetaseul$ is compositional according to \Cref{coro:compositional}. The function can be understood as computing each pairwise product of the (L2) norms for each pixel's gradients with respect to any two distinct slots $k \neq j$ and taking the sum. This quantity is non-negative and will only be zero if each pixel is affected by at most one slot, ensuring that $\gthetaseul$ satisfies \Cref{coro:compositional}.  We can use this function to measure the compositional of a decoder in our experiments (see \Cref{sec:proposedmethod}), where it serves as a key indicator of how effectively the model decomposes its inputs into independent components. More empirical and theoretical details on the function can be found in~\citet{brady2023provably}.


% The compositional contrast given by~\citet{brady2023provably} is defined as follows:
% \begin{definition}[Compositional Contrast] \label{def:compositional_contrast}
%     Let $\gthetaseul: \gZ\to\gX$ be differentiable. The \emph{compositional contrast} of $\gthetaseul$ at $\rvz$ is
%     \begin{equation} \label{eq:compositional_contrast}
%     %\resizebox{0.89\linewidth}{!}{$
%         {C_{\mathrm{comp}}}(\gthetaseul, \rvz) = \sum\limits_{n=1}^{N} 
%         \sum\limits_{\substack{k=1
%         }}^{K}
%         \sum\limits_{j=k+1}^{K}
%         \left\|\frac{\partial f_{n}}{\partial \rvz_{k}}(\rvz)\right\|
%         \left\|\frac{\partial f_{n}}{\partial \rvz_{j}}(\rvz)\right\|
%         \,.
%     \end{equation}
% \end{definition}
% This contrast function was proven to be zero if and only if $\gthetaseul$ is compositional according to \Cref{eq:compositional}. The function can be understood as computing each pairwise product of the (L2) norms for each pixel's gradients w.r.t any two distinct slots $k\neq j$ and taking the sum. This quantity is non-negative and will only be zero if each pixel is affected by at most one slot such that $\gthetaseul$ satisfies \Cref{eq:compositional}. We use this function to measure compositionality of a decoder in our experiments in \Cref{sec:experiments}. More empirical and theoretical details on the function may be found in~\citet{brady2023provably}. 

% \section{Proof of Element Wise Contrastive Sparsity-inducing}\label{app:proofs}
% In this section, we present the theoretical contributions of the paper along with the corresponding proofs. While the core content remains unchanged, we have refined the wording of certain definitions and theorems for improved clarity and to address edge cases that were overlooked in the main body. As a result, the numbering of the restated elements aligns with that found in the main text.\par

% \begin{restatable}[Linear Identifiability for possibly degenerate multivariate distribution with Piecewise Affine $\gthetaseul$ adapted from \citet{kivva2022identifiability}]{lemma}{lemma_pcaffine}
% \label{adap_thm:identif-inv-affine}
% Assume $\gthetaseul, \gthetaseulhat:\mathbb{R}^n \rightarrow \mathbb{R}^d$ are injective and piecewise affine. We assume $\rvz$ and $\hat{\rvz}$ follow a (degenerate) multivariate normal distribution. If $\gtheta{\rvz} \equiv \gthetaseulhat(\hat{\rvz})$, then there exists an invertible affine transformation $\rvh:\sR^n \rightarrow \mathbb{R}^n$ such that $\rvh(\rvz) \equiv \hat{\rvz}$.
% \end{restatable}

% We first prove an important \Cref{proposition:element_wise_h} that shows that by enforcing sparsity of the transformed variables, the corresponding transformation is an element-wise linear function. Intuitively, these transformed variables will be the reconstructed masked latent variables $\rvz$.
% \propositionelementwisesparsity*

% \begin{proof}
%     We reuse the definition of the shared support indices $\Ib \in \gI$ and analyze each side of inequality~\eqref{equ:sparsity_constraint}.
%     \begin{align}
%         \mathbb{E}||\rvz||_1 = \mathbb{E}\sum^n_{k=1} \mathbbm{1}(\rvz_k \not= 0) &= \sum_{\rvi \in \mathcal{S}} p(\rvi) \mathbb{E}\left[\sum^n_{k=1} \mathbbm{1}(\rvz_k \not= 0) \mid \rvbs=\rvi\right] \\
%         &= \sum_{\rvi \in \mathcal{S}} p(\rvi) \sum^n_{k=1}\mathbb{E}[\mathbbm{1}(\rvz_k \not= 0) \mid \rvbs=\rvi]\\
%         &= \sum_{\rvi \in \mathcal{S}} p(\rvi) \sum^n_{k=1}\mathbbm{1}( k \in \rvi)\, 
%         \label{eq:rhs_ineq}
%     \end{align}
%     Now analyzing the left hand side of~\eqref{equ:sparsity_constraint}, starting with similar steps as previously we get
%     \begin{align}
%         \mathbb{E}||\rvh(\rvz)||_1 = \sum_{\rvi \in \mathcal{S}} p(\rvi) \sum^n_{k=1}\mathbb{E}[\mathbbm{1}(\vh_{k}(\rvz) \not= 0) \mid \rvbs = \rvi] &= \sum_{\rvi \in \mathcal{S}} p(\rvi) \sum^n_{k=1}\mathbb{P}_{\rvz\mid \rvbs=\rvi}[\vh_{k}(\rvz) \not= 0] \label{eq:left_ineq}\\
%         &= \sum_{\rvi \in \mathcal{S}} p(\rvi) \sum^n_{k=1}\left(1 - \mathbb{P}_{\rvz\mid \rvbs=\rvi}[\vh_{k}(\rvz) = 0]\right) \,. \label{eq:lhs_ineq}
%     \end{align}

% For $\rvh$ to be a permutation composed with an element-wise invertible linear transformation on $\gZ$, it is enough to show there exists a permutation $\Pi: [n] \rightarrow [n]$ such that, for every $k$, $N_k = \{\Pi(k)\}$. To achieve this, we are going to first show that 
% \begin{align}\label{eq:equality_seb}
%     \mathbb{P}_{\rvz\mid \rvbs=\rvi}[\vh_{k}(\rvz) = 0] = \alpha_k\mathbbm{1}(N_k \cap \rvi = \emptyset) \,,
% \end{align}


% where $\alpha_k \in {0,1}$. Given that $\vh_{k}$ is linear on $\gZ$, we have $\vh_{k}(\rvz) = \rvbw^k \cdot \rvz + \rvc_k$ for some $\rvbw^k \in \mathbb{R}^n$. We need to demonstrate that $N_k = {j \in [n] \mid \rvbw^k_j \neq 0}$. First, if $\rvbw^k_j = 0$, then $\vh_{k}$ remains constant in dimension $j$, so for $j$ to be included in $N_k$, $\rvbw^k_j$ must be non-zero. Conversely, if $\rvbw^k_j \neq 0$, there are two scenarios where $j \not\in N_k$: 

% \begin{itemize}
%     \item if $\rvbw^k_j \rvz_j$ always cancels out with another term $\rvbw^k_k \rvz_k$ (where both $\rvbw^k_j, \rvbw^k_k \neq 0)$
%     \item if $\rvz_j$ can only take a single value.
% \end{itemize}

% We show that neither of these scenarios can occur due to \Cref{assump:sufficient_partial_pairing}. Specifically, the first scenario is impossible because the masks for two different variables cannot be identical and still satisfy this assumption. The second scenario cannot occur because \Cref{assump:sufficient_partial_pairing} requires there to be a shared support indices set in which $k$ is masked and one in which it is not. Therefore,
% \begin{align*}
%     \vh_{k}(\rvz) &= \rvbw^k \cdot \rvz + \rvc_k \\
%     &= \rvbw^k_{N_k} \cdot \rvz_{N_k} + \rvc_k\,.
% \end{align*}

% Note that the event $\{\rvz_\rvi \mid \rvbw^k_\rvi \cdot \rvz_\rvi +\rvc_k = 0\}$ corresponds to the kernel of the linear map $\rvbw^k_\rvi$. We can thus infer its dimensionality via the rank-nullity theorem~\citep{friedberg2014linear} which states that $\text{rank}(\rvbw_{\rvi}^k) + \text{dim}(\text{Ker}(\rvbw_{\rvi}^k)) = \text{dim}(\text{Dom}(\rvbw_{\rvi}^k))$, where Ker() is nullity and Dom() is domain, which here implies that $1 + \text{dim}(\text{Ker}(\rvbw_{\rvi}^k)) = |\rvi|$. We thus have $\text{dim}(\{\rvz_\rvi \mid \rvbw^k_\rvi \cdot \rvz_\rvi +\rvc_k= 0\}) = |\rvi| - 1$. Since $\mathbb{P}_{\rvz\mid \rvbs=\rvi}$ has a density w.r.t. to the Lebesgue measure, we have that $\mathbb{P}_{\rvz\mid \rvbs=\rvi}[\rvbw^k_\rvi \cdot \rvz_\rvi +\rvc_k = 0] = 0$ (since a density w.r.t. to Lebesgue cannot concentrate mass on a lower-dimensional linear subspace).

%  We thus have proved that indeed, $\mathbb{P}_{\rvz\mid \rvbs=\rvi}[\vh_{k}(\rvz) = 0] = \alpha_k\mathbbm{1}(N_k \cap \rvi = \emptyset)$.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
% Putting \eqref{equ:sparsity_constraint}, \eqref{eq:rhs_ineq}, \eqref{eq:lhs_ineq} and \eqref{eq:equality_seb} together, we obtain
%     \begin{align}
%         \sum_{\rvi \in \mathcal{S}} p(\rvi) \sum^n_{k=1} [(1 -\alpha_k)+  \alpha_k\mathbbm{1}(N_k \cap \rvi \neq \emptyset)] \leq \sum_{\rvi \in \mathcal{S}} p(\rvi) \sum^n_{k=1}\mathbbm{1}( k \in \rvi)
%     \end{align}
% We can now use Lemma~\ref{lemma:Dv_perm}, because Ass.~\ref{assump:sufficient_partial_pairing} implies the existence of the $\rvz_1$ point, which in this case is $(0,...,0)$.

% By using this lemma, we can show that there exists a permutation $\Pi$ such that, for all  $k \in [n]$, $k \in N_{\Pi(k)}$.
%  We now permute the terms on the l.h.s. according to $\Pi$ and reorganize the terms as:
%     \begin{align}
%         &\sum_{\rvi \in \mathcal{S}} p(\rvi) \sum^n_{k=1} [(1 -\alpha_{\Pi(k)})+  \alpha_{\Pi(k)} \mathbbm{1}(N_{\Pi(k)} \cap \rvi \neq \emptyset)] \leq \sum_{\rvi \in \mathcal{S}} p(\rvi) \sum^n_{k=1}\mathbbm{1}( k \in 
%         \rvi) \nonumber \\
%         &\sum_{\rvi \in \mathcal{S}} p(\rvi) \sum^n_{k=1} [(1 -\alpha_{\Pi(k)})+  \alpha_{\Pi(k)}\mathbbm{1}(N_{\Pi(k)} \cap \rvi \neq \emptyset) - \mathbbm{1}( k \in \rvi)] \leq 0
%         \label{eq:leq_zero}
%     \end{align}
%     Note how, for all $k$, $(1 -\alpha_{\Pi(k)})+  \alpha_{\Pi(k)}\mathbbm{1}(N_{\Pi(k)} \cap \rvi \neq \emptyset) - \mathbbm{1}( k \in \rvi) \geq 0$, since whenever $k \in \rvi$, we must have $N_{\Pi(k)} \cap \rvi \not= \emptyset$, because we chose a permutation such that $k \in N_{\Pi(k)}$. Note also
%     that this is true irrespective of the value of $\alpha_{\Pi(k)}\in\{0,1\}$. 
    
%     We then can notice that if $k \not \in \rvi$, then $\alpha_{\Pi(k)}=0$. The function can have either value $0$ or $1$, but in any case not negative. Hence the inequality in \eqref{eq:leq_zero} is actually an equality and hence for all $\rvi \in \mathcal{S}$ and all $k \in [n]$,
%     \begin{align}
%         (1 -\alpha_{\Pi(k)})+  \alpha_{\Pi(k)}\mathbbm{1}(N_{\Pi(k)} \cap \rvi \neq \emptyset) - \mathbbm{1}( k \in \rvi)=0
%         \,. 
%         \label{equ: mid}
%     \end{align}
% The first thing we conclude is that if $k \not\in \rvi$,  then $\alpha_{\Pi(k)}=1$, since otherwise
% Equ.\eqref{equ: mid} is violated. Under Ass.~\ref{assump:sufficient_partial_pairing}, we have that, for all $k \in [n]$, there
% exists an $\rvi \in \gS $ such that $k \not\in \rvi$. We thus conclude that $\alpha_k=1$ for all $k \in [n]$, which allows us to write
% \begin{align}
%         \mathbbm{1}(N_{\Pi(k)} \cap \rvi \not= \emptyset) &= \mathbbm{1}( k \in \rvi) \\
%         \mathbbm{1}(N_{\Pi(k)} \cap \rvi = \emptyset) &= \mathbbm{1}( k\not\in \rvi)\,. \label{eq:cool_eq}
%     \end{align}


%     Importantly, this means
%     \begin{align}
%         \forall k \in [n], \forall \rvi\in \mathcal{S},\ k\not\in \rvi &\implies N_{\Pi(k)} \cap \rvi = \emptyset 
%         \implies N_{\Pi(k)} \subseteq \rvi^c\,,
%     \end{align}
%     which can be rewritten as 
%     \begin{align}
%         \forall k \in [n], N_{\Pi(k)} \subseteq \bigcap_{\rvi \in \mathcal{S} \mid k \not\in \rvi} \rvi^c\,. \label{eq:final_eq}
%     \end{align}
%     We now rewrite Assumption~\ref{assump:sufficient_partial_pairing} below  
%     and take the complement on both:
%   \begin{align}
%         \forall k \in [n], \bigcup_{\rvi \in \mathcal{S} \mid k \not\in \rvi} \rvi &= [n] \setminus \{k\}\\
%         \bigcap_{\rvi \in \mathcal{S} \mid k \not\in \rvi} \rvi^c &= \{k\} \label{eq:final_eq2}
%     \end{align} 
%     Combining \eqref{eq:final_eq} with \eqref{eq:final_eq2} implies that $N_{\Pi(k)} = \{k\}$ for all $k$, which concludes the proof.
% \end{proof}

% \subsection{Objective function overall}
% The term $\gR_{inv}(\rvz)$ is designed to penalize the similarity between latent variables across different indices. However, this formulation does not strictly enforce independence between the individual components of $\rvz_k$ and $\rvz_{k+1}$. Specifically, while the Dissimilarity Loss minimizes the overall similarity between the vectors $z_k$ and $\rvz_{k+1}$, it does not ensure that individual components, such as $z_{k,d}$ and $z_{k+1,d}$, are independent of one another. For example, consider two vectors $\rvz_k = (1, 1)$ and $\rvz_{k+1} = (2, 2)$; minimizing their similarity would not inherently disrupt the relationship between specific components if they were generated from correlated distributions. Consequently, while $\gR_{inv}(\rvz)$ can effectively reduce overall vector similarity, additional measures are required to enforce independence among the components of the latent variables.
% \subsection{Connection with  Spike-and-Slab Lasso prior for theoretical guarantees}
% In a standard DGM, the vector $\mbz_i$ is a latent variable that is fed through a neural network to reconstruct the distribution of $\mbx_i$.  The sparse DGM introduces an additional parameter $\mbw_{j}\in\mathbb{R}^K$, $j\in\{1,\dots,G\}$, a per-feature vector that selects which of the $K$ latent factors are used to produce the $j$th feature of $\mbx_i$.   The sparse DGM models the prior covariance between factors with the positive definite matrix $\bm{\Sigma}_{z}\in \mathbb{R}^{K\times K}$. The sparse DGM is:
% \begin{align}
% %\theta &\sim \mathcal{N}(0, \tau)\\
% w_{jk} &\sim \text{Spike-and-Slab Lasso}(\lambda_0, \lambda_1, a, b), \ \ k = 1,\dots, K  \notag\\
% \mbz_i &\sim \mathcal{N}_K(0, \bm{\Sigma}_{z}), \ \ i = 1,\dots, N  \notag\\
% x_{ij} &\sim \mathcal{N}((f_{\theta}(\mbw_{j} \odot \mbz_i))_j, \sigma_j^2), \ \ j=1,\dots, G\label{eq:sparse_vae_model}
% \end{align}
% where $f_{\theta}:\mathbb{R}^K\to\mathbb{R}^G$ is a neural network with weights $\theta$, $\sigma_j^2$ is the per-feature noise variance and $\odot$ denotes element-wise multiplication. The Spike-and-Slab Lasso \citep{rovckova2018spike} is a sparsity-inducing prior which we will describe below.~\looseness=-1


% In the data generating distribution in \Cref{eq:sparse_vae_model}, the parameter $w_{jk}$ controls whether the distribution of $x_{ij}$ depends on factor $k$. If $w_{jk}\neq0$, then $z_{ik}$ contributes to $x_{ij}$, while if $w_{jk}=0$, $z_{ik}$ cannot contribute to $x_{ij}$.  If $\bm{w}_{j}$ is sparse, then $x_{ij}$ depends on only a small set of factors.  Such a sparse factor-to-feature relationship is shown in \Cref{fig:intro_sparse_vae}.~\looseness=-1 

% Note that $x_{ij}$ depends on the same set of factors for every sample $i$. This dependency only makes sense when each $x_{ij}$ has a consistent meaning across samples.  For example, in genomics data $x_{ij}$ corresponds to the same gene for all $i$ samples. (This dependency is not reasonable for image data, where pixel $j$ has no consistent meaning across samples.)

% \textbf{The Spike-and-Slab Lasso.}  The parameter $w_{jk}$ has a SSL prior \citep{rovckova2018spike}, which is defined as a hierarchical model,
% \begin{align}
%     \eta_k &\sim \text{Beta}(a, b), \\
%     \gamma_{jk} &\sim \text{Bernoulli}(\eta_k), \\
%     w_{jk} &\sim \gamma_{jk}\psi_1(w_{jk}) + (1 - \gamma_{jk})\psi_0(w_{jk}).
% \end{align}
% In this prior, $\psi_s(w) = \frac{\lambda_s}{2}\exp(-\lambda_s |w|)$ is the Laplace density and $\lambda_0 \gg \lambda_1$. The variable $w_{jk}$ is drawn \emph{a priori} from either a Laplacian ``spike'' parameterized by $\lambda_0$, and is consequentially negligible, or a Laplacian ``slab'' parameterized by $\lambda_1$, and can be large.~\looseness=-1 

% Further, the variable $\gamma_{jk}$ is a binary indicator variable that determines whether $w_{jk}$ is negligible. The Beta-Bernoulli prior on $\gamma_{jk}$ allows for uncertainty in determining which factors contribute to each feature.~\looseness=-1

% Finally, the parameter $\eta_k\in[0,1]$ controls the proportion of features that depend on factor $k$. By allowing $\eta_k$ to vary, the sparse DGM allows each factor to contribute to different numbers of features.  In movie-ratings data, for example, a factor  corresponding to the action/adventure genre may be associated with more movies than a more esoteric genre.~\looseness=-1   

% Notice the prior on $\eta_k$ helps to ``zero out'' extraneous factor dimensions and consequently estimate the number of factors $K$. If the hyperparameters are set to $a\propto1/K$ and $b=1$, the Beta-Bernoulli prior corresponds to the finite Indian Buffet Process prior.~\looseness=-1  

% We consider the case of known anchor features, where we have at least two anchor features per factor. We consider the case where the likelihood is Gaussian. We have two solutions, $(\widetilde{\theta}, \widetilde{\mbz}, \widetilde{\sigma}^2)$ and $(\widehat{\theta}, \widehat{\mbz}, \widehat{\sigma}^2)$, with equal likelihood.  We show that $\widetilde{\mbz}$ must be a coordinate-wise transform of $\widehat{\mbz}$.


% We have
% \begin{align}
%     \sum_{i=1}^N\sum_{j=1}^G \frac{1}{\widetilde{\sigma}_j^2}[x_{ij} - f_{\widetilde{\theta}_j}(\widetilde{\mbw_{j } }\odot \widetilde{\mbz}_i)]^2 =  \sum_{i=1}^N\sum_{j=1}^G \frac{1}{\widehat{\sigma}_j^2}[x_{ij} - f_{\widehat{\theta}_j}(\widehat{\mbw_{j }} \odot \widehat{\mbz}_i)]^2,
% \end{align}
% where $f_{\theta_j}(\cdot)$ denotes $f_{\theta}(\cdot)_j$, the $j$th output of $f_{\theta}$.

% We prove that, given the anchor feature for factor $k$, $z_{ik}$ is identifiable. As all factors are assumed to have anchor features, this holds for all $k\in\{1,\dots, K\}$.   Suppose $j$ is an anchor feature for $k$. That is, $w_{jl}=0$ for all $l\neq k$.  Then, $\widetilde{\mbw_{j } }\odot \widetilde{\mbz}_i = (0, \dots, 0, \widetilde{w}_{jk} \widetilde{z}_{ik}, 0, \dots, 0)$. With slight abuse of notation, we then write $f_{\widetilde{\theta}_j}(\widetilde{\mbw_{j } }\odot \widetilde{\mbz}_i) = f_{\widetilde{\theta}_j}(\widetilde{w}_{jk} \widetilde{z}_{ik})$.

% We have:
% \begin{align}
%        \sum_{i=1}^N \frac{1}{\widetilde{\sigma}_j^2}[x_{ij} - f_{\widetilde{\theta}_j}(\widetilde{w}_{jk}\widetilde{z}_{ik})]^2 &=  \sum_{i=1}^N \frac{1}{\widehat{\sigma}_j^2}[x_{ij} - f_{\widehat{\theta}_j}(\widehat{w}_{jk}\widehat{z}_{ik})]^2 \\
%        \left(\frac{1}{\widetilde{\sigma}_j^2}- \frac{1}{\widehat{\sigma}_j^2}\right)\sum_{i=1}^N x_{ij}^2 -2\sum_{i=1}^N x_{ij}&\left( \frac{f_{\widetilde{\theta}_j}(\widetilde{w}_{jk}\widetilde{z}_{ik})}{\widetilde{\sigma}_j^2} - \frac{f_{\widehat{\theta}_j}(\widehat{w}_{jk}\widehat{z}_{ik})}{\widehat{\sigma}_j^2} \right) + \sum_{i=1}^N \left( \frac{f_{\widetilde{\theta}_j}^2(\widetilde{w}_{jk}\widetilde{z}_{ik})}{\widetilde{\sigma}_j^2} - \frac{f_{\widehat{\theta}_j}^2(\widehat{w}_{jk}\widehat{z}_{ik})}{\widehat{\sigma}_j^2} \right) = 0. \label{eq:id_known_w}
% \end{align}

% For \Cref{eq:id_known_w} to hold for all $x_{ij}$, we must have the coefficients equal to zero:
% \begin{align}
%     \frac{1}{\widetilde{\sigma}_j^2}- \frac{1}{\widehat{\sigma}_j^2} = 0 \implies \widetilde{\sigma}_j^2 = \widehat{\sigma}_j^2
% \end{align}
% and 
% \begin{align}
%     \frac{f_{\widetilde{\theta}_j}(\widetilde{w}_{jk}\widetilde{z}_{ik})}{\widetilde{\sigma}_j^2} - \frac{f_{\widehat{\theta}_j}\widehat{w}_{jk}(\widehat{z}_{ik})}{\widehat{\sigma}_j^2} =0 \implies f_{\widetilde{\theta}_j}(\widetilde{w}_{jk}\widetilde{z}_{ik}) = f_{\widehat{\theta}_j}(\widehat{w}_{jk}\widehat{z}_{ik}).
% \end{align}

% If $f_{\widetilde{\theta}_j}: \mathbb{R}\to\mathbb{R}$ is invertible, then we have 
% \begin{align}
%     \widetilde{z}_{ik} = \widetilde{w}_{jk}^{-1}f_{\widetilde{\theta}_j}^{-1}(f_{\widehat{\theta}_j}(\widehat{w}_{jk}\widehat{z}_{ik})).
% \end{align}
% That is, $z_{ik}$ is identifiable up to coordinate-wise transformation. 

% If $f_{\theta_j}$ is not invertible, then there exists bijective functions $g: \mathbb{R}\to \mathbb{R}$, $h:\mathbb{R}\to \mathbb{R}$ such that $\widehat{w}_{jk}\widehat{z}_{ik} = \fphi(\widetilde{w}_{jk}\widetilde{z}_{ik})$ and the equality is satisfied 
% \begin{align}
% f_{\widetilde{\theta}_j}(\widetilde{w}_{jk}\widetilde{z}_{ik}) &= f_{\widetilde{\theta}_j}(h(\fphi(\widetilde{w}_{jk}\widetilde{z}_{ik})))   \quad \text{(as }f_{\theta_j}\text{ is not invertible, $\widetilde{w}_{jk}\widetilde{z}_{ik}\neq h(\fphi(\widetilde{w}_{jk}\widetilde{z}_{ik}))$)}\\
% &=f_{\widehat{\theta}_j}(\widehat{w}_{jk}\widehat{z}_{ik}).
% \end{align}
% As we have $\widehat{z}_{ik} = \widehat{w}_{jk}^{-1}\fphi(\widetilde{w}_{jk}\widetilde{z}_{ik})$, $z_{ik}$ is identifiable up to coordinate-wise transformation. 





% %Furthermore, the sparsity, enable the model to align with the sparsity of the true underlying variables. This approach resolves ambiguities that arise from rotations in the latent space, leading to more accurate and interpretable representations.
% % \subsection{Generalization to Uncommon Correlations}\label{sec:proposed_method_generalization}
% % \Cref{thm:disentanglement_piecewise} ensures identifiability of representations within the in-data-distribution (IDD) via contrastive sparsity constraint, resolving a key issue. The decoder $\gthetaseulhat$ generalizes such that $\gthetaseulhat(\prvz = \gtheta{\rvz}$ and $\gthetaseulhat(\gZ') = \gX$. The condition on the encoder from contrastive sparsity-inducing ensures that $\hatfphi$ inversely maps $\gthetaseulhat$ across $\gX$, guaranteed within the training space by minimizing the reconstruction objective and contrastive sparsity loss. However, there's no enforcement that $\hatfphi$ also inversely maps $\gthetaseulhat$ out-of-distribution (OOD) beyond $\gX$. To address this problem, we draw on the possibility of combining the inferred latents of two different sequences, $(\rvx^{(1)}, \rvx^{(2)})$. This approach differs from what \Cref{thm:disentanglement_piecewise} describes, as we assume $(\rvx^{(1)}, \rvx^{(2)})$ are distinct and that each $\rvx^{(i)}$ has its own augmentation samples. By inferring latents based on their respective masks index, we create combinations that not exit in the intervention support \ie entirely out-of-distribution (OOD). This forces $\hat{f}_{\phi}$ to invert $\hat{g}_{\theta}$ outside of the training distribution $\gX$. While this approach does not necessarily guarantee performance on the test dataset, it helps the model generalize better to unseen combinations. 


% % % \begin{align}\label{eq:generalization}
% % %     &\gL_{\text{Time-CSL}}(\gthetaseulhat, \hatfphi, \gZ) = \E_{\prvz \sim q_{\prvz}} \Big[\big\Vert \hatfphi\fphi(\gthetaseulhat(\prvz\big) - \prvz \big\Vert_2^2\Big]
% % % \end{align}

% % \subsection{Putting it All Together in Practice}
% % \Cref{thm:disentanglement_piecewise} showed how slot identifiability and how can be generalized to all of $\gZ$ in ID if we minimizing  \Cref{eq:disentanglement_piecewise} and ensuring the sparsity, while $\gL_{\text{Time-CSL}}^{out}$ in \Cref{eq:generalization} help to generalize for OOD senearios. Putting these results together, we can now prove conditions for which an autoencoder will identify and generalize time series representaitons. We introduce an explicit trade-off parameter $\alpha \in [0,1]$ to smoothly interpolate between in-and-out data distribution of the representations.
% % \begin{align}
% % \label{equ: loss_pw}
% % \gL = \gL_{\text{VAE}} + (1-\alpha)\gL_{\text{Time-CSL}} + \alpha\gL^{cons}_{\text{Time-CSL}},
% % % \nonumber \\
% % %     &\text{subject to: \quad}
% % %     \frac{1}{nN} \sum_{i \in [N]} \norm{\fphi(\rvx^i)}_1 \leq \epsilon, 
% % \end{align}
% % when $\alpha=0$, the model focus only on the identifiability task. 

% % !TEX root = ../icml_2023.tex
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Element-wise Identifiability given index support i for Piecewise Linear}\label{app:propositionpiecewiselinearsupportindex}

\begin{table}[t]
\centering
\begin{minipage}[b]{.99\textwidth}
\centering
\caption{Related work in nonlinear ICA for time series. A blue check denotes that a method has an attribute, whereas a red cross denotes the opposite. $^\dagger$ indicates an approach we implemented.\label{app:related_work}}
%\vspace{-3mm}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccc}
\toprule
% Reference & Temporal & Causality  & Nonstationarity  & Nonparametric & XX \\
\bf{Approach}  & \bf{Temporal Data} & \bf{Dependent Factors} & \bf{Nonparametric Expression} & \bf{Stationary Process} \\
\midrule
{TCL~\citep{hyvarinen2016unsupervised} }  & \cmark & \xmark & \xmark & \xmark \\
{PCL~\citep{hyvarinen2017nonlinear} } & \cmark & \xmark & \cmark & \cmark \\
{GCL~\citep{hyvarinen2019nonlinear}  }  & \cmark & \xmark & \cmark & \xmark  \\
{iVAE~\citep{khemakhem2020variational}} & \xmark & \xmark & \xmark & \xmark \\
{GIN~\citep{sorrenson2020disentanglement}} & \xmark & \xmark & \xmark & \xmark \\
{HM-NLICA~\citep{halva2020hidden}} & \cmark & \xmark & \cmark & \xmark  \\
{SlowVAE~\citep{klindt2020towards} }  & \cmark & \xmark & \xmark & \cmark \\
\citep{yao2021learning} {\textbf{LEAP (Theorem 1)} } & \cmark & \cmark & \cmark & \xmark\\
\citep{yao2021learning} {\textbf{LEAP (Theorem 2)} } & \cmark & \cmark & \xmark & \cmark\\
TimeCSL (our)$^\dagger$ {\textbf{TimeCSL (Theorem 1)} } & \cmark & \cmark & \cmark & \cmark +  \xmark\\
\bottomrule
\end{tabular}
}
\end{minipage}
\vspace{-0.5cm}
\end{table}

In this section, we present the proof of \Cref{thm:Elementwise_Identifiability_piecewise}. To establish a solid foundation for the argument, we first restate \Cref{assump:sufficient_partial_pairing}, which plays a pivotal role in the proof.
\assumptionsufficientsparsity*
Additionally, we introduce some notation. For \( \rvi \in \mathcal{I} \), we assume that the probability measure \( \mathbb{P}_{\rvz_{\rvi}} \) admits a density with respect to the Lebesgue measure on \( \mathbb{R}^{|\rvi|} \). We let $\equiv$ denote equality in the distribution.

\propositionpiecewiselinearsupportindex*

\begin{proof}
The proving strategy has three steps: Intuitively, based result \citep{kivva2022identifiability} combined with contrastivity beteween tow latent based their shared support indices $\rvi$. This means that for the data that satisfy \Cref{assump:sufficient_partial_pairing}, $\gthetaseul(\rvz)$ and $\gthetaseulhat(\hrvz)$ are equally distributed, then there exists an invertible affine transformation such that $\rvh(\rvz)=\prvz$. 
Second, we use the strategy of linear identifiability \citep{lachapelle2022partial} to obtain element wise identifiabiltiy:

\paragraph{{{\bf Step }\textcolor{crefcolor}{1)}} Contrastive Sparsity and Linear Identifiability given pairs $\rvi$}\label{proof:step1} We begin by recalling the result from~\citet{kivva2022identifiability} on the existing of an invertible function affine transformation $\vh_{k}$, we adapt this for the case where if the reconstruction objective is minizzed and alignment. The theorem on identifiability of MVNs states:

    %%%%%%%%%%% thm.identifiability of non-degenerate MVNs
    
    \begin{theorem}\label{thm:main:npmixF2}
    Let $\gthetaseul, \gthetaseul^{\prime}:\mathbb{R}^{d\times n} \rightarrow \mathbb{R}^{C\times T}$ be piecewise affine functions satisfying \ref{assump:injective_piecewise}. Let $\rvz~\sim \sum\limits_{i=1}^{J} \omega_i\gN(\boldsymbol\mu_i, \boldsymbol\Sigma_i)$ and $\prvz \sim \sum\limits_{j=1}^{J'} \omega_j'\gN(\boldsymbol\mu_j', \boldsymbol\Sigma_j')$ be a pair of GMMs (in reduced form). Suppose that $\gthetaseul(\rvz)$ and $\gthetaseul^{\prime}(\prvz)$ are equally distributed. Then there exists an invertible affine transformation $\rvh:\mathbb{R}^{d\times n} \rightarrow \mathbb{R}^{d\times n}$ such that $\rvh(\rvz) \equiv \prvz$, i.e., $J = J'$ and for some permutation $\pi$ we have $\omega_i = \omega'_{\pi(k)}$ and $\rvh\sharp\gN(\boldsymbol\mu_i, \boldsymbol\Sigma_i) = \gN(\boldsymbol\mu'_{\pi(i)}, \boldsymbol\Sigma'_{\pi(i)})$.
    \end{theorem}
    
    We recall that the transformation and the number of components can be unknown and arbitrary, and that no assumption of separation or independence is necessary for the distribution.\par
    
    By Theorem C.2 \citep{kivva2022identifiability}, since contrastive learning involves the minimisation of a contrastive loss which ensures that similar data points (positive pairs) are moved closer together and dissimilar data points (negative pairs) are moved further apart. Let the inferred latent representation $(\rvz, \prvz)$ be handled by the exact same function $\fphi$, and we consider the zero reconstruction under $\gR_{aling}=0$ for all slot indices in $\rvi$. Alongside this, contrastive loss minimization induces the distributions of \( \gthetaseul(\rvz) \) and \( \gthetaseul(\prvz)\) to become indistinguishable on $i \in \rvi$ to be well-aligned, apart from for $k \notin \rvi$, but as we consider the \Cref{assump : sufficient_partial_pairing} on the sufficient partial pairing that will cover this factor $k$ in another pairing sample of the pair $(\rvx,\prvx)$. Thus, according to Theorem C.2 \citep{kivva2022identifiability}, there must exist an invertible affine transformation \( \rvh \) such that \( \rvh(\rvz) \equiv \prvz \rvz) \). It is more likely to observe that :
    \begin{align}
        \sum_{j=1}^{J} \omega_{k}\gthetaseul\sharp\gN(\mu_{k},\sigma_{k})
        \sim
        \gthetaseul\sharp\fphi(\sum_{j=1}^{J} \omega_{k}\gN(\mu_{k},\sigma_{k})\Big).
    \end{align}
    In other words, minimizing to hold (i) and zeros error construction, implies a mixture model whose components are piecewise affine transformations identifiable.  
    
\paragraph{{{\bf Step }\textcolor{crefcolor}{2)}} Sparsity Pattern of an Invertible Matrix with an element-wise linear transformation} Since \(\rvx = \gthetaseul(\rvz)\), we can rewrite perfect reconstruction as:
\[
\sE \|\gthetaseul(\rvz) - \gthetaseulhat(\fphi(\gthetaseul(\rvz)))\|^2_2 = 0 \tag{10}
\]
This means \( \gthetaseul \) and \( \gthetaseulhat \circ \fphi \circ \gthetaseul \) are equal $\mathbb{P}_{\rvz}$-almost everywhere. Both of these functions are continuous, \( \gthetaseul \) by \Cref{assump:injective_piecewise}, and \( \gthetaseulhat \circ \fphi \circ \gthetaseul \) because \( \gthetaseulhat \) is continuous, and \( \gthetaseul \), \( \fphi \) are linear. Since they are continuous and equal $\mathbb{P}_{\rvz}$-almost everywhere $\gZ$, this means that they must be equal over the support of \( \gZ \), i.e.,
\[
\gthetaseul(\rvz) = \gthetaseulhat \circ \fphi \circ \gthetaseul(\rvz), \quad \forall \rvz \in \gZ. \tag{11}
\]
This can be easily shown by contradiction considering any slot latent \( \rvz' \in \gZ \) on which \( \gthetaseul \) and \( \gthetaseulhat \circ \fphi \circ \gthetaseul \) are different, i.e., \( \gthetaseulhat \circ \fphi \circ \gthetaseulhat(\prvz) \neq \gthetaseul(\prvz) \). This would imply that \( (\gthetaseul - \gthetaseulhat \circ \fphi \circ \gthetaseul) \), which is also a continuous function, is non-zero at \( \prvz \) and in its neighborhood, which contradict the assumption that \( \gthetaseul \) and \( \gthetaseulhat \circ \fphi \circ \gthetaseul \) are the same $\mathbb{P}_{\rvz}$-almost everywhere. We can now apply the inverse of \( \gthetaseulhat \) on both sides to obtain
\[
\gthetaseulhat^{-1} \circ \gthetaseul(\rvz) = \fphi \circ \gthetaseul(\rvz)=\rvh(\rvz), \quad \forall \rvz \in \gZ. \tag{12}
\]
Since both \( \gthetaseul \) and \( \fphi \) are invertible linear functions, given the fisrt part of the proof ({\bf Step }\textcolor{crefcolor}{1}-\Cref{proof:step1}) \( \rvh \) is also an invertible linear function. We now show that \( \rvh \) is a permutation composed with an element-wise linear transformation. To do this, we leverage the sparsity constraint:
\begin{align}
\sE \|\hrvz \|_0 \leq \sE \|\rvz\|_0 \\
\sE \|\fphi(\gthetaseul(\rvz))\|_0 \leq \sE \|\rvz\|_0 \\
\sE \|\rvh(\rvz)\|_0 \leq \sE \|\rvz\|_0 \\
\end{align}
Since $\vh_{k}$ is invertible linear transformation, we have that $\vh_{k}(\rvz)=\rvw_k\cdot\rvz$ and its determinant is non-zero, i.e.,
\begin{align}
\det(\rvh) := \sum_{\pi \in \gP} \text{sign}(\pi) \prod_{k=1}^{n} \rvh_{k, \pi(k)} \neq 0,
\end{align}
where \( \gP \) denotes the set of all \( n \)-permutations. This expression implies that at least one term in the sum is non-zero, meaning there exists a permutation \( \pi \in \gP \) such that for every \( k \in [n] \), \( \frac{\partial \vh_{k}}{\partial \rvz_{\pi(k)}} \neq 0 \).\par
Following the steps outlined in Theorem B.4 by \citep{lachapelle2022disentanglement}, and under the assumption of \Cref{assump:sufficient_partial_pairing}, we extend the disentanglement analysis to our setting. This leads to the conclusion that \( \rvh \) can be expressed as a permutation composed with an element-wise invertible linear transformation, based on the shared support indices \( \rvi \) of the latent slot within the subspace \( \gZ_{\rvi} \). Specifically, there exists a permutation \( \pi \) on \( [n] \) such that, for each latent slot \( k \), the corresponding permutation is given by \( \pi(k) \).
%\item[i)] {\bf Element-wise Identifiability for Linear Transformation}\label{linear_case}
%Now we show that we sparsity this function is a permutation composed with element-wise invertible linear transformation by \citep{lachapelle2021disentanglement}.
%\rebutall{We now prove identifiability up to permutation and element-wise linear transformations for the case of a linear mixing function, given the assumption of sufficient shared support indices variability.}
%\end{itemize}
Since \( \gI \) is a finite set, which allows us to order its elements as \( \{\rvi_1, \dots, \rvi_{|\gI|}\} \). Therefore, we can express \( \gZ \) as the union \( \gZ = \bigcup_{i=1}^{|\gI|} \gZ^{(\rvi_i)} \). While we have already shown that \( \rvh \) is affine on each \( \gZ_{\rvi} \), we now demonstrate that \( \rvh \) is linear on \( \gZ \), i.e., \( \rvh(\rvz) \) is a linear function on the entire set \( \gZ = \bigcup_{\rvi \in \gI} \gZ_{\rvi} \). This completes the proof.
\end{proof}

\subsection{The Generative Process and The ELBO for Multivariates Mixture Gaussian}\label{sec:gen_process}
We in this subsection how \TimeCSL is trained based an a VAE process does similar to \citep{kivva2022identifiability, jang2016categorical}, whcih more kind of unsupervised generative approach for clustering that performance well, we herein first
describe the generative process of \TimeCSL. Specifically, suppose there are $n$ slots latents each has a dimension $d$,  an observed sample $\rvx \sim \gX$ is generated by the following process:

\begin{algorithm}[H]\label{algo_generative_process}
    \caption{\bf Generative Process}\label{alg:generative_process}
    \begin{algorithmic}[1] 
    \STATE {\bf {Input:}} Prior probabilities $\boldsymbol{w}$, neural network parameters $\boldsymbol{\theta}$
    \FOR{$j = 1, 2, \dots, N$}
        \STATE Sample slot $ k \sim \textup{Cat}(\boldsymbol{w}) $
        \STATE Sample latent vector $ \mathbf{z}^{(j)} \sim \mathcal{N}(\boldsymbol{\mu}_{k}^{(j)}, \boldsymbol{\sigma}_{k}^{(j)}^2 \mathbf{I})$
        \STATE Compute $ [\muphi{\rvx^{(j)}}; \log \sigmaphi{\rvx^{(j)}}^2] = \gthetaseul(\mathbf{z}^{(j)}) $
        \STATE Sample observation $ \mathbf{x}_j \sim \mathcal{N}(\mutheta{\rvx^{(j)}}, \sigmatheta{\rvx^{(j)}}^2 \mathbf{I}) $ or $ \textup{Ber}(\mutheta{\rvx^{(j)}}) $
    \ENDFOR
    \RETURN $\{ \mathbf{x}^{(j)}, \mathbf{z}^{(j)}, k \}_{j=1}^N$
    \end{algorithmic}
\end{algorithm}

\begin{lemma}\label{lemma:GMM}
Given two multivariate Gaussian distributions $q({\bf z})=\mathcal{N}({\bf z};\boldsymbol{\hat\mu},\boldsymbol{\hat\sigma}^2{\bf I})$ and $p({\bf z})=\mathcal{N}({\bf z};\boldsymbol \mu,\boldsymbol{\sigma}^2{\bf I})$, we have:
\begin{flalign}
\int q({\bf z})\log p({\bf z})\,d{\bf z}
&=\sum_{j=1}^J-\frac{1}{2}\log{(2\pi \sigma_j^2)}-\frac{\hat\sigma_j^2}{2\sigma_j^2}-\frac{(\hat\mu_j-\mu_j)^2}{2\sigma_j^2}\label{eq:lemma}
\end{flalign}
where $\mu_j$, $\sigma_j$, ${\hat\mu}_j$ and ${\hat\sigma}_j$ simply denote the $j$\textsuperscript{th} element of $\boldsymbol{\mu}$, $\boldsymbol{\sigma}$, $\boldsymbol{\hat\mu}$ and $\boldsymbol{\hat\sigma}$, respectively, and $J=d \times n $ is the dimensionality of $\bf{z}$.\vspace{5mm}
\end{lemma}

\begin{proof}
\begin{align}
&\int q({\bf z})\log p({\bf z})\,d{\bf z}
=\int \mathcal{N}({\bf z};\boldsymbol{\hat\mu},\boldsymbol{\hat\sigma}^2{\bf I})\log{\mathcal{N}({\bf z};\boldsymbol \mu,\boldsymbol{\sigma}^2{\bf I})}\,d{\bf z}&\nonumber\\
=&\int\prod_{j=1}^J\frac{1}{\sqrt{2\pi\hat\sigma_j^2}}\exp(-\frac{(z_j-\hat\mu_j)^2}{2\hat\sigma_j^2})\log\left[{\prod_{j=1}^J\frac{1}{\sqrt{2\pi\sigma_j^2}}\exp(-\frac{(z_j-\mu_j)^2}{2\sigma_j^2}})\right]\,d{\bf z}\nonumber\\
=&\sum_{j=1}^J\int \frac{1}{\sqrt{2\pi\hat\sigma_j^2}}\exp(-\frac{(z_j-\hat\mu_j)^2}{2\hat\sigma_j^2})\log\left[{\frac{1}{\sqrt{2\pi\sigma_j^2}}\exp(-\frac{(z_j-\mu_j)^2}{2\sigma_j^2}})\right]\,d{z_j}\nonumber\\
=&\sum_{j=1}^J\int \frac{1}{\sqrt{2\pi\hat\sigma_j^2}}\exp(-\frac{(z_j-\hat\mu_j)^2}{2\hat\sigma_j^2})\left[-\frac{1}{2}\log (2\pi\sigma_j^2)\right]\,d{z_j}-\int\frac{1}{\sqrt{2\pi\hat\sigma_j^2}}\exp(-\frac{(z_j-\hat\mu_j)^2}{2\hat\sigma_j^2})\frac{(z_j-\mu_j)^2}{2\sigma_j^2}\,d{z_j}\nonumber\\
=&\sum_{j=1}^J -\frac{1}{2}\log (2\pi\sigma_j^2)-\int\frac{1}{\sqrt{2\pi\hat\sigma_j^2}}\exp(-\frac{(z_j-\hat\mu_j)^2}{2\hat\sigma_j^2})\frac{(z_j-\hat\mu_j)^2+2(z_j-\hat\mu_j)(\hat\mu_j-\mu_j)+(\hat\mu_j-\mu_j)^2}{2\hat\sigma_j^2}\frac{\hat\sigma_j^2}{\sigma_j^2}\,d{z_j}\nonumber\\
=&\vb -\frac{\hat\sigma_j^2}{\sigma_j^2}\int\frac{1}{\sqrt{2\pi\hat\sigma_j^2}}\exp(-\frac{(z_j-\hat\mu_j)^2}{2\hat\sigma_j^2})\frac{(z_j-\hat\mu_j)^2}{2\hat\sigma_j^2}\,d{z_j}\nonumber
-\int\frac{1}{\sqrt{2\pi\hat\sigma_j^2}}\exp(-\frac{(z_j-\hat\mu_j)^2}{2\hat\sigma_j^2})\frac{(\hat\mu_j-\mu_j)^2}{2\sigma_j^2}\,d{z_j}&\nonumber\\
=&\vb-\frac{\hat\sigma_j^2}{\sigma_j^2}\int\frac{1}{\sqrt{2\pi}}\exp(-\frac{x_j^2}{2})\frac{x_j^2}{2}\,d{x_j}-\frac{(\hat\mu_j-\mu_j)^2}{2\sigma_j^2}\nonumber\\
=&\vb-\frac{\hat\sigma_j^2}{\sigma_j^2}\int\frac{1}{\sqrt{2\pi}}(-\frac{x_j}{2})\,d{(\exp(-\frac{x_j^2}{2}))}-\frac{(\hat\mu_j-\mu_j)^2}{2\sigma_j^2}\nonumber\\
=&\vb-\frac{\hat\sigma_j^2}{\sigma_j^2}\left[\frac{1}{\sqrt{2\pi}}(-\frac{x_j}{2})\exp(-\frac{x_j^2}{2})\Big|_{-\infty}^{\infty}-\int\frac{1}{\sqrt{2\pi}}\exp(-\frac{x_j^2}{2})\,d{(-\frac{x_j}{2})}\right]-\frac{(\hat\mu_j-\mu_j)^2}{2\sigma_j^2}\nonumber\\
=&\sum_{j=1}^J-\frac{1}{2}\log{(2\pi \sigma_j^2)}-\frac{\hat\sigma_j^2}{2\sigma_j^2}-\frac{(\hat\mu_j-\mu_j)^2}{2\sigma_j^2}\nonumber
\end{align}

where $\vb$ denotes $\sum_{j=1}^J -\frac{1}{2}\log (2\pi\sigma_j^2)$ for simplicity.
    
\end{proof}


\subsubsection{Variational Lower Bound for \TimeCSL}
\label{sec:vlowerbound}
A \TimeCSL instance is tuned to maximize the likelihood of the given data points. Given 
the generative process in Section~\ref{sec:gen_process},  by using Jensen's inequality, 
the log-likelihood of \TimeCSL can be written as:
\begin{flalign}
\log p({\bf x})&=\log\int_{\bf z}\sum_{k}p({\bf x,z},k)d{\bf z}\nonumber\\
&\geq E_{q({\bf z},k|{\bf x})}[\log\frac{p({\bf x,z},k)}{q({\bf z},k|{\bf x})}]=\mathcal{L}_{\textup{ELBO}}({\bf x})\label{eqn:loglikelihood}
\end{flalign}
where $\mathcal{L}_{\textup{ELBO}}$ is the evidence lower bound (ELBO), $q({\bf z},k|{\bf x})$ is the variational 
posterior to approximate the true posterior $p({\bf z},k|{\bf x})$. In \TimeCSL, we 
assume $q({\bf z},k|{\bf x})$ to be a mean-field distribution and can be factorized as:
\begin{equation}
q({\bf z},k| {\bf x}) = q({\bf z|x})q(k|{\bf x}).
\label{eqn:va_q}
\end{equation}

Then, according to Equation~\ref{eqn:va_q}, the $\mathcal{L}_{\textup{ELBO}}({\bf x})$ 
in Equation~\ref{eqn:loglikelihood} can be rewritten as:
\begin{eqnarray}
\mathcal{L}_{\textup{ELBO}}({\bf x})&=&E_{q({\bf z},k|{\bf x})}\left[\log \frac{p({\bf x,z},k)}{q({\bf z},k|{\bf x})}\right]\nonumber\\
&=&E_{q({\bf z},k|{\bf x})}\left[\log p({\bf x,z},k)-\log q({\bf z},k|{\bf x})\right]\nonumber\\
&=&E_{q({\bf z},k|{\bf x})}[\log p({\bf x}|{\bf z})+\log p({\bf z}|k)\label{eqn:elbo_fact}\\
&\quad&+ \log p(k) -\log q({\bf z}| {\bf x}) - \log q(k|{\bf x})]\nonumber
\end{eqnarray} 

In \TimeCSL, similar to VAE, we use a neural network $g$ to model $q({\bf z|x})$:
\begin{eqnarray}
[\boldsymbol{\hat\mu};\log \boldsymbol{\hat\sigma}^2]&=&\fphi({\bf x};\boldsymbol{\phi})\label{eqn:g_mu_sigma}\\
q({\bf z}|{\bf x})&=&\mathcal{N}({\bf z};\boldsymbol{\hat\mu},{\boldsymbol{\hat\sigma}}^2{\bf I})
\label{eqn:q_z_x}
\end{eqnarray}
where $\boldsymbol{\phi}$ is the parameter of network $g$.

By substituting the terms in Equation~\ref{eqn:elbo_fact}
and using the SGVB estimator and the {\it reparameterization} trick,
the $\mathcal{L}_{\textup{ELBO}}({\bf x})$ can be rewritten as:
\footnote{This is the case when the observation $\bf x$ is binary. For the real-valued situation, the ELBO
can be obtained in a similar way.}
\begin{small}
\begin{flalign}
\mathcal{L}_{\textup{ELBO}}({\bf x})
=&\frac{1}{N}\sum_{l=1}^N\sum_{i=1}^{C\times T}{x_i}\log\boldsymbol{{\mu}}^{(l)}_x|_i+(1-x_i)\lo\fphi(1-\boldsymbol{{\mu}}^{(l)}_x|_i)\nonumber\\
&-\frac{1}{2}\sum_{k=1}^{n}\gamma_{k}\sum_{j=1}^J(\log\boldsymbol{\sigma}^2_{k}|_{j}+
%\sum_{j=1}^J
\frac{\hat{\boldsymbol{\sigma}}^2|_j}{\boldsymbol{\sigma}^2_{k}|_{j}}+
%\sum_{j=1}^J
\frac{(\hat{\boldsymbol{\mu}}|_j-\boldsymbol{\mu}_{k}|_{j})^2}{\boldsymbol{\sigma}^2_{k}|_{j}})\nonumber\\
&+\sum_{k=1}^n\gamma_{k}\log \frac{w_k}{\gamma_{k}}
+\frac{1}{2}\sum_{j=1}^J(1+\log\hat{\boldsymbol{\sigma}}^2|_j)\label{eqn:ELBO_detail}
\end{flalign}
\end{small}
where $N$ is the number of Monte Carlo samples in the SGVB estimator,
$C\times T$ is the dimensionality of ${\bf x}$, $n$ is number of slots or factors, and $\boldsymbol{\mu}_x^{(l)}$, $x_i$
is the $i$\textsuperscript{th} element of $\bf x$,
$J$ is the dimensionality of $\boldsymbol{\mu}_k$, $\boldsymbol{\sigma}_k^2$, 
$\hat{\boldsymbol{\mu}}$ and $\hat{\boldsymbol{\sigma}}^2$, 
and ${\bf \ast}|_j$ denotes the $j$\textsuperscript{th} element of $\bf \ast$,
$n$ is the number of slots, $w_k$ is the prior probability of slot $k$,
and $\gamma_k$ denotes $q(k|{\bf x})$ for simplicity.

In Equation~\ref{eqn:ELBO_detail}, 
we compute $\boldsymbol{\mu}_x^{(l)}$ as
\begin{equation}
    \boldsymbol{\mu}_x^{(l)}=\fphiseul({\bf z}^{(l)};{\bf \theta}),
\end{equation}
where ${\bf z}^{(l)}$ is
the $l$\textsuperscript{th} sample from $q({\bf z}|{\bf x})$ by Equation~\ref{eqn:q_z_x} to produce
the Monte Carlo samples. According to the {\it reparameterization} trick, ${\bf z}^{(l)}$ is 
obtained by 
\begin{equation}
    {\bf z}^{(l)}=\boldsymbol{\hat\mu}+\boldsymbol{\hat\sigma}\circ\boldsymbol{\epsilon}^{(l)},
\end{equation}
where $\boldsymbol{\epsilon}^{(l)}\sim \mathcal{N}(0 ,{\bf I})$, $\circ$ is element-wise multiplication, and 
$\boldsymbol{\hat\mu}$, $\boldsymbol{\hat\sigma}$ are derived by Equation~\ref{eqn:g_mu_sigma}.

We now describe how to formulate $\gamma_c \triangleq q(k|{\bf x})$ in Equation~\ref{eqn:ELBO_detail} to
maximize the ELBO. Specifically, $\mathcal{L}_{\textup{ELBO}}({\bf x})$
can be rewritten as:
\begin{small}
\begin{flalign}
&\mathcal{L}_{\textup{ELBO}}({\bf x})=E_{q({\bf z},c|{\bf x})}\left[\log \frac{p({\bf x,z},c)}{q({\bf z},c|{\bf x})}\right]\nonumber\\
=&\int_{\bf z}\sum_c q(k| {\bf x})q({\bf z|x})\left[\log\frac{p({\bf x|z})p({\bf z})}{q({\bf z|x})}+\log\frac{p(k|{\bf z})}{q(k|{\bf x})}\right]d{\bf z}\nonumber\\
=&\int_{\bf z}q({\bf z|x})\log\frac{p({\bf x|z})p({\bf z})}{q({\bf z|x})}d{\bf z}
-\int_{\bf z}q({\bf z|x})D_{KL}(q(k|{\bf x})||p(k|{\bf z}))d{\bf z}\label{eq:q_x_prove}
\end{flalign}
\end{small}

% In Equation~\ref{eq:q_x_prove}, the first term has no relationship with $k$ and the second term is non-negative. 
% Hence, to maximize $\mathcal{L}_{\textup{ELBO}}({\bf x})$, $D_{KL}(q(k|{\bf x})||p(k|{\bf z})) \equiv 0$
% should be satisfied. As a result, we use the following equation to compute $q(k|{\bf x})$ in \TimeCSL:
% \begin{equation}
% q(k|{\bf x})=p(k|{\bf z})\equiv\frac{p(k)p({\bf z}|k)}{\sum_{k'=1}^Kp(k')p({\bf z}|k')}
% \label{eqn:p_c_z}
% \end{equation}

% By using Equation~\ref{eqn:p_c_z}, the information loss induced by the mean-field 
% approximation can be mitigated, since $p(k|{\bf z})$ captures the 
% relationship between $c$ and $\bf z$. It is worth noting that $p(k|{\bf z})$
% is only an approximation to $q(k|{\bf x})$,
% and we find it works well in practice\footnote{We approximate $q(k|{\bf x})$ by: 
% 1) sampling a ${\bf z}^{(i)}\sim q({\bf z|x})$; 2) computing $q(k|{\bf x})=p(k|{\bf z}^{(i)})$ 
% according to Equation~\ref{eqn:p_c_z}}.

Once the training is done by maximizing the ELBO w.r.t the parameters of 
$\lbrace \boldsymbol{\pi}, \boldsymbol{\mu}_k, \boldsymbol{\sigma}_k, 
 \boldsymbol{\theta}, \boldsymbol{\phi} \rbrace$, $ k \in \lbrace 1,\cdots, K\rbrace$,
a latent representation $\bf z$ 
can be extracted for each observed sample $\bf x$ by Equation~\ref{eqn:g_mu_sigma} 
and Equation~\ref{eqn:q_z_x}. %, and the clustering assignments can be obtained  by Equation~\ref{eqn:p_c_z}.



\subsubsection{The Equivalence Between Matrix Normal and Multivariate Normal Distributions}\label{app:equivalent_GMM}

In our formulation, we use a vectorization of the matrix \(\rvz \in \mathbb{R}^{d \times n}\), which follows a multivariate Gaussian model.  We now show that this can also be interpreted as a Matrix Normal distribution. The equivalence between the Matrix Normal and the Multivariate Normal density functions can be established using properties of the trace and the Kronecker product.
\begin{proof} 
Let \(\rvz\) be modeled as a mixture of \(J\) Matrix Normal distributions. Each component of this mixture is characterized by a mean matrix \(\boldsymbol{\mu}_j \in \mathbb{R}^{d \times n}\) and a covariance matrix \(\boldsymbol{\Sigma}_j = \boldsymbol\Sigma_n \otimes \Sigma_n \in \mathbb{R}^{d \times d} \otimes \mathbb{R}^{n \times n}\), where \(\boldsymbol\Sigma_n\) and \(\boldsymbol\Sigma_n\) are the row and column covariance matrices, respectively. The probability density function of \(\rvz\) is thus given by

\[
f_{\rvz}(\rvz) = \sum_{j=1}^{J} \omega_j \mathcal{N}(\rvz \mid \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j),
\]

where \(\omega_j\) are the mixing weights such that \(\omega_j > 0\) and \(\sum_{j=1}^{J} \omega_j = 1\).

The Matrix Normal distribution is defined as

\[
\mathcal{N}(\rvz \mid \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j) = \frac{1}{(2\pi)^{\frac{dn}{2}} |\boldsymbol{\Sigma}_j|^{\frac{n+d}{2}}} \exp\left( -\frac{1}{2} \, \text{tr}\left[ \boldsymbol{\Sigma}_d^{-1} (\rvz - \boldsymbol{\mu}_j)^T \boldsymbol{\Sigma}_n^{-1} (\rvz - \boldsymbol{\mu}_j) \right] \right),
\]

where \(\rvz\) is a \(d \times n\) matrix, and the covariance matrix \(\boldsymbol{\Sigma}_j\) is the Kronecker product \(\boldsymbol\boldsymbol\Sigma_n \otimes \boldsymbol\Sigma_n\), with \(\boldsymbol\boldsymbol\Sigma_n\) and \(\boldsymbol\Sigma_n\) being the covariance matrices of the rows and columns of \(\rvz\), respectively.

To connect the Matrix Mixture Normal distribution with the Mixture of Multivariate Normal distributions, we vectorize the matrix \(\rvz\). The vectorization of a matrix \(\rvz \in \mathbb{R}^{d \times n}\) is given by

\[
\text{vec}(\rvz) = \begin{bmatrix}
z_{11} & z_{21} & \cdots & z_{d1} & z_{12} & \cdots & z_{dn}
\end{bmatrix}^T \in \mathbb{R}^{1 \times (d \cdot n)}
\]



where \(\rvz_i\) denotes the \(i\)-th column of \(\rvz\), and the resulting vector \(\text{vec}(\rvz)\) is a \(d \cdot n\)-dimensional vector.

Now, substituting the vectorized form of \(\rvz\) into the Matrix Normal distribution, we have

\begin{align}
\mathcal{N}(\text{vec}(\rvz) \mid \text{vec}(\boldsymbol{\mu}_j), \boldsymbol{\Sigma}_j) = \frac{1}{(2\pi)^{\frac{dn}{2}} |\boldsymbol{\Sigma}_j|^{\frac{d+n}{2}}} \exp\left( -\frac{1}{2} \Bar{\rvz}^T \boldsymbol{\Sigma}_{j}^{-1}\Bar{\rvz} \right),
\end{align}
where $\Bar{\rvz}=\text{vec}(\rvz) - \text{vec}(\boldsymbol{\mu}_j)$. Next, observe that the mixture model for \(\rvz\) in the original form becomes

\begin{align}
f_{\rvz}(\rvz) = \sum_{j=1}^{J} \omega_j \mathcal{N}(\text{vec}(\rvz) \mid \text{vec}(\boldsymbol{\mu}_j), \boldsymbol\Sigma_n \otimes \Sigma_n),
\end{align}

which is a mixture of multivariate normal distributions in the vectorized space \(\mathbb{R}^{d \cdot n}\). This shows that the Matrix Mixture Normal distribution is equivalent to a Mixture of Multivariate Normal distributions upon vectorization. To complete the proof, we use the determinant property of the Kronecker product:
\begin{align}
|\boldsymbol\Sigma_n \otimes \boldsymbol\Sigma_n| = |\boldsymbol\Sigma_n|^n |\boldsymbol\Sigma_n|^d.
\end{align}
Thus, the determinant of the covariance matrix \(\boldsymbol\Sigma_n \otimes \boldsymbol\Sigma_n\) can be written as the product of the determinants of \(\boldsymbol\Sigma_n\) and \(\boldsymbol\Sigma_n\), raised to the appropriate powers. This confirms that the matrix mixture normal distribution is indeed equivalent to the mixture of multivariate normal distributions.
\end{proof}


\subsection{Assumption Structural Variability and Partial}\label{app:assumption_validation}


Assumption-1~\citep{ng2023identifiability} stipulates that for any pair of sources \(k\) and \(\ell\), the supports of the corresponding columns in the mixing matrix \( \mA \) (denoted \( \rva_k \) and \( \rva_\ell \)) must differ in at least two observed variables, i.e.,

\[
| \text{supp}(\rva_k) \cup \text{supp}(\rva_\ell) | - | \text{supp}(\rva_k) \cap \text{supp}(\rva_\ell) | > 1.
\]

\textbf{Example.1 (Assumption-1 fails)} This ensures distinct influences across observed variables. If the supports are nearly identical, Assumption-1 fails. For example, consider the mixing matrix \( \mA \):

\[
\begin{bmatrix}
\rvx_1(t) \\
\rvx_2(t) \\
\rvx_3(t) \\
\rvx_4(t)
\end{bmatrix}
=
\begin{bmatrix}
1 & 0.5 & 0 & 0.2 \\
0.3 & 1 & 0.4 & 0 \\
0 & 0.2 & 1 & 0.5 \\
0.1 & 0 & 0.6 & 1
\end{bmatrix}
\begin{bmatrix}
\vy_1(t) \\
\vy_2(t) \\
\vy_3(t) \\
\vy_4(t)
\end{bmatrix}
+ \epsilon
\]

with supports \( \text{supp}(\rva_1) = \{1, 2, 4\} \), \( \text{supp}(\rva_2) = \{1, 2, 3\} \), \( \text{supp}(\rva_3) = \{2, 3, 4\} \), and \( \text{supp}(\rva_4) = \{1, 3, 4\} \). For \( \vy_1 \) and \( \vy_2 \), the difference in support is \(2\) (validating Assumption-1), as is the case for \( \vy_3 \) and \( \vy_4 \). However, the significant overlap in the observed variables they influence (\( \vy_1 \) and \( \vy_2 \) both affect \( \rvx_1(t), \rvx_2(t) \), and \( \vy_3 \) and \( \vy_4 \) affect \( \rvx_3(t), \rvx_4(t) \)) limits the ability to uniquely identify each source, pointing to a practical challenge in real-world data.

% According to Assumption-1~\citep{ng2023identifiability}, for every pair of sources \( k \) and \( \ell \), the supports of the corresponding columns in \( \mA \) (denoted as \( \rva_k \) and \( \rva_\ell \)) must differ in more than one observed variable. Mathematically:

% \begin{equation}
% | \text{supp}(\rva_k) \cup \text{supp}(\rva_\ell) | - | \text{supp}(\rva_k) \cap \text{supp}(\rva_\ell) | > 1.
% \end{equation}

% This requires that the supports of \( \rva_k \) and \( \rva_\ell \) differ in at least two observed variables. If two sources \( \vy_k \) and \( \vy_\ell \) have nearly identical supports (e.g., they mostly influence the same observed variables), then \textbf{Assumption 1 fails}.

% Let the mixing matrix \( \mA \) be:
% \begin{align}
% \mA =\begin{bmatrix}
% 1 & 0.5 & 0 & 0.2 \\
% 0.3 & 1 & 0.4 & 0 \\
% 0 & 0.2 & 1 & 0.5 \\
% 0.1 & 0 & 0.6 & 1
% \end{bmatrix}.
% \end{align}

% The supports of the columns of \( \mA \) are:
% \begin{itemize}
%     \item \( \text{supp}(\rva_1) = \{1, 2, 4\} \),
%     \item \( \text{supp}(\rva_2) = \{1, 2, 3\} \),
%     \item \( \text{supp}(\rva_3) = \{2, 3, 4\} \),
%     \item \( \text{supp}(\rva_4) = \{1, 3, 4\}. \).
% \end{itemize}

% Let us analyze whether Assumption 1 holds for all pairs of sources:

% \begin{enumerate}
%     \item \textbf{\( \vy_1 \) and \( \vy_2 \):}  
%     \[
%     \text{supp}(\rva_1) = \{1, 2, 4\}, \quad \text{supp}(\rva_2) = \{1, 2, 3\}.
%     \]
%     \textbf{Union:} \( \{1, 2, 3, 4\} \), \textbf{Intersection:} \( \{1, 2\} \).  
%     Difference in support:
%     \[
%     | \text{Union} | - | \text{Intersection} | = 4 - 2 = 2.
%     \]
%     Assumption 1 holds for this pair.

%     \item \textbf{\( \vy_3 \) and \( \vy_4 \):}  
%     \[
%     \text{supp}(\rva_3) = \{2, 3, 4\}, \quad \text{supp}(\rva_4) = \{1, 3, 4\}.
%     \]
%     \textbf{Union:} \( \{1, 2, 3, 4\} \), \textbf{Intersection:} \( \{3, 4\} \).  
%     Difference in support:
%     \[
%     | \text{Union} | - | \text{Intersection} | = 4 - 2 = 2.
%     \]
%     Assumption 1 holds for this pair.
% \end{enumerate}

% \textbf{The Real Issue: Overlap in Observed Variables} The key observation is that the support structure of the sources \( \vy_1, \vy_2 \) and \( \vy_3, \vy_4 \) suggests significant overlap in the observed variables they influence:
% \begin{itemize}
%     \item \( \vy_1 \) and \( \vy_2 \) both influence \( \rvx_1(t) \) and \( \rvx_2(t) \).
%     \item \( \vy_3 \) and \( \vy_4 \) both influence \( \rvx_3(t) \) and \( \rvx_4(t) \).
% \end{itemize}

% This creates a situation where disentangling these sources becomes challenging because the observed variables \( \rvx_1(t), \rvx_2(t), \rvx_3(t), \rvx_4(t) \) do not contain enough distinct information to uniquely identify each source. This is a \textbf{practical limitation} in real-world time series data, where overlapping influences are common.



\newpage
\section{Experiments and Implementation Settings}\label{appendix:setup}

\subsection{Implementation source. (TimeCSL-Lib)}

We have implemented the ResTimeCSL architecture from scratch, and our code is available at \url{https://anonymous.4open.science/r/TimeCSL-4320}. Some components of our code are inspired by the following works:
\begin{itemize}
    \item The GMM-based VAE sampling is inspired by VaDE \citep{jiang2016variational}, and we adapted the implementation from \url{https://github.com/mperezcarrasco/Pytorch-VaDE}.
    \item For the Diffusion model D3VAE \citep{li_generative_2023}, we utilized the authors' implementation from \url{https://github.com/PaddlePaddle/PaddleSpatial/tree/main/research/D3VAE}.
    \item Regarding the methods listed in \Cref{app:related_work}, the TCL model was adapted from \url{https://github.com/hmorioka/TCL/tree/master/tcl}, while the other models are derived from \url{https://github.com/rpatrik96/nl-causal}.
    \item For iVAE \citep{khemakhem2020variational}, we used the implementation available at \url{https://github.com/MatthewWilletts/algostability}.
\end{itemize}

Our experiments were conducted with 5 different random seeds, and we report the average results along with standard deviations. The experiments were run using 8 NVIDIA A100 GPUs.




\subsection{Datasets.}
In this section, we provide details about the datasets used for our experiments. We consider both real-world and synthetic datasets, each with specific characteristics relevant to the study. The table below summarizes the key properties of these datasets, including the number of samples, input dimensions, the number of sources/factors, and the names of the factors. The real-world datasets include REDD, REFIT, and UKDALE, which are commonly used in energy consumption modeling. Additionally, we employ synthetic datasets (Synthetic-1, Synthetic-2, and Synthetic-3) to simulate various scenarios with varying factors and input sizes. These datasets allow for comprehensive testing of our proposed method across different contexts.
\begin{table}[H]
\caption{Synthetic Dataset  and Real world datasets}
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{llcccc}
\hline
Dataset & $\#$ Samples & Input Dim & $\#$ Sources/Factors & Factors name \\
\toprule
REDD    & $5400$& $256 $& $3$ &~\RealFactors~\\
REFIT    & $1299$& $256 $ &$5$ &~\RealFactors~\\
UKDALE  & $1300$& $256 $ &$5$ &~\RealFactors~\\
Synthetic-1  &$12000$& $24$&$3$ &~\SynFactors~\\
Synthetic-2  &$11000$& $96$&$5$  &~\SynFactors\\
Synthetic-3  &$11000$& $64$&$3$  &~\SynFactors~\\
\bottomrule
\end{tabular}}
\end{center}
\label{table:dataset}
\end{table}


\subsection{Contrastive Partial Selective Pairing - Data Augmentations}\label{app:experiments}
\label{app:augmentations}
Four augmentations were sequentially applied to all contrastive methods' pipeline branches. The parameters from the random search are: \textcolor{blue}{1}) \textbf{Crop and delay:} applied with a $0.5$ probability and a minimum size of $50\%$ of the initial sequence. \textcolor{blue}{2}) \textbf{Cutout or Masking:} time cutout of $5$ steps with a $0.8$ probability. \textcolor{blue}{3}) \textbf{Channel Masks powers:} each time series is randomly masked out with a $0.4$ probability. \textcolor{blue}{4}) \textbf{Gaussian noise:} random Gaussian noise is added to window input $\mathbf{x}$ with a standard deviation form $0.1$ to $0.3$. Further details in \Cref{app:partial_selective}. Also in our experiments, we utilize a composition of three data augmentations, applied in the following order - scaling, shifting, and jittering, activating with a probability of $0.3$ to $0.5$.
\paragraph{Scaling}
The time-series is scaled by a single random scalar value, obtained by sampling $\epsilon \sim \mathcal{N}(0, 0.5)$, and each time step is $\prvx_t = \epsilon x_t$.
\paragraph{Shifting}
The time-series is shifted by a single random scalar value, obtained by sampling $\epsilon \sim \mathcal{N}(0, 0.5)$ and each time step is $\prvx_t = x_t + \epsilon$.
\paragraph{Jittering}
I.I.D. Gaussian noise is added to each time step, from a distribution $\epsilon_t \sim \mathcal{N}(0, 0.5)$, where each time step is now $\prvx_t = x_t + \epsilon_t$.



\subsection{Implementation of Metrics and study case}\label{appendix:disnetangelement}\label{appendix:TDS_metric}
Previous work has relied on the Mean Correlation Coefficient (MCC) as a metric to quantify identifiability. For consistency with previous work, we report this metric, but also propose a new metric to quantify identifiability up to an affine transformation.
There are two challenges in designing such a metric:
Firstly, for two Gaussian mixtures, standard distance metrices such as TV-distance or KL-divergence do not have a closed form. Secondly, we need to find an affine map $A$ that best aligns a pair of Gaussian mixtures. 
Therefore, developing a metric to quantify identifiability up to an affine transformation has natural challenges. 
We propose $\vd_{\text{ aff }, L2}$, defined below, as an additional metric in this setting.

\subsubsection{Alignment prior to measuring Weak \MCC}\label{app:alignement}
We seek an affine map \( \Gamma \) to align two GMMs using two methods. One approach, used in previous works on MCC, is Canonical Correlation Analysis (CCA). Alternatively, we explore a different method. For two GMMs, we iterate over all permutations of the components, and for each permutation, we compute the optimal map \( \Gamma \) that aligns the components. While ideally \( \Gamma \) would align both the means and the covariance matrices, solving this as an optimization problem is challenging. Thus, we focus on aligning the means of the first GMM to those of the second GMM. The map \( \Gamma \) is found by solving the least-squares problem:
\begin{equation}
\min_{\Gamma} \sum_{i} \| \mu_1^{(i)} - \Gamma \mu_2^{(i)} \|^2
\end{equation}
This can be efficiently solved using Singular Value Decomposition (SVD). Empirically, aligning the means provides good results.


\subsubsection{Measuring Identifiability strong-MCC and weak-MCC}
The other metric we consider is the Mean Correlation Coefficient (MCC) metric which had been used in prior works \citep{iVAEkhemakhem20a}. There are two versions of MCC that have been used:
\begin{enumerate}
    \item The \emph{weak} MCC is defined to be the MCC after alignment via the affine map $\Gamma$ transformation see \Cref{app:alignement}.
    \item The \emph{strong} MCC is defined to be the MCC before alignment.
\end{enumerate}

Furthermore, in this work, we consider two different metrics.
For a pair of distributions $p_1, p_2$, we define $\vd_{\text{ aff }, L2}$ loss as
\begin{equation}
  \vd_{\text{ aff }, L2}(p_1, p_2) =  \min\limits_{\substack{A:\mathbb{R}^m\rightarrow \mathbb{R}^m,\\ \text{affine}}} 
  \Delta_{L_2}(\Gamma_{\sharp}p_1, p_2), \quad \text{where}\quad   \Delta_{L_2}(p_1, p_2) = \dfrac{ \norm{p_1 - p_2}_{L_2}}{\norm{p_1}^{1/2}_{L_2}\norm{p_2}_{L_2}^{1/2}}
\end{equation}

In our experiments, we report both the strong \MCC and weak \MCC. Moreover, all reported \MCC~s are out-of-sample, i.e. the optimal affine map $\Gamma$ is computed over half the dataset and then reused for the other half of the dataset.


\subsubsection{Measuring disentanglement of the learned representation}
In implementing the disentanglement metrics, we adhere to the methodology outlined in \citep{locatello2019challenging}, expanding it to accommodate time series data. For the computation of DCI metrics, we employ a gradient boosted tree from the scikit-learn package.




\paragraph{$\beta$-VAE,  Metric}
Disentanglement is then measured as the accuracy of a linear classifier that predicts the index of the fixed factor based on the coordinate-wise sum of absolute differences between the representation vectors in the two mini-batches.  \citep{higgins_beta-vae_2016} suggest fixing a random attributes of variation in the underlying generative model and sampling two mini-batches of observations $\rvx$. We sample two batches of 256 points with a random factor fixed to a randomly sampled value across the two batches, and the others varying randomly. We compute the mean representations for these points and take the absolute difference between pairs from the two batches. We then average these 64 values to form the features of a training (or testing) point.

\paragraph{FactorVAE Metric~\citep{kim_disentangling_2019}}

\citep{kim_disentangling_2019} address several issues with this metric by using a majority vote classifier that predicts the index of the fixed ground-truth attribute based on the index of the representation vector with the least variance. First, we estimate the variance of each latent dimension by embedding $10k$ random samples from the data set, excluding collapsed dimensions with variance smaller than .05. Second, we generate the votes for the majority vote classifier by sampling a batch of 64 points, all with a factor fixed to the same random value. Third, we compute the variance of each dimension of their latent representation and divide it by the variance of that dimension computed on the data without interventions. The training point for the majority vote classifier consists of the index of the dimension with the smallest normalized variance. We train on $10k$ points and evaluate on $5k$ points.

\paragraph{Mutual Information Gap Metric \citep{chen_isolating_2018}}
$\beta$-VAE metric and the FactorVAE metric are neither general nor unbiased as they depend on some hyperparameters~\citep{chen_isolating_2018}. They compute the mutual information between each ground-truth factor and each dimension in the computed representation $r(\rvx)$. For each ground-truth factor $\vz_k$, they then consider the two dimensions in $r(\rvx)$ that have the highest and second highest mutual information with $\vz_k$. The Mutual Information Gap (MIG) is then defined as the average, normalized difference between the highest and second highest mutual information of each factor with the dimensions of the representation. The original metric was proposed evaluating the sampled representation. Instead, we consider the mean representation, in order to be consistent with the other metrics. We estimate the mutual information by binning each dimension of the representations. Then, the score is computed as follows:
\[
MIG = \frac{1}{K} \sum_{k=1}^{K} \left[ I(v_{jk},z_k) - \max I(v_j,z_k) \right]
\]

Where $\vz_k$ is a factor of variation, $\vv_i$ is a dimension of the latent representation.

MIG [8] computes the MI between each code and factor I(vi
, zj). Then the code dimension with maximum MI is
identified I(vi
, z⋆) for each factor. Next, the second highest MI, I(vi
, z◦), is subtracted from this maximal value. This
difference constitutes the gap. The gap is then normalized by the entropy of the factor:
MIG =
I(vi
, z⋆) − I(vi
, z◦)
H(vi)
(3)
The MIG score of all factors are averaged to report one score.
Robust MIG (RMIG) was proposed in [9]. It is identical to MIG in essence, but proposes a more robust formulation
when MI is computed from the input space, which does not apply in our context. For the remainder of the paper we
will refer to both MIG and RMIG as MIG-RMIG because our results apply to both in the same way.


\paragraph{Disentanglement, Completeness and Informativeness (DCI)}
In \citep{carbonneau_measuring_2022}, a framework is proposed to evaluate disentangled representations using metrics for modularity, compactness, and explicitness, referred to as disentanglement, completeness, and informativeness. Regressors predict factors from codes, with modularity and compactness estimated by importance weights \(R_{ij}\). These weights are computed using a lasso regressor or random forests. The compactness for factor \( \rvv_i \) is defined as:
\[
C_i = 1 + \sum_{j=1}^d p_{ij} \log_d p_{ij}, \quad p_{ij} = \frac{R_{ij}}{\sum_{k=1}^d R_{ik}}.
\]
Compactness for the entire representation is the average over all factors. The modularity for code dimension \( \rvz_j \) is:
\[
D_j = 1 + \sum_{i=1}^M p_{ij} \log_M p_{ij}, \quad p_{ij} = \frac{R_{ij}}{\sum_{k=1}^M R_{kj}}.
\]
The modularity score is the weighted average over all code dimensions, with weights \( \rho_j \) reflecting their importance in predicting factors. Explicitness is defined by the MSE of the regressor, normalized between 0 and 1:
\[
\text{Explicitness} = 1 - 6 \cdot \text{MSE}, \quad \text{MSE} = E[(\rvx - \rvy)^2] = \frac{1}{6}.
\].
\paragraph{Time Disentanglement Score TDS} Time series data often exhibit variations that may not always align with conventional metrics, especially when considering the presence or absence of underlying attributes. To address this challenge, \citep{oublal2024disentangling} introduce the Time Disentanglement Score (TDS), a metric designed to assess the disentanglement of attributes in time series data. The foundation of TDS lies in an Information Gain perspective, which measures the reduction in entropy when an attribute is present compared to when it's absent.

\begin{equation}
    TDS = \frac{1}{dim(\rvz)}\sum_{n\neq m}\sum_{k} \frac{||z_{m} - z^{+}_{n,k}||^{2}}{\mathrm{Var}[z_{m}]},
\end{equation}
\vspace{-0.3cm}

% Augmentation of Factor m
In the context of TDS, we augment factor $m$ in a time series window $\mathbf{x}$ with a specific objective: to maintain stable entropy when the factor is present and reduce entropy when it's absent. This augmentation aims to capture the essence of attribute-related information within the data.

% TDS relies on the correlation matrix between $\rvz$ and $\mathbf{z^{+}}$, where $\rvz=f_{\phi}(\mathbf{x})$ and $\mathbf{z^{+}}=f_{\phi}(\mathcal{T}(\mathbf{x}))$, with $\mathcal{T}$ denoting an augmentation function. This correlation matrix quantifies the consistency of attribute components. Additionally, TDS evaluates how well $\rvz$ contributes to the reconstruction of $\mathbf{y}$ and how $\mathbf{z^{+}}$ contributes to the reconstruction of $\mathcal{T}(\mathbf{y})$. Specifically, it assesses whether each $z_{m}$ (or $z_{m}^{+}$) can effectively reconstruct the corresponding $y_{m}$ (or $y_{m}^{+}$). TDS aligns with qualitative observations of disentanglement.

% \begin{equation}
% \textit{TDS} = \frac{1}{2} \left[ (1-\sum_{i} Corr^{(I)}{(\rvz,\mathbf{z^{+}})}_{ii})^{2} + (1-\sum_{i} Corr^{(I)}(\mathbf{y-\hat{y}},\mathbf{y^{+}-\hat{y}^{+})}_{ii}) \right]^{2}
% \end{equation}


% \subsection{DIOSCussion on Relation between Disentangling And Contrastive Learning Under Causality assumptions}\label{appendix:CAUSAL_and_contrastive^{(l)}earning}
% In this section, we delve into a comprehensive DIOSCussion on the intricate relationship between disentangling and contrastive learning under the foundational framework of causality assumptions. Unraveling the interplay between these two pivotal concepts is crucial for understanding their synergies and implications within the context of the broader research landscape. By examining their connections through the lens of causality, we aim to illuminate the nuanced dynamics that shape the effectiveness and interpretability of disentangled representations achieved through contrastive learning methodologies. 


\subsection{Pipeline Correlated samples.}\label{appendix:pipeline_correlated_sampling} Robustness of the model to correlations between data is assessed by examining different pairs. We focus mainly on linear correlations between two different devices and on the case where one device correlates with two others. To do this, we parameterize the correlations by sampling a dataset from the common distribution. We build on the correlation time series framework by introducing a pairwise correlation between the attributes $y_{m}$ and $y_{n}$ as follows: $p(y_{m}, y_{n}) \propto \exp\left(-||y_{m} - \alpha y_{n}||^2/2\sigma^2\right)$, where $\alpha$ is a scaling factor. A high value of $\sigma$ indicates a lower correlation between the normalised attributes $y_{m}$ and $y_{n}$ (No.Corr, $\sigma=\infty$). We also extend this framework to cover correlations between several attributes in the time window $T$. Therefore, we consider correlation pair scenarios such as : \textit{No correlation}; \textit{Pair:1 } washer-dryer; \textit{Pair:2 } dryer-oven and, finally, a \textit{Random pair:} approach with randomly selected appliances.
\begin{itemize}
  \item \textbf{No Corr.:} No Correlation during training. (default evaluation setting) 
  \item \textbf{Pair: 1} washer-dryer.
  \item \textbf{Pair: 2} dryer-oven.
  \item \textbf{Pair: 3} lighting and television.
  \item \textbf{Pair: 4} microwave and oven.
  \item \textbf{Pair: 5} washer-dishwasher.
  \item \textbf{Random Pairs},  
\end{itemize}




\subsection{Impact of GELU vs ReLU/LeakyReLU on Decoder Behavior}

In this study, we evaluate the impact of different activation functions on the decoder's behavior to satifies \Cref{assump:injective_piecewise}. Specifically, we compare the use of ReLU (a piecewise affine activation) and GELU (a smooth, nonlinear activation) within an MLP decoder. The results suggest that the choice of activation function has a significant impact on the latent representation produced by the model.

\paragraph{ReLU Activation:}
The decoder becomes piecewise affine, meaning that it can be broken down into affine transformations over different regions of the input space. This causes the decoder to create latent representations that reflect distinct linear transformations in various regions of the input. As a result, the learned latent space is structured around these distinct affine regions, potentially making the model more sensitive to certain regions of the data space and leading to more discrete or sharply defined latent representations.

\paragraph{LeakyReLU Activation:}
In contrast, the GELU activation is smooth and nonlinear across the entire input space. This means that the decoder no longer operates piecewise affine, and the latent space learned by the model is more continuous and smooth. Since GELU smoothly transforms the input, it enables the decoder to create more nuanced, continuous latent representations. The absence of piecewise linear behavior allows for better modeling of complex, smooth relationships in the data, which may improve generalization to unseen data or tasks that require such smooth transformations.



\subsection{Validation of results on synthetic Data Generation}
\label{app:synthetic}
\
We first construct two trend patterns. The first trend patterns follows a nonlinear, saturating pattern, $y_t = \frac{1}{1 + \exp{\beta_1(t-\beta_1)}} + \epsilon_t $ for $\beta_1=0.2, \beta_1=60, \epsilon_t \sim \mathcal{N}(0,0.3)$. The second pattern is a mixture of ARMA process, $\mathrm{ARMA}(2,2) + \mathrm{ARMA}(3,3) + \mathrm{ARMA}(4,4)$, where the AR and MA parameters are as follows, (\{.9, -.1\}, \{.2, -.5\}), (\{.1, .2, .3\}, \{.1, .65, -.45\}), (\{.3, .5, -.5, -.3\}, \{.1, .1, -.2, -.3\}). Next, we construct three seasonal patterns, consisting of sine waves with the following period, phase, and amplitudes, \{20, 0, 3\}, \{50, .2, 3\}, \{100, .5, 3\}.
The final time series are constructed as follows, generate a trend pattern $\fphi(t)$ and seasonal pattern $s(t)$, the final time series is $y(t) = \fphi(t) + s(t), t={0,\ldots,999}$. We do this for all pairs of trend and seasonal patterns, constructing a total of 6 time series.





\subsection{\pIs that sparsity enough for Robustness, is donwstream task ?} 
\label{app:impact_about_sparsity}
\begin{table}
\centering
\caption{Average performance, considering factors~\RealFactors~with 5 seed on real datasets REFIT and REDD. Metrics reported are: \DCI, \RMIG and \RMSE. Lower values are better for all metrics. ($\downarrow$ lower is better, $\uparrow$ higher is worse  {\small \colorbox{gray!25}{Top-1}, \colorbox{gray!10}{Top-2}}). \label{tab:results}}
\resizebox{\textwidth}{!}{
\begin{tabular}{p{.4cm}p{3.5cm}|rrr|rrr|rrr|rrr}\toprule
\bf Sc. & \bf Methods &\multicolumn{3}{c}{$\boldsymbol\sigma=\infty$} &\multicolumn{3}{c}{ $\boldsymbol\sigma=0.3$} & \multicolumn{3}{c}{ $\boldsymbol\sigma=0.8$} \\ \midrule
& \bf Metrics $\Rightarrow$ & \bf \DCI $\downarrow$ & \bf \RMIG$\downarrow$ & \bf \RMSE $\downarrow$ & \bf \DCI $\downarrow$ & \bf \RMIG$\downarrow$ & \bf \RMSE $\downarrow$ & \bf \DCI $\downarrow$ & \bf \RMIG$\downarrow$ & \bf \RMSE $\downarrow$ \\ \midrule
\multirow{7}{*}{\textbf{\rotatebox{90}{\shortstack{{REFIT}}}}}& \BertNILMcolor BertNILM &- &- &36.86  $\pm$ 1.68 &- &- &45.84  $\pm$  1.00 &- &- &46.29  $\pm$  0.76 \\
& \StoScolor S2S &- &- &35.46  $\pm$ 2.04 &- &- &45.36 $\pm$ 2.47 &- &- &45.76 $\pm$ 2.26 \\
& \Autoformercolor Autoformer &- &- &32.45  $\pm$  0.56 &- &- &33.02  $\pm$  1.49 &- &- &34.68  $\pm$  1.13 \\
& \Informercolor Informer &- &- &32.92  $\pm$  1.67 &- &- &35.03  $\pm$  1.71 &- &- &38.47  $\pm$  1.54 \\
&  \TimesNetcolor TimesNet &- &- &32.12  $\pm$  1.99 &- &- &33.38  $\pm$  1.83 &- &- &35.84  $\pm$  1.61 \\
&  \CoSTcolor CoST &44.68 $\pm$ 1.57 &0.61 $\pm$ 0.02 &31.14  $\pm$ 0.93 &48.01 $\pm$ 1.57 & 0.64 $\pm$ 0.09 &34.81  $\pm$ 0.71 &46.98 $\pm$ 1.13 & 0.65 $\pm$ 0.01 &38.14 $\pm$ 0.57 \\
&  \SVAEcolor SlowVAE &50.96 $\pm$ 0.71 &0.61 $\pm$ 0.09 &28.26  $\pm$ 1.54 &53.04 $\pm$ 1.26 & 0.61 $\pm$ 0.09 &32.15  $\pm$ 0.78 &52.14 $\pm$ 0.58 &0.70 $\pm$ 0.08 &35.74 $\pm$ 1.03 \\
&  \SVAEHDFcolor SlowVAE+HDF &52.17  $\pm$  0.07 & 0.42  $\pm$  0.02 &37.35  $\pm$  1.49 &53.00  $\pm$  0.12 & 0.46  $\pm$  0.05 &38.86  $\pm$  1.26 &52.53 $\pm$ 0.03 &0.47 $\pm$ 0.01 &40.22 $\pm$ 1.06 \\
& \TDRLcolor TDRL &42.34 $\pm$ 1.02 & 0.28 $\pm$ 0.04 &18.64 $\pm$ 1.41 &49.75 $\pm$ 0.87 & 0.31 $\pm$ 0.01 &17.18 $\pm$ 1.36 &50.43 $\pm$ 0.69 & 0.38 $\pm$ 0.08 &20.91 $\pm$ 1.07 \\
& \CDSVAEcolor D3VAE &41.30 $\pm$ 1.97 & 0.26 $\pm$ 0.05 &27.64 $\pm$ 1.40 &41.55 $\pm$ 0.91 & 0.33 $\pm$ 0.26 &30.11 $\pm$ 1.10 &43.47 $\pm$ 1.31 & 0.44 $\pm$ 0.03 &32.77 $\pm$ 0.51 \\
&  \CDSVAEcolor C-DSVAE &47.35 $\pm$ 2.14 & 0.59 $\pm$ 0.05 &31.78  $\pm$ 1.61 &47.79 $\pm$ 0.99 & 0.62 $\pm$ 0.26 &34.55  $\pm$ 1.18 &50.02 $\pm$ 1.42 &0.71 $\pm$ 0.03 &37.57 $\pm$ 0.53 \\
& \CDSVAEHDFcolor C-DSVAE + HDF  &44.31 $\pm$ 1.93 & 0.56 $\pm$ 0.05 &29.68 $\pm$ 1.51 &45.01 $\pm$ 0.92 & 0.59 $\pm$ 0.25 &32.42 $\pm$ 1.04 &46.68 $\pm$ 1.33 & 0.66 $\pm$ 0.03 &35.12 $\pm$ 0.50 \\
&  \SparseVAEcolor SparseVAE &40.15 $\pm$ 0.86 & 0.25 $\pm$ 0.09 &13.72 $\pm$ 1.30 &43.98 $\pm$ 0.81 & 0.28 $\pm$ 0.21 &14.81 $\pm$ 1.20 &44.53 $\pm$ 0.58 &0.31 $\pm$ 0.07 &18.89 $\pm$ 1.30 \\
&  \TimeCSLcolor \TimeCSL &\first 39.02 $\pm$ 0.87 &\first 0.23 $\pm$ 0.07 &\first 12.03 $\pm$ 1.26 &42.51 $\pm$ 0.74 &\first 0.27 $\pm$ 0.15 &\first 12.72 $\pm$ 1.16 &42.91 $\pm$ 0.59 &\first 0.31 $\pm$ 0.05 &\first 14.76 $\pm$ 0.92 \\ \cmidrule{2-11}
 & \bf Avg.  & 45.62 $\pm$ 1.27 & 0.52 $\pm$ 0.07 &31.02 $\pm$ 1.26 &48.02 $\pm$ 0.85 & 0.58 $\pm$ 0.12 &34.08 $\pm$ 1.04 &48.92 $\pm$ 1.18 & 0.64 $\pm$ 0.06 &35.67 $\pm$ 0.91 \\ \cmidrule{1-11}
\multirow{7}{*}{\textbf{\rotatebox{90}{\shortstack{{REDD}}}}} & \BertNILMcolor BertNILM & - & - & 40.06  $\pm$  2.41 & - & - & 44.14  $\pm$  1.22 & - & - & 45.04  $\pm$  0.99 \\
& \StoScolor S2S & - & - & 38.48  $\pm$  2.87 & - & - & 45.07 $\pm$  2.71 & - & - & 46.22 $\pm$  2.26 \\
& \Autoformercolor Autoformer & - & - & 33.56  $\pm$  0.79 & - & - & 34.13  $\pm$  2.07 & - & - & 37.51  $\pm$  1.81 \\
& \Informercolor Informer & - & - & 36.02  $\pm$  2.37 & - & - & 37.61  $\pm$  1.98 & - & - & 38.81  $\pm$  2.36 \\
& \TimesNetcolor TimesNet & - & - & 36.69  $\pm$  2.08 & - & - & 39.08  $\pm$  2.71 & - & - & 42.55  $\pm$  2.35 \\
& \CoSTcolor CoST & 50.87 $\pm$ 1.13 & 0.58 $\pm$ 0.06 & 28.93  $\pm$  1.81 & 53.10 $\pm$ 1.23 & 0.61 $\pm$ 0.14 & 30.72 $\pm$ 1.31 & 52.63 $\pm$ 1.19 & 0.67 $\pm$ 0.14 & 33.15 $\pm$ 1.12 \\
& \SVAEcolor SlowVAE & 48.11 $\pm$ 1.06 & 0.45 $\pm$ 0.05 & 31.73 $\pm$ 2.19 & 50.15 $\pm$ 1.35 & 0.47 $\pm$ 0.06 & 34.12  $\pm$  1.57 & 50.97 $\pm$ 0.78 & 0.55 $\pm$ 0.02 & 35.27 $\pm$ 1.06 \\
& \SVAEHDFcolor SlowVAE + HDF & 51.09 $\pm$ 1.64 & 0.34 $\pm$ 0.04 & 32.85 $\pm$ 2.40 & 51.97 $\pm$ 1.07 & 0.39 $\pm$ 0.05 & 35.72 $\pm$ 2.17 & 51.85 $\pm$ 1.58 & 0.43 $\pm$ 0.06 & 37.38 $\pm$ 2.51 \\
& \TDRLcolor TDRL & 45.12 $\pm$ 2.15 & 0.39 $\pm$ 0.05 & 22.87 $\pm$ 1.36 & 50.61 $\pm$ 1.53 & 0.44 $\pm$ 0.03 & 23.98 $\pm$ 1.41 & 51.18 $\pm$ 0.90 & 0.49 $\pm$ 0.08 & 27.13 $\pm$ 2.30 \\
& \CDSVAEcolor D3VAE & 43.77 $\pm$ 1.31 & 0.36 $\pm$ 0.06 & 28.43 $\pm$ 1.61 & 46.17 $\pm$ 0.86 & 0.39 $\pm$ 0.04 & 30.14 $\pm$ 1.35 & 48.02 $\pm$ 1.23 & 0.44 $\pm$ 0.06 & 32.46 $\pm$ 1.10 \\
& \CDSVAEcolor C-DSVAE & 49.68 $\pm$ 2.12 & 0.55 $\pm$ 0.07 & 31.03 $\pm$ 2.15 & 49.92 $\pm$ 1.05 & 0.58 $\pm$ 0.08 & 33.60 $\pm$ 1.77 & 51.51 $\pm$ 1.76 & 0.61 $\pm$ 0.03 & 35.38 $\pm$ 1.42 \\
& \CDSVAEHDFcolor C-DSVAE + HDF & 47.38 $\pm$ 1.19 & 0.53 $\pm$ 0.05 & 30.76 $\pm$ 2.13 & 48.85 $\pm$ 1.62 & 0.56 $\pm$ 0.03 & 32.89 $\pm$ 2.04 & 49.98 $\pm$ 1.34 & 0.60 $\pm$ 0.05 & 34.25 $\pm$ 1.22 \\
& \SparseVAEcolor SparseVAE & 46.56 $\pm$ 2.49 & 0.44 $\pm$ 0.08 & 19.88 $\pm$ 2.06 & 50.49 $\pm$ 1.07 & 0.47 $\pm$ 0.06 & 21.42 $\pm$ 2.53 & 50.83 $\pm$ 1.73 & 0.53 $\pm$ 0.05 & 23.59 $\pm$ 2.17 \\
& \TimeCSLcolor \TimeCSL & \first 43.45 $\pm$ 1.12 & \first 0.33 $\pm$ 0.02 & \first 16.32 $\pm$ 2.16 & \first 47.33 $\pm$ 1.29 & \first 0.35 $\pm$ 0.04 & \first 17.22 $\pm$ 2.01 & \first 48.09 $\pm$ 0.81 & \first 0.39 $\pm$ 0.06 & \first 18.95 $\pm$ 2.08 \\ \cmidrule{2-11}
 & \bf Avg.  & 47.02 $\pm$ 1.56 & 0.45 $\pm$ 0.06 & 28.04 $\pm$ 1.84 & 50.43 $\pm$ 1.19 & 0.48 $\pm$ 0.09 & 30.32 $\pm$ 1.56 & 50.95 $\pm$ 1.26 & 0.54 $\pm$ 0.07 & 32.83 $\pm$ 1.57 \\ \bottomrule
\end{tabular}
}
\end{table}


\newpage
\begin{table}
\centering
\caption{Average performance, considering factors~\RealFactors~with 5 seed on real datasets REFIT and REDD. Metrics reported are \DCI, \RMIG and \RMSE. Lower values are better for all metrics. ($\downarrow$ lower is better, $\uparrow$ higher is worse  {\small \colorbox{gray!25}{Top-1}, \colorbox{gray!10}{Top-2}}). \label{tab:results}}
\resizebox{\textwidth}{!}{
\begin{tabular}{p{.4cm}p{3.5cm}|rrr|rrr|rrr|rrr}\toprule
\bf Sc. & \bf Methods &\multicolumn{3}{c}{$\boldsymbol\sigma=\infty$} &\multicolumn{3}{c}{ $\boldsymbol\sigma=0.3$} & \multicolumn{3}{c}{ $\boldsymbol\sigma=0.8$} \\ \midrule
& \bf Metrics $\Rightarrow$ & \bf \DCI $\downarrow$ & \bf \RMIG$\downarrow$ & \bf \RMSE $\downarrow$ & \bf \DCI $\downarrow$ & \bf \RMIG$\downarrow$ & \bf \RMSE $\downarrow$ & \bf \DCI $\downarrow$ & \bf \RMIG$\downarrow$ & \bf \RMSE $\downarrow$ \\ \midrule
% \multirow{7}{*}{\textbf{\rotatebox{90}{\shortstack{{synthetic}}}}} & \BertNILMcolor Bert4NILM & - & - & 67.96  $\pm$  3.72 & - & - & 84.54  $\pm$  2.11 & - & - & 85.38  $\pm$  2.13 \\
% & \StoScolor S2S & - & - & 65.45  $\pm$  4.27 & - & - & 83.84 $\pm$  4.29 & - & - & 84.56 $\pm$  4.10 \\
% & \Autoformercolor  Autoformer & - & - & 60.00  $\pm$  1.06 & - & - & 61.99  $\pm$  1.79 & - & - & 63.11  $\pm$  1.74 \\
% & \Informercolor Informer & - & - & 65.37  $\pm$  2.14 & - & - & 69.55  $\pm$  2.14 & - & - & 75.51  $\pm$  2.13 \\
% &  \FEDformercolor FEDformer & - & - & 60.91  $\pm$  2.04 & - & - & 67.24  $\pm$  2.21 & - & - & 76.19  $\pm$  2.05 \\
% &  \TimesNetcolor TimesNet & - & - & 61.91  $\pm$  2.91 & - & - & 73.34  $\pm$  2.58 & - & - & 74.69  $\pm$  2.47 \\
% &  \CDSVAEcolor DSVAE & 87.21 $\pm$ 5.52 & 1.25 $\pm$ 0.19 & 56.86  $\pm$  2.55 & 88.85 $\pm$ 4.19 & 1.18 $\pm$ 0.17 & 63.01  $\pm$  2.29 & 88.97 $\pm$ 4.96 & 1.33 $\pm$ 0.19 & 64.31 $\pm$ 2.33 \\
% &  \SVAEcolor SlowVAE & 93.43 $\pm$ 2.16 & 1.13 $\pm$ 0.18 & 57.26  $\pm$  2.69 & 93.87 $\pm$ 2.02 & 1.18 $\pm$ 0.17 & 62.23  $\pm$  2.05 & 96.20 $\pm$ 1.52 & 1.28 $\pm$ 0.19 & 62.85 $\pm$ 2.07 \\
% &  \CoSTcolor CoST & 85.27 $\pm$ 3.47 & 1.16 $\pm$ 0.12 & 57.87  $\pm$  2.19 & 85.41 $\pm$ 3.49 & 1.17 $\pm$ 0.12 & 63.02  $\pm$  2.03 & 86.60 $\pm$ 3.19 & 1.21 $\pm$ 0.13 & 65.52 $\pm$ 1.86 \\
% &  \SVAEHDFcolor SlowVAE+HDF & 96.15 $\pm$ 0.17 & 0.87 $\pm$ 0.07 & 68.96 $\pm$ 2.91 & 96.07 $\pm$ 0.17 & 0.87 $\pm$ 0.07 & 72.00 $\pm$ 2.74 & 96.58 $\pm$ 0.09 & 0.89 $\pm$ 0.05 & 73.09 $\pm$ 2.77 \\
% &  \CDSVAEHDFcolor C-DSVAE + HDF & 88.87 $\pm$ 1.02 & 0.83 $\pm$ 0.06 & 41.29 $\pm$ 2.11 & 88.88 $\pm$ 1.02 & 0.83 $\pm$ 0.06 & 44.87 $\pm$ 1.89 & 89.43 $\pm$ 0.67 & 0.85 $\pm$ 0.05 & 47.37 $\pm$ 2.06 \\
% &  \SparseVAEcolor SparseVAE & 84.47 $\pm$ 3.01 & 0.74 $\pm$ 0.05 & 30.51  $\pm$  1.47 & 84.47 $\pm$ 3.01 & 0.74 $\pm$ 0.05 & 34.13  $\pm$  1.77 & 85.93 $\pm$ 3.04 & 0.76 $\pm$ 0.06 & 35.58 $\pm$ 1.62 \\
% &  \TimeCSLcolor \TimeCSL & 80.88 $\pm$ 2.12 & 0.53 $\pm$ 0.04 & 23.75 $\pm$ 2.02 & 83.29 $\pm$ 2.03 & 0.55 $\pm$ 0.04 & 24.47 $\pm$ 2.14 & 85.01 $\pm$ 2.13 & 0.58 $\pm$ 0.04 & 26.53 $\pm$ 2.18 \\
% %\worstResult
% \cmidrule{1-11}
\multirow{7}{*}{\textbf{\rotatebox{90}{\shortstack{{REFIT}}}}}& \BertNILMcolor BertNILM &- &- &56.4  $\pm$ 2.58 &- &- &70.2  $\pm$  1.45 &- &- &70.92  $\pm$  1.15 \\
& \StoScolor S2S &- &- &54.3  $\pm$ 3.12 &- &- &69.5 $\pm$ 3.56 &- &- &69.95 $\pm$ 3.26 \\
& \Autoformercolor Autoformer &- &- & 49.7  $\pm$  0.81 &- &- & 50.5  $\pm$  2.15 &- &- & 52.95  $\pm$  1.63 \\
& \Informercolor Informer &- &- & 50.3  $\pm$  2.41 &- &- & 53.5  $\pm$  1.98 &- &- & 58.95  $\pm$  1.89 \\
& \FEDformercolor FEDformer &- &- & 50.3  $\pm$  2.12 &- &- & 52.5  $\pm$  2.45 &- &- & 59.01  $\pm$  1.76 \\
&  \TimesNetcolor TimesNet &- &- & 49.24  $\pm$  2.87 &- &- & 51.10  $\pm$  2.64 &- &- & 54.91  $\pm$  2.31 \\
&  \CDSVAEcolor C-DSVAE & 72.42 $\pm$ 3.10 &0.96 $\pm$ .15 &48.6  $\pm$ 2.32 &73.12 $\pm$ 1.43 & 0.95 $\pm$ .15 & 52.9  $\pm$ 2.31 & 74.29 $\pm$ 2.04 &1.08 $\pm$ .09 &52.99 $\pm$ 1.91 \\
&  \SVAEcolor SlowVAE &78.0 $\pm$ 1.09 &0.94 $\pm$ .13 &43.2  $\pm$ 2.23 &78.0 $\pm$ 1.09 &0.94 $\pm$ .13 &49.2  $\pm$ 1.13 &79.74 $\pm$ 0.84 &1.07 $\pm$ .11 &49.65 $\pm$ 1.43 \\
&  \CoSTcolor CoST &68.4 $\pm$ 2.41 &0.97 $\pm$ .03 &47.7  $\pm$ 1.35 &68.4 $\pm$ 2.41 &0.97 $\pm$ .03 &53.2  $\pm$ 1.02 &69.95 $\pm$ 1.63 &1.00 $\pm$ .02 &53.45 $\pm$ 0.82 \\
&  \SVAEHDFcolor SlowVAE+HDF &79.8  $\pm$  .10 &0.64  $\pm$  .05 &57.2  $\pm$  2.15 &79.8  $\pm$  .10 &0.64  $\pm$  .05 &61.3  $\pm$  1.82 &80.37 $\pm$ .05 &0.72 $\pm$ .03 &61.64 $\pm$ 1.52 \\
& \CDSVAEHDFcolor C-DSVAE + HDF &73.1 $\pm$ 1.01 &0.69 $\pm$ .02 &34.4 $\pm$ 1.89 &73.1 $\pm$ 1.01 &0.69 $\pm$ .02 &38.1 $\pm$ 1.34 &74.25 $\pm$ 0.59 &0.73 $\pm$ .05 &38.48 $\pm$ 1.04 \\
&  \SparseVAEcolor SparseVAE &\second 67.2 $\pm$ 2.01 &\second 0.52 $\pm$ .02 &\second 24.3  $\pm$ 1.81 &\second 67.2 $\pm$ 2.01 &\second 0.52 $\pm$ .02 &\second 27.4  $\pm$ 1.13 &\second 71.79 $\pm$ 1.27 &\second 0.58 $\pm$ .04 &\second 27.77 $\pm$ 0.83 \\
&  \TimeCSLcolor \TimeCSL &\first 63.5 $\pm$ 1.35 &\first 0.38 $\pm$ .02 &\first 19.6 $\pm$ 1.95 &\first 69.3 $\pm$ 1.2 &\first 0.44 $\pm$ .02 &\first 20.3 $\pm$ 1.79 &\first 70.12 $\pm$ 0.91 &\first 0.51 $\pm$ .01 &\first 23.63 $\pm$ 1.49 \\ \cmidrule{1-11}
\multirow{7}{*}{\textbf{\rotatebox{90}{\shortstack{{REDD}}}}} & \BertNILMcolor BertNILM & - & - & 61.42  $\pm$  3.47 & - & - & 67.61  $\pm$  1.95 & - & - & 69.06  $\pm$  1.43 \\
& \StoScolor S2S & - & - & 59.08  $\pm$  4.15 & - & - & 68.60 $\pm$  3.91 & - & - & 70.68 $\pm$  3.25 \\
& \Autoformercolor Autoformer & - & - & 49.87  $\pm$  0.92 & - & - & 51.53  $\pm$  1.48 & - & - & 51.88  $\pm$  1.34 \\
& \Informercolor Informer & - & - & 54.23  $\pm$  1.78 & - & - & 57.70  $\pm$  1.78 & - & - & 62.51  $\pm$  1.55 \\
& \FEDformercolor FEDformer & - & - & 52.84  $\pm$  1.69 & - & - & 55.83  $\pm$  1.82 & - & - & 61.92  $\pm$  1.57 \\
&  \TimesNetcolor TimesNet & - & - & 51.37  $\pm$  2.41 & - & - & 55.35  $\pm$  2.23 & - & - & 58.47  $\pm$  2.21 \\
& \CDSVAEcolor C-DSVAE & 72.97 $\pm$ 3.44 & 1.04 $\pm$ 0.16 & 47.17  $\pm$  2.11 & 73.60 $\pm$ 1.82 & 0.98 $\pm$ 0.14 &  52.16  $\pm$  1.89 & 73.96 $\pm$ 2.46 & 1.11 $\pm$ 0.12 & 53.73 $\pm$ 1.79 \\
&  \SVAEcolor SlowVAE & 77.41 $\pm$ 1.67 & 0.94 $\pm$ 0.15 & 46.61  $\pm$  1.91 & 77.80 $\pm$ 1.63 & 0.95 $\pm$ 0.14 & 49.82  $\pm$  1.71 & 79.47 $\pm$ 1.26 & 1.04 $\pm$ 0.13 & 50.88 $\pm$ 1.58 \\
&  \CoSTcolor CoST & 70.75 $\pm$ 2.01 & 0.96 $\pm$ 0.09 & 48.92  $\pm$  1.62 & 70.87 $\pm$ 2.04 & 0.96 $\pm$ 0.09 & 52.73  $\pm$  1.34 & 71.93 $\pm$ 1.84 & 0.98 $\pm$ 0.09 & 54.46 $\pm$ 1.19 \\
&  \SVAEHDFcolor SlowVAE+HDF & 79.97 $\pm$ 0.14 & 0.72 $\pm$ 0.05 & 56.96 $\pm$ 2.34 & 79.77 $\pm$ 0.14 & 0.72 $\pm$ 0.05 & 59.75 $\pm$ 2.21 & 80.22 $\pm$ 0.07 & 0.75 $\pm$ 0.03 & 60.77 $\pm$ 2.22 \\
& \CDSVAEHDFcolor C-DSVAE + HDF & 73.85 $\pm$ 0.85 & 0.69 $\pm$ 0.05 & 34.19 $\pm$ 1.47 & 73.71 $\pm$ 0.85 & 0.69 $\pm$ 0.05 & 37.53 $\pm$ 1.21 & 74.34 $\pm$ 0.56 & 0.71 $\pm$ 0.04 & 39.35 $\pm$ 1.06 \\
& TDRL & 70.86 $\pm$ 0.816 & 0.57 $\pm$ 0.041 & 32.80 $\pm$ 1.41 & 70.75 $\pm$ 0.816 & 0.57 $\pm$ 0.041 & 36.04 $\pm$ 1.16 & 71.94 $\pm$ 0.54 & 0.58 $\pm$ 0.033 & 37.83 $\pm$ 1.02 \\
& \SparseVAEcolor SparseVAE & 70.13 $\pm$ 1.44 & 0.61 $\pm$ 0.04 & 25.46  $\pm$  1.10 & 70.13 $\pm$ 1.44 & 0.61 $\pm$ 0.04 & 28.99  $\pm$  1.22 & 71.44 $\pm$ 1.30 & 0.63 $\pm$ 0.05 & 29.47 $\pm$ 1.10 \\
&   \TimeCSLcolor \TimeCSL & 66.14 $\pm$ 1.66 & 0.40 $\pm$ 0.04 & 19.81 $\pm$ 1.29 & 69.00 $\pm$ 1.41 & 0.44 $\pm$ 0.04 & 20.46 $\pm$ 1.45 & 70.41 $\pm$ 1.22 & 0.48 $\pm$ 0.03 & 22.08 $\pm$ 1.36 \\
%\worstResult
\bottomrule
\end{tabular}}
\vspace{-0.6cm}
\label{tab:results}
\end{table}
\newpage

\begin{table}
\centering
\caption{Performance of different methods on synthetic datasets for various test cases. Metrics reported are \DCI, \RMIG and \RMSE. Lower values are better for all metrics. With factors~\SynFactors~\textbf{Bold} indicates the best performance, while \textbf{second} and \textbf{first} indicate the second and first best performance respectively. \label{tab:results}}
\resizebox{\textwidth}{!}{
\begin{tabular}{p{.4cm}p{2.5cm}|rrr|rrr|rrr|rrr}\toprule
\bf Sc. & \bf Methods &\multicolumn{3}{c}{$\boldsymbol\sigma=\infty$} &\multicolumn{3}{c}{ $\boldsymbol\sigma=0.3$} & \multicolumn{3}{c}{ $\boldsymbol\sigma=0.8$} \\ \midrule
& \bf Metrics $\Rightarrow$ & \bf \DCI $\downarrow$ & \bf \RMIG$\downarrow$ & \bf \RMSE $\downarrow$ & \bf \DCI $\downarrow$ & \bf \RMIG$\downarrow$ & \bf \RMSE $\downarrow$ & \bf \DCI $\downarrow$ & \bf \RMIG$\downarrow$ & \bf \RMSE $\downarrow$ \\ \midrule
%\worstResult
\multirow{7}{*}{\textbf{\rotatebox{90}{\shortstack{{REFIT}}}}} & BertNILM & - & - & 52.81  $\pm$ 25.41 & - & - & 75.78  $\pm$  7.76 & - & - & 66.50  $\pm$  6.69 \\
& S2S & - & - & 47.99  $\pm$ 24.45 & - & - & 63.64 $\pm$ 20.56 & - & - & 67.93 $\pm$ 15.57 \\
& Autoformer & - & - & 61.52  $\pm$  7.66 & - & - & 52.23  $\pm$  11.25 & - & - & 48.45  $\pm$  9.31 \\
& Informer & - & - & 48.59  $\pm$  10.89 & - & - & 59.29  $\pm$  11.36 & - & - & 63.45  $\pm$  10.52 \\
& FEDformer & - & - & 48.29  $\pm$  10.88 & - & - & 58.74  $\pm$  11.23 & - & - & 64.55  $\pm$  9.92 \\
& TimesNet & - & - & 63.57  $\pm$  10.61 & - & - & 67.02  $\pm$  9.10 & - & - & 69.93  $\pm$  9.89 \\
& C-DSVAE & 72.83 $\pm$ 11.71 & 1.08 $\pm$ 0.45 & 40.50  $\pm$  6.45 & 71.76 $\pm$ 9.74 & 1.08 $\pm$ 0.44 & 51.67  $\pm$  7.88 & 72.64 $\pm$ 10.89 & 1.23 $\pm$ 0.51 & 55.26 $\pm$ 7.80 \\
& SlowVAE & 82.31 $\pm$ 11.96 & 1.08 $\pm$ 0.47 & 43.46  $\pm$  7.93 & 81.65 $\pm$ 10.75 & 1.08 $\pm$ 0.46 & 54.81  $\pm$  5.93 & 84.09 $\pm$ 6.93 & 1.27 $\pm$ 0.49 & 53.65 $\pm$ 7.48 \\
& CoST & 79.86 $\pm$ 10.86 & 1.16 $\pm$ 0.23 & 50.14  $\pm$  6.77 & 79.16 $\pm$ 10.49 & 1.15 $\pm$ 0.22 & 55.91  $\pm$  5.72 & 80.16 $\pm$ 9.68 & 1.25 $\pm$ 0.20 & 58.76 $\pm$ 5.51 \\
& SlowVAE+HDF & 88.69 $\pm$ 1.11 & 1.11 $\pm$ 0.24 & 65.87 $\pm$ 8.13 & 85.99 $\pm$ 1.34 & 0.97 $\pm$ 0.21 & 69.94 $\pm$ 7.29 & 89.47 $\pm$ 0.58 & 1.14 $\pm$ 0.24 & 72.21 $\pm$ 7.47 \\
& C-DSVAE + HDF & 76.94 $\pm$ 6.38 & 0.89 $\pm$ 0.37 & 33.61 $\pm$ 5.80 & 75.66 $\pm$ 6.53 & 0.84 $\pm$ 0.33 & 37.92 $\pm$ 5.88 & 74.45 $\pm$ 5.78 & 0.89 $\pm$ 0.40 & 42.58 $\pm$ 6.49 \\
& SparseVAE & 71.35 $\pm$ 8.48 & 0.67 $\pm$ 0.25 & 26.46  $\pm$  5.68 & 72.67 $\pm$ 8.54 & 0.68 $\pm$ 0.27 & 31.07  $\pm$  5.34 & 73.98 $\pm$ 8.23 & 0.74 $\pm$ 0.29 & 32.56 $\pm$ 5.16 \\
& \TimeCSL & 75.44 $\pm$ 6.93 & 0.59 $\pm$ 0.17 & 25.53 $\pm$ 6.69 & 74.50 $\pm$ 6.29 & 0.61 $\pm$ 0.19 & 29.23 $\pm$ 6.57 & 76.66 $\pm$ 5.70 & 0.74 $\pm$ 0.16 & 33.76 $\pm$ 6.73 \\
\cmidrule{1-11}
\multirow{7}{*}{\textbf{\rotatebox{90}{\shortstack{{REFIT}}}}} & BertNILM & - & - & 60.83  $\pm$ 5.80 & - & - & 72.63  $\pm$  2.25 & - & - & 71.02  $\pm$  2.55 \\
& S2S & - & - & 53.73  $\pm$ 5.84 & - & - & 65.57 $\pm$ 5.35 & - & - & 69.21 $\pm$ 4.06 \\
& Autoformer & - & - & 54.60  $\pm$  1.70 & - & - & 50.48  $\pm$  2.82 & - & - & 50.39  $\pm$  2.26 \\
& Informer & - & - & 45.92  $\pm$  3.03 & - & - & 53.77  $\pm$  2.86 & - & - & 61.08  $\pm$  2.51 \\
& FEDformer & - & - & 45.55  $\pm$  3.00 & - & - & 53.62  $\pm$  2.93 & - & - & 60.26  $\pm$  2.51 \\
& TimesNet & - & - & 54.68  $\pm$  3.68 & - & - & 55.28  $\pm$  3.02 & - & - & 59.24  $\pm$  3.41 \\
& C-DSVAE & 74.83 $\pm$ 5.72 & 1.12 $\pm$ 0.23 & 47.04  $\pm$  3.14 & 73.42 $\pm$ 2.40 & 1.10 $\pm$ 0.21 & 53.02  $\pm$  3.49 & 75.29 $\pm$ 3.34 & 1.21 $\pm$ 0.14 & 54.81 $\pm$ 3.46 \\
& SlowVAE & 80.92 $\pm$ 2.73 & 1.10 $\pm$ 0.20 & 44.58  $\pm$  3.11 & 79.95 $\pm$ 2.64 & 1.09 $\pm$ 0.18 & 51.92  $\pm$  2.58 & 81.45 $\pm$ 1.57 & 1.21 $\pm$ 0.14 & 50.69 $\pm$ 2.99 \\
& CoST & 71.18 $\pm$ 3.83 & 1.04 $\pm$ 0.06 & 47.10  $\pm$  1.66 & 71.01 $\pm$ 3.86 & 1.05 $\pm$ 0.05 & 53.58  $\pm$  1.39 & 70.56 $\pm$ 3.50 & 1.14 $\pm$ 0.04 & 55.29 $\pm$ 1.22 \\
& SlowVAE+HDF & 81.13 $\pm$ 0.17 & 0.85 $\pm$ 0.08 & 60.50 $\pm$ 3.01 & 80.21 $\pm$ 0.19 & 0.79 $\pm$ 0.07 & 62.72 $\pm$ 2.77 & 81.68 $\pm$ 0.10 & 0.89 $\pm$ 0.05 & 64.03 $\pm$ 2.99 \\
& C-DSVAE + HDF & 74.77 $\pm$ 1.56 & 0.78 $\pm$ 0.05 & 35.62 $\pm$ 2.52 & 74.39 $\pm$ 1.51 & 0.75 $\pm$ 0.05 & 38.40 $\pm$ 1.83 & 74.88 $\pm$ 0.98 & 0.79 $\pm$ 0.07 & 39.95 $\pm$ 1.62 \\
& SparseVAE & 69.84 $\pm$ 4.10 & 0.62 $\pm$ 0.06 & 27.28  $\pm$  2.59 & 69.95 $\pm$ 4.15 & 0.60 $\pm$ 0.05 & 29.61  $\pm$  1.67 & 72.52 $\pm$ 3.77 & 0.65 $\pm$ 0.07 & 30.35 $\pm$ 1.45 \\
& \TimeCSL & 71.72 $\pm$ 3.23 & 0.46 $\pm$ 0.04 & 25.02 $\pm$ 2.77 & 71.21 $\pm$ 2.58 & 0.51 $\pm$ 0.03 & 25.91 $\pm$ 2.62 & 72.68 $\pm$ 2.33 & 0.61 $\pm$ 0.02 & 28.82 $\pm$ 2.83 \\
%\worstResult
\bottomrule
\end{tabular}}
\vspace{-0.5cm}
\label{table:syntheticResults}
\end{table}







\end{document}
