@article{Hoerl1970,
  publisher = {Taylor \& Francis},
  year = {1970},
  pages = {55--67},
  number = {1},
  volume = {12},
  journal = {Technometrics},
  author = {A. E. Hoerl and R. W. Kennard},
  title = {Ridge regression: Biased estimation for nonorthogonal problems}
}
@inproceedings{lenc2015understanding,
  year = {2015},
  pages = {991--999},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  author = {Lenc, Karel and Vedaldi, Andrea},
  title = {Understanding image representations by measuring their equivariance and equivalence}
}
@article{wiedemer2023provable,
  year = {2023},
  journal = {arXiv preprint arXiv:2310.05327},
  author = {Wiedemer, Thadd{\"a}us and Brady, Jack and Panfilov, Alexander and Juhos, Attila and Bethge, Matthias and Brendel, Wieland},
  title = {Provable Compositional Generalization for Object-Centric Learning}
}
@article{jiang2016variational,
  year = {2016},
  journal = {arXiv preprint arXiv:1611.05148},
  author = {Jiang, Zhuxi and Zheng, Yin and Tan, Huachun and Tang, Bangsheng and Zhou, Hanning},
  title = {Variational deep embedding: An unsupervised and generative approach to clustering}
}
@article{bach2011convex,
  publisher = {Now Publishers, Inc.},
  year = {2012},
  pages = {1--106},
  number = {1},
  volume = {4},
  journal = {Foundations and Trends{\textregistered} in Machine Learning},
  author = {Bach, Francis and Jenatton, Rodolphe and Mairal, Julien and Obozinski, Guillaume and others},
  title = {Optimization with sparsity-inducing penalties}
}
@inproceedings{locatelloObjectCentricLearningSlot2020,
  urldate = {2023-02-28},
  publisher = {{Curran Associates, Inc.}},
  pages = {11525--11538},
  volume = {33},
  year = {2020},
  author = {Locatello, Francesco and Weissenborn, Dirk and Unterthiner, Thomas and Mahendran, Aravindh and Heigold, Georg and Uszkoreit, Jakob and Dosovitskiy, Alexey and Kipf, Thomas},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  title = {Object-{{Centric Learning}} with {{Slot Attention}}}
}
@article{kuhnHungarianMethodAssignment1955,
  langid = {english},
  urldate = {2023-08-21},
  doi = {10.1002/nav.3800020109},
  issn = {00281441, 19319193},
  pages = {83--97},
  number = {1-2},
  volume = {2},
  journal = {Naval Research Logistics Quarterly},
  month = {March},
  year = {1955},
  author = {Kuhn, H. W.},
  title = {The {{Hungarian}} Method for the Assignment Problem}
}
@inproceedings{eastwood2018framework,
  year = {2018},
  booktitle = {International Conference on Learning Representations},
  author = {Eastwood, Cian and Williams, Christopher KI},
  title = {A framework for the quantitative evaluation of disentangled representations}
}
@article{Tibshirani1996,
  publisher = {Wiley Online Library},
  year = {1996},
  pages = {267--288},
  number = {1},
  volume = {58},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  author = {R. Tibshirani},
  title = {Regression shrinkage and selection via the lasso}
}
@article{Lounici_Pontil_Tsybakov2010,
  year = {2011},
  journal = {The Annals of statistics},
  author = {K. Lounici and M. Pontil and A. B. Tsybakov},
  title = {Oracle inequalities and optimal inference under group sparsity}
}
@article{Bickel_Ritov_Tsybakov2009,
  publisher = {Institute of Mathematical Statistics},
  year = {2009},
  pages = {1705--1732},
  number = {4},
  volume = {37},
  journal = {The Annals of statistics},
  author = {P. J. Bickel and Y. Ritov and A. B. Tsybakov},
  title = {Simultaneous analysis of Lasso and {Dantzig} selector}
}
@article{Mairal_Ponce_Sapiro_Zisserman_Bach2008,
  year = {2008},
  volume = {21},
  journal = {Advances in neural information processing systems},
  author = {J. Mairal and J. Ponce and G. Sapiro and A. Zisserman and F. Bach},
  title = {Supervised dictionary learning}
}
@article{Chen1998atomic,
  year = {1998},
  journal = {SIAM Journal on Scientific Computing},
  title = {Atomic Decomposition by Basis Pursuit},
  author = {Chen, S. S. and Donoho, D. L. and Saunders, M. A.}
}
@article{Mairal2011,
  publisher = {IEEE},
  year = {2011},
  pages = {791--804},
  number = {4},
  volume = {34},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  author = {J. Mairal and F. Bach and J. Ponce},
  title = {Task-driven dictionary learning}
}
@inproceedings{sparseComponentAnalysisSurvey,
  year = {2006},
  booktitle = {{ESANN'06 proceedings - 14th European Symposium on Artificial Neural Networks}},
  author = {Gribonval, R. and Lesage, S.},
  title = {A survey of Sparse Component Analysis for blind source separation: principles, perspectives, and new challenges}
}
@article{Richtarik2014,
  publisher = {Springer},
  year = {2014},
  pages = {1--38},
  number = {1},
  volume = {144},
  journal = {Mathematical Programming},
  author = {P. Richt{\'a}rik and M. Tak{\'a}{\v{c}}},
  title = {Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function}
}
@article{Lecun2015,
  publisher = {Nature Publishing Group},
  year = {2015},
  pages = {436--444},
  number = {7553},
  volume = {521},
  journal = {nature},
  author = {Y. LeCun and Y. Bengio and G. Hinton},
  title = {Deep learning}
}
@inproceedings{He2016,
  year = {2016},
  pages = {770--778},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  author = {K. He and X. Zhang and S. Ren and J. Sun},
  title = {Deep residual learning for image recognition}
}
@article{Wright_Nocedal1999,
  year = {1999},
  pages = {7},
  number = {67-68},
  volume = {35},
  journal = {Springer Science},
  author = {S. Wright and J. Nocedal},
  title = {Numerical optimization}
}
@article{kreutz2003dictionary,
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…},
  year = {2003},
  pages = {349--396},
  number = {2},
  volume = {15},
  journal = {Neural computation},
  author = {K. Kreutz-Delgado and J. F. Murray and B. D. Rao and K. Engan and T.-W. Lee and T. J. Sejnowski},
  title = {Dictionary learning algorithms for sparse representation}
}
@inproceedings{Mairal_Bach_Ponce2009,
  year = {2009},
  pages = {689--696},
  booktitle = {Proceedings of the 26th annual international conference on machine learning},
  author = {J. Mairal and F. Bach and J. Ponce and G. Sapiro},
  title = {Online dictionary learning for sparse coding}
}
@article{layernorm,
  year = {2016},
  journal = {arXiv preprint arXiv:1607.06450},
  author = {J. L. Ba and J. R. Kiros and G. E. Hinton},
  title = {Layer normalization}
}
@article{Malezieux2022,
  year = {2022},
  journal = {ICLR},
  author = {B. Mal{\'e}zieux and T. Moreau and M. Kowalski},
  title = {Dictionary and prior learning with unrolled algorithms for unsupervised inverse problems}
}
@article{Kingma_Ba2014,
  year = {2014},
  journal = {arXiv preprint arXiv:1412.6980},
  author = {D. P. Kingma and J. Ba},
  title = {Adam: A method for stochastic optimization}
}
@article{Obozinski2006,
  publisher = {Citeseer},
  year = {2006},
  pages = {2},
  number = {2.2},
  volume = {2},
  journal = {Statistics Department, UC Berkeley, Tech. Rep},
  author = {G. Obozinski and B. Taskar and M. Jordan},
  title = {Multi-task feature selection}
}
@article{Lounici2009,
  year = {2009},
  journal = {arXiv preprint arXiv:0903.1468},
  author = {K. Lounici and M. Pontil and A. B. Tsybakov and S. Van De Geer},
  title = {Taking advantage of sparsity in multi-task learning}
}
@article{Argyriou2008,
  publisher = {Springer},
  year = {2008},
  pages = {243--272},
  number = {3},
  volume = {73},
  journal = {Machine learning},
  author = {A. Argyriou and T. Evgeniou and M. Pontil},
  title = {Convex multi-task feature learning}
}
@article{benefitOfMTRL2016,
  journal = {J. Mach. Learn. Res.},
  year = {2016},
  title = {The Benefit of Multitask Representation Learning},
  author = {Maurer, A. and Pontil, M. and Romera-Paredes, B.}
}
@article{lounici2011oracle,
  year = {2011},
  journal = {The annals of statistics},
  author = {Lounici, K. and Pontil, M. and Van De Geer, S. and Tsybakov, A. B},
  title = {Oracle inequalities and optimal inference under group sparsity}
}
@book{Goodfellow2016,
  publisher = {MIT press},
  year = {2016},
  author = {I. Goodfellow and Y. Bengio and A. Courville},
  title = {Deep learning}
}
@inproceedings{Bertrand2020,
  organization = {PMLR},
  year = {2020},
  pages = {810--821},
  booktitle = {International Conference on Machine Learning},
  author = {Q. Bertrand and Q. Klopfenstein and M. Blondel and S. Vaiter and A. Gramfort and J. Salmon},
  title = {Implicit differentiation of Lasso-type models for hyperparameter optimization}
}
@article{Bolte2022,
  year = {2022},
  journal = {NeurIPS},
  author = {J. Bolte and E. Pauwels and S. Vaiter},
  title = {Automatic differentiation of nonsmooth iterative algorithms}
}
@article{Bolte2021,
  year = {2021},
  pages = {13537--13549},
  volume = {34},
  journal = {Advances in neural information processing systems},
  author = {J. Bolte and T. Le and E. and Pauwels and T. Silveti-Falls},
  title = {Nonsmooth implicit differentiation for machine-learning and optimization}
}
@article{Bertrand2022,
  year = {2022},
  journal = {JMLR},
  author = {Q. Bertrand and Q. Klopfenstein and M. Massias and M. Blondel and S. Vaiter and A. Gramfort and J. Salmon},
  title = {Implicit differentiation for fast hyperparameter selection in non-smooth convex learning}
}
@inproceedings{Franceschi2018,
  organization = {PMLR},
  year = {2018},
  pages = {1568--1577},
  booktitle = {International Conference on Machine Learning},
  author = {L. Franceschi and P. Frasconi and S. Salzo and R. Grazzi and M. Pontil},
  title = {Bilevel programming for hyperparameter optimization and meta-learning}
}
@inproceedings{Pedregosa2016,
  organization = {PMLR},
  year = {2016},
  pages = {737--746},
  booktitle = {International conference on machine learning},
  author = {F. Pedregosa},
  title = {Hyperparameter optimization with approximate gradient}
}
@article{bengio2000,
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…},
  year = {2000},
  pages = {1889--1900},
  number = {8},
  volume = {12},
  journal = {Neural computation},
  author = {Y. Bengio},
  title = {Gradient-based optimization of hyperparameters}
}
@incollection{Bengio+chapter2007,
  year = {2007},
  title = {Scaling Learning Algorithms Towards {AI}},
  publisher = {MIT Press},
  booktitle = {Large Scale Kernel Machines},
  author = {Y. Bengio and Y. LeCun}
}
@article{Hinton06,
  year = {2006},
  volume = {18},
  title = {A Fast Learning Algorithm for Deep Belief Nets},
  pages = {1527--1554},
  journal = {Neural Computation},
  author = {G. E.Hinton and S. Osindero and Y. W. Teh}
}
@book{goodfellow2016deep,
  publisher = {MIT Press},
  year = {2016},
  volume = {1},
  author = {I. Goodfellow and A. Courville and Bengio, Yoshua},
  title = {Deep learning}
}
@article{Darmois1953AnalyseGD,
  year = {1953},
  author = {G. Darmois},
  journal = {Revue de l’Institut International
de Statistique},
  title = {Analyse g{\'e}n{\'e}rale des liaisons stochastiques: etude particuli{\`e}re de l'analyse factorielle lin{\'e}aire}
}
@article{Skitovich1953,
  year = {1953},
  journal = {Izvestiya Akademii Nauk SSSR. Seriya
Matematicheskaya},
  author = {Skitivic, V. P.},
  title = {On a property of the normal distribution.}
}
@article{comon1992,
  year = {1992},
  journal = {Higher-Order Statistics},
  author = {Comon, P.},
  title = {Independent component analysis.}
}
@book{ICAbook,
  publisher = {Wiley},
  title = {Independent Component Analysis},
  year = {2001},
  author = {Hyvärinen, A. and Karhunen, J. and Oja, E.}
}
@inproceedings{AmuseICA90,
  year = {1990},
  title = {AMUSE: a new blind identification algorithm},
  booktitle = {IEEE International Symposium on Circuits and Systems},
  author = {Tong, L. and Soon, V.C. and Huang, Y.F. and Liu, R.}
}
@article{HYVARINEN1999429,
  author = {Hyvärinen, A. and Pajunen, P.},
  year = {1999},
  journal = {Neural Networks},
  title = {Nonlinear independent component analysis: Existence and uniqueness results}
}
@article{Pavan2018OnTD,
  year = {2018},
  journal = {Journal of Communication and Information Systems},
  author = {F. R. M. Pavan and M. D. Miranda},
  title = {On the Darmois-Skitovich Theorem and Spatial Independence in Blind Source Separation}
}
@inproceedings{TCL2016,
  year = {2016},
  title = {Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hyvärinen, A. and Morioka, H.}
}
@inproceedings{PCL17,
  year = {2017},
  booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  author = {Hyvärinen, A. and Morioka, H.},
  title = {{Nonlinear ICA of Temporally Dependent Stationary Sources}}
}
@inproceedings{HyvarinenST19,
  year = {2019},
  title = {Nonlinear ICA Using Auxiliary Variables and Generalized Contrastive Learning},
  publisher = {PMLR},
  booktitle = {AISTATS},
  author = {Hyvärinen, A. and Sasaki, H. and Turner, R. E.}
}
@article{Gutmann12JMLR,
  year = {2012},
  title = {Noise-contrastive estimation of unnormalized statistical models, with
applications to natural image statistics.},
  journal = {The Journal of Machine Learning Research},
  author = {Gutmann, M. U. and Hyvärinen, A.}
}
@inproceedings{iVAEkhemakhem20a,
  year = {2020},
  booktitle = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  author = {Khemakhem, I. and Kingma, D. and Monti, R. and Hyvärinen, A.},
  title = {Variational Autoencoders and Nonlinear ICA: A Unifying Framework}
}
@inproceedings{Sorrenson2020Disentanglement,
  year = {2020},
  booktitle = {International Conference on Learning Representations},
  author = {Sorrenson, P. and Rother, C. and Köthe, U.},
  title = {Disentanglement by Nonlinear ICA with General Incompressible-flow Networks (GIN)}
}
@inproceedings{ice-beem20,
  year = {2020},
  title = {ICE-BeeM: Identifiable Conditional Energy-Based Deep Models Based on Nonlinear ICA},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Khemakhem, I. and Monti, R. and Kingma, D. and Hyvärinen, A.}
}
@article{oord2018representation,
  year = {2018},
  title = {Representation learning with contrastive predictive coding},
  journal = {Advances in Neural Information Processing Systems},
  author = {Oord, A. and Li, Y. and Vinyals, O.}
}
@inproceedings{chen2021exploring,
  year = {2021},
  pages = {15750--15758},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  author = {Chen, Xinlei and He, Kaiming},
  title = {Exploring simple siamese representation learning}
}
@article{grill2020bootstrap,
  year = {2020},
  pages = {21271--21284},
  volume = {33},
  journal = {Advances in neural information processing systems},
  author = {Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and others},
  title = {Bootstrap your own latent-a new approach to self-supervised learning}
}
@inproceedings{chen2020simple,
  year = {2020},
  title = {A Simple Framework for Contrastive Learning of Visual Representations},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  author = {Chen, T. and
Kornblith, S. and
Norouzi, M. and
Hinton, G. E.}
}
@inproceedings{pmlr-v80-kim18b,
  year = {2018},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  author = {Kim, H. and Mnih, A.},
  title = {Disentangling by Factorising}
}
@inproceedings{kumar2018variational,
  year = {2018},
  booktitle = {International Conference on Learning Representations},
  author = {A. Kumar and P. Sattigeri and A. Balakrishnan},
  title = {VARIATIONAL INFERENCE OF DISENTANGLED LATENT CONCEPTS FROM UNLABELED OBSERVATIONS}
}
@inproceedings{roeder2020linear,
  year = {2021},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  author = {Roeder, G. and Metz, L. and Kingma, D. P.},
  title = {On Linear Identifiability of Learned Representations}
}
@conference{GreRubMehLocSch19,
  year = {2019},
  booktitle = {Proceedings of the 35th Conference on Uncertainty in Artificial Intelligence (UAI)},
  author = {Gresele, L. and Rubenstein, P. K. and Mehrjou, A. and Locatello, F. and Sch{\"{o}}lkopf, B.},
  title = {The Incomplete Rosetta Stone problem: Identifiability results for Multi-view Nonlinear {ICA}}
}
@inproceedings{ahuja2023interventional,
  organization = {PMLR},
  year = {2023},
  pages = {372--407},
  booktitle = {International conference on machine learning},
  author = {Ahuja, Kartik and Mahajan, Divyat and Wang, Yixin and Bengio, Yoshua},
  title = {Interventional causal representation learning}
}
@inproceedings{vonkugelgen2021selfsupervised,
  year = {2021},
  booktitle = {Thirty-Fifth Conference on Neural Information Processing Systems},
  author = {Von K{\"u}gelgen, J. and Sharma, Y. and Gresele, L. and Brendel, W. and Sch{\"o}lkopf, B. and Besserve, M. and Locatello, F.},
  title = {Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style}
}
@inproceedings{tcvae,
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2018},
  title = {Isolating Sources of Disentanglement in VAEs},
  author = {Chen, R. T. Q. and Li, X. and G., R. and Duvenaud, D.}
}
@inproceedings{pmlr-v97-locatello19a,
  year = {2019},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  author = {Locatello, F. and Bauer, S. and Lucic, M. and Raetsch, G. and Gelly, S. and Sch{\"o}lkopf, B. and Bachem, O.},
  title = {Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations}
}
@inproceedings{Locatello2020Disentangling,
  url = {https://openreview.net/forum?id=SygagpEKwB},
  year = {2020},
  booktitle = {International Conference on Learning Representations},
  author = {F. Locatello and M. Tschannen and S. Bauer and G. Rätsch and B. Schölkopf and O. Bachem},
  title = {Disentangling Factors of Variations Using Few Labels}
}
@article{Bouchacourt_Tomioka_Nowozin_2018,
  year = {2018},
  author = {Bouchacourt, D. and Tomioka, R. and Nowozin, S.},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title = {Multi-Level Variational Autoencoder: Learning Disentangled Representations From Grouped Observations}
}
@article{TalebJutten1999,
  year = {1999},
  title = {Source separation in post-nonlinear mixtures},
  journal = {IEEE Transactions on Signal Processing},
  author = {Taleb, A. and Jutten, C.}
}
@inproceedings{pmlr-v119-locatello20a,
  year = {2020},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  author = {Locatello, F. and Poole, B. and Raetsch, G. and Sch{\"o}lkopf, B. and Bachem, O. and Tschannen, M.},
  title = {Weakly-Supervised Disentanglement Without Compromises}
}
@inproceedings{zimmermann2021cl,
  year = {2021},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  author = {Zimmermann, R. S. and Sharma, Y. and Schneider, S. and Bethge, M. and Brendel, W.},
  title = {Contrastive Learning Inverts the Data Generating Process}
}
@inproceedings{Yang_2021_CVPR,
  year = {2021},
  booktitle = {Proceedings of the {IEEE/CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  title = {{CausalVAE}: Disentangled Representation Learning via Neural Structural Causal Models},
  author = {Yang, M. and Liu, F. and Chen, Z. and Shen, X. and Hao, J. and Wang, J.}
}
@misc{shen2021disentangled,
  journal = {arXiv preprint arXiv:2010.02637},
  year = {2021},
  author = {Shen, X. and Liu, F. and Dong, H. and Lian, Q. and Chen, Z. and Zhang, T.},
  title = {Disentangled Generative Causal Representation Learning}
}
@inproceedings{kocaoglu2018causalgan,
  year = {2018},
  booktitle = {International Conference on Learning Representations},
  author = {Kocaoglu, M. and Snyder, C. and Dimakis, A. G. and Vishwanath, S.},
  title = {Causal{GAN}: Learning Causal Implicit Generative Models with Adversarial Training}
}
@misc{nair2019causal,
  journal = {arXiv preprint arXiv:1910.01751},
  year = {2019},
  author = {Nair, S. and Zhu, Y. and Savarese, S. and Fei-Fei, L.},
  title = {Causal Induction from Visual Observations for Goal Directed Tasks}
}
@article{peters2015causal,
  year = {2016},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  title = {Causal inference by using invariant prediction: identification and confidence intervals},
  author = {Peters, J. and Bühlmann, P. and Meinshausen, N.}
}
@misc{arjovsky2020invariant,
  journal = {arXiv preprint arXiv:1907.02893},
  year = {2020},
  author = {Arjovsky, M. and Bottou, L. and Gulrajani, I. and Lopez-Paz, D.},
  title = {Invariant Risk Minimization}
}
@inproceedings{IRMgames,
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  year = {2020},
  title = {Invariant Risk Minimization Games},
  author = {Ahuja, K. and Shanmugam, K. and Varshney, K. R. and Dhurandhar, A.}
}
@misc{krueger2021outofdistribution,
  year = {2021},
  author = {D. Krueger and E. Caballero and J.-H. Jacobsen and A. Zhang and J. Binas and R. Le Priol and D. Zhang and A. Courville},
  title = {Out-of-Distribution Generalization via Risk Extrapolation ({\{}RE{\}}x)}
}
@misc{lu2021nonlinear,
  journal = {arXiv preprint arXiv:2102.12353},
  year = {2021},
  author = {Chaochao Lu and Yuhuai Wu and Jośe Miguel Hernández-Lobato and Bernhard Schölkopf},
  title = {Nonlinear Invariant Risk Minimization: A Causal Approach}
}
@inproceedings{ahuja2022towards,
  year = {2022},
  booktitle = {First Conference on Causal Learning and Reasoning},
  author = {K. Ahuja and D. Mahajan and V. Syrgkanis and I. Mitliagkas},
  title = {Towards efficient representation identification in supervised learning}
}
@inproceedings{gulrajani2020search,
  year = {2021},
  booktitle = {International Conference on Learning Representations},
  author = {Gulrajani, I. and Lopez-Paz, D.},
  title = {In Search of Lost Domain Generalization}
}
@inproceedings{slowVAE,
  year = {2021},
  booktitle = {9th International Conference on Learning Representations},
  title = {Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse
Coding},
  author = {Klindt, D. A. and
Schott, L. and
Sharma, Y  and
Ustyuzhaninov, I and
Brendel, W. and
Bethge, M. and
Paiton, D. M.}
}
@inproceedings{Duan2020UDR,
  year = {2020},
  booktitle = {International Conference on Learning Representations},
  author = {Duan, S. and Matthey, L. and Saraiva, A. and Watters, N. and Burgess, C. and Lerchner, A. and Higgins, I.},
  title = {Unsupervised Model Selection for Variational Disentangled Representation Learning}
}
@article{consciousBengio,
  journal = {arXiv preprint arXiv:1709.08568},
  year = {2017},
  title = {The Consciousness Prior},
  author = {Bengio, Y.}
}
@article{Chalupka2017CausalFL,
  year = {2017},
  journal = {Behaviormetrika},
  author = {Chalupka, K. and Eberhardt, F. and Perona, P.},
  title = {Causal feature learning: an overview}
}
@article{ng2019masked,
  year = {2019},
  journal = {arXiv preprint arXiv:1910.08527},
  author = {Ng, I. and Fang, Z. and Zhu, S. and Chen, Z. and Wang, J.},
  title = {Masked Gradient-Based Causal Structure Learning}
}
@inproceedings{Bengio2020A,
  year = {2020},
  booktitle = {International Conference on Learning Representations},
  author = {Bengio, Y. and Deleu, T. and Rahaman, N. and Ke, N. R. and Lachapelle, S. and Bilaniuk, O. and Goyal, A. and Pal, C.},
  title = {A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms}
}
@inproceedings{dcdi,
  year = {2020},
  title = {Differentiable Causal Discovery from Interventional Data},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Brouillard, P. and Lachapelle, S. and Lacoste, A. and Lacoste-Julien, S. and Drouin, A.}
}
@article{ut_igsp,
  year = {2020},
  journal = {Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence},
  author = {Squires, C. and Wang, Y. and Uhler, C.},
  title = {Permutation-Based Causal Structure Learning with Unknown Intervention Targets}
}
@article{JCI_jmlr,
  year = {2020},
  journal = {Journal of Machine Learning Research},
  title = {Joint Causal Inference from Multiple Contexts},
  author = {Mooij, J. M. and Magliacane, S. and Claassen, T.}
}
@inproceedings{eaton2007exact,
  year = {2007},
  booktitle = {Artificial Intelligence and Statistics},
  author = {Eaton, D. and Murphy, K.},
  title = {Exact Bayesian structure learning from uncertain interventions}
}
@incollection{NIPS2019_9581,
  year = {2019},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {Kocaoglu, M. and Jaber, A. and Shanmugam, K. and Bareinboim, E.},
  title = {Characterization and Learning of Causal Graphs with Latent Variables from Soft Interventions}
}
@inproceedings{NEURIPS2020_6cd9313e,
  year = {2020},
  title = {Causal Discovery from Soft Interventions with Unknown Targets: Characterization and Learning},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Jaber, A. and Kocaoglu, M. and Shanmugam, K. and Bareinboim, E.}
}
@book{pearl2009causality,
  publisher = {Cambridge university press},
  year = {2009},
  author = {Pearl, J.},
  title = {Causality}
}
@book{peters2017elements,
  year = {2017},
  publisher = {MIT Press},
  author = {Peters, J. and Janzing, D. and Sch{\"o}lkopf, B.},
  title = {Elements of Causal Inference - Foundations and Learning Algorithms}
}
@article{Pearl2018TheSP,
  year = {2019},
  journal = {Commun. ACM},
  author = {Pearl, J.},
  title = {The Seven Tools of Causal Inference, with Reflections on Machine Learning}
}
@misc{scholkopf2019causality,
  journal = {arXiv preprint arXiv:1911.10500},
  year = {2019},
  author = {Schölkopf, B.},
  title = {Causality for Machine Learning}
}
@article{scholkopf2021causal,
  year = {2021},
  journal = {Proceedings of the IEEE - Advances in Machine Learning and Deep Neural Networks},
  author = {Sch{\"o}lkopf, B. and Locatello, F. and Bauer, S. and Ke, N. R. and Kalchbrenner, N. and Goyal, A. and Bengio, Y.},
  title = {Toward Causal Representation Learning}
}
@article{InducGoyal2021,
  year = {2022},
  journal = {Proc. R. Soc. A 478: 20210068},
  title = {Inductive Biases for Deep Learning of Higher-Level Cognition},
  author = {Goyal, A. and
Bengio, Y.}
}
@misc{CITRIS,
  journal = {arXiv preprint arXiv:2202.03169},
  year = {2022},
  title = {{CITRIS}: Causal Identifiability from Temporal Intervened Sequences},
  author = {Lippe, P. and Magliacane, S. and Löwe, S. and Asano, Y. M. and Cohen, T. and Gavves, E.}
}
@inproceedings{lippe2022icitris,
  year = {2022},
  booktitle = {UAI 2022 Workshop on Causal Representation Learning},
  author = {P. Lippe and S. Magliacane and S. L{\"o}we and Y. M Asano and T. Cohen and E. Gavves},
  title = {i{CITRIS}: Causal Representation Learning for Instantaneous Temporal Effects}
}
@book{pollard_2001,
  year = {2001},
  author = {Pollard, D.},
  publisher = {Cambridge University Press},
  title = {A User's Guide to Measure Theoretic Probability}
}
@inproceedings{Kingma2014,
  year = {2014},
  title = {Auto-Encoding Variational Bayes},
  author = {Kingma, D. P. and Welling, M.},
  booktitle = {2nd International Conference on Learning Representations}
}
@inproceedings{Higgins2017betaVAELB,
  year = {2017},
  booktitle = {ICLR},
  author = {Higgins, I. and Matthey, L. and Pal, A. and Burgess, C. P. and Glorot, X. and Botvinick, M. and Mohamed, S. and Lerchner, A.},
  title = {beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework}
}
@inproceedings{Adam,
  year = {2015},
  booktitle = {3rd International Conference on Learning Representations},
  title = {Adam: {A} Method for Stochastic Optimization},
  author = {Kingma, D. P. and
Ba, J.}
}
@article{jang2016categorical,
  year = {2017},
  journal = {Proceedings of the 34th International Conference on Machine Learning},
  author = {Jang, E. and Gu, S. and Poole, B.},
  title = {Categorical Reparameterization with Gumbel-Softmax}
}
@article{maddison2016concrete,
  year = {2017},
  journal = {Proceedings of the 34th International Conference on Machine Learning},
  author = {Maddison, C. J. and Mnih, A. and Teh, Y. W.},
  title = {The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables}
}
@inproceedings{pmlr-v9-glorot10a,
  year = {2010},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  author = {Glorot, X. and Bengio, Y.},
  title = {Understanding the difficulty of training deep feedforward neural networks}
}
@article{WainwrightJordan08,
  journal = {Found. Trends Mach. Learn.},
  year = {2008},
  title = {Graphical Models, Exponential Families, and Variational Inference},
  author = {Wainwright, M. J. and Jordan, M. I.}
}
@article{ke2021systematic,
  year = {2021},
  author = {Ke, Nan Rosemary and Didolkar, Aniket Rajiv and Mittal, Sarthak and Goyal, Anirudh and Lajoie, Guillaume and Bauer, Stefan and Rezende, Danilo Jimenez and Mozer, Michael Curtis and Bengio, Yoshua and Pal, Christopher},
  title = {Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning}
}
@article{goyal2019recurrent,
  year = {2019},
  journal = {arXiv preprint arXiv:1909.10893},
  author = {A. Goyal and A. Lamb and J. Hoffmann and S. Sodhani and S. Levine and Y. Bengio and B. Sch{\"o}lkopf},
  title = {Recurrent independent mechanisms}
}
@book{Goodfellow-et-al-2016,
  year = {2016},
  publisher = {MIT Press},
  author = {Goodfellow, I and Bengio, Y. and Courville, A.},
  title = {Deep Learning}
}
@inproceedings{batchnorm2015,
  year = {2015},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  author = {Ioffe, S. and Szegedy, C.},
  title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}
}
@article{Bohning1992,
  publisher = {Springer},
  year = {1992},
  pages = {197--200},
  number = {1},
  volume = {44},
  journal = {Annals of the institute of Statistical Mathematics},
  author = {B{\"o}hning, D},
  title = {Multinomial logistic regression algorithm}
}
@inproceedings{Lee_Maji_Ravichandran_Soatto2019meta,
  year = {2019},
  pages = {10657--10665},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  author = {K. Lee and S.Maji and A. Ravichandran and S. Soatto},
  title = {Meta-learning with differentiable convex optimization}
}
@article{Crammer_Singer2001,
  year = {2001},
  pages = {265--292},
  number = {Dec},
  volume = {2},
  journal = {Journal of machine learning research},
  author = {K. Crammer and Y. Singer},
  title = {On the algorithmic implementation of multiclass kernel-based vector machines}
}
@article{Tseng2001,
  publisher = {Springer},
  year = {2001},
  pages = {475--494},
  number = {3},
  volume = {109},
  journal = {Journal of optimization theory and applications},
  author = {P. Tseng},
  title = {Convergence of a block coordinate descent method for nondifferentiable minimization}
}
@inproceedings{Bertrand_Massias2021,
  organization = {PMLR},
  year = {2021},
  pages = {1288--1296},
  booktitle = {International Conference on Artificial Intelligence and Statistics},
  author = {Q. Bertrand and M. Massias},
  title = {Anderson acceleration of coordinate descent}
}
@article{jaxopt,
  year = {2022},
  journal = {NeurIPS},
  author = {M. Blondel and Q. Berthet and M. Cuturi and R. Frostig and S. Hoyer and F. Llinares-L{\'o}pez and F. Pedregosa and J.-P. Vert},
  title = {Efficient and modular implicit differentiation}
}
@inproceedings{Tiomoko_Couillet2019,
  organization = {PMLR},
  year = {2019},
  pages = {6254--6263},
  booktitle = {ICML},
  title = {M. Tiomoko and R. Couillet and F. Bouchard and G. Ginolhac}
}
@inproceedings{lachapelle2022disentanglement,
  year = {2022},
  booktitle = {First Conference on Causal Learning and Reasoning},
  author = {Lachapelle, S. and Rodriguez Lopez, P. and Sharma, Y. and Everett, K. E. and {Le Priol}, R. and Lacoste, A. and Lacoste-Julien, S.},
  title = {Disentanglement via Mechanism Sparsity Regularization: A New Principle for Nonlinear {ICA}}
}
@inproceedings{lachapelle2022partial,
  year = {2022},
  booktitle = {UAI 2022 Workshop on Causal Representation Learning},
  author = {S. Lachapelle and S. Lacoste-Julien},
  title = {Partial Disentanglement via Mechanism Sparsity}
}
@inproceedings{zheng2022on,
  year = {2022},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Y. Zheng and I. Ng and K. Zhang},
  title = {On the Identifiability of Nonlinear {ICA}: Sparsity and Beyond}
}
@article{moran2022identifiable,
  year = {2022},
  journal = {Transactions on Machine Learning Research},
  author = {G. E. Moran and D. Sridhar and Y. Wang and D. Blei},
  title = {Identifiable Deep Generative Models via Sparse Decoding}
}
@misc{ahuja2022sparse,
  year = {2022},
  journal = {arXiv preprint arXiv:2206.01101},
  title = {Weakly Supervised Representation Learning with Sparse Perturbations},
  author = {Ahuja, K. and Hartford, J. and Bengio, Y.}
}
@inproceedings{ahuja2022properties,
  year = {2022},
  booktitle = {International Conference on Learning Representations},
  author = {Ahuja, K. and Hartford, J. and Bengio, Y.},
  title = {Properties from mechanisms: an equivariance perspective on identifiable representation learning}
}
@inproceedings{bertinetto2018r2d2,
  year = {2019},
  journal = {International Conference on Learning Representations},
  author = {L. Bertinetto and J. F. Henriques and P. HS Torr and Vedaldi, Andrea},
  title = {Meta-learning with differentiable closed-form solvers}
}
@article{vinyals2016matchingnet,
  year = {2016},
  volume = {29},
  journal = {Advances in neural information processing systems},
  author = {Vinyals, O. and Blundell, C. and Lillicrap, T. and Wierstra, D. and others},
  title = {Matching networks for one shot learning}
}
@article{bengio2000implicitdiff,
  publisher = {MIT Press},
  year = {2000},
  journal = {Neural computation},
  author = {Y. Bengio},
  title = {Gradient-based optimization of hyperparameters}
}
@inproceedings{lorraine2020million,
  organization = {PMLR},
  year = {2020},
  pages = {1540--1552},
  booktitle = {International Conference on Artificial Intelligence and Statistics},
  author = {J. Lorraine and P. Vicol and D. Duvenaud},
  title = {Optimizing millions of hyperparameters by implicit differentiation}
}
@article{rajeswaran2019imaml,
  year = {2019},
  volume = {32},
  journal = {Advances in Neural Information Processing Systems},
  author = {A. Rajeswaran and C. Finn and S. M. Kakade and S. Levine},
  title = {Meta-learning with implicit gradients}
}
@article{snell2017protonet,
  year = {2017},
  volume = {30},
  journal = {Advances in Neural Information Processing Systems},
  author = {J. Snell and K. Swersky and R. Zemel},
  title = {Prototypical networks for few-shot learning}
}
@article{hospedales2021metalearningsurvey,
  publisher = {IEEE},
  year = {2021},
  pages = {5149--5169},
  number = {9},
  volume = {44},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  author = {Hospedales, T. and Antoniou, A. and Micaelli, P. and Storkey, A.},
  title = {Meta-learning in neural networks: A survey}
}
@article{brown2020gpt3,
  year = {2020},
  pages = {1877--1901},
  volume = {33},
  journal = {Advances in Neural Information Processing Systems},
  author = {Brown, T. and Mann, B. and Ryder, N. and Subbiah, M. and Kaplan, J. D and Dhariwal, P. and Neelakantan, A. and Shyam, P. and Sastry, G. and Askell, A. and others},
  title = {Language models are few-shot learners}
}
@article{dosovitskiy2020vit,
  year = {2021},
  journal = {International Conference on Learning Representations},
  author = {Dosovitskiy, A. and Beyer, L. and Kolesnikov, A. and Weissenborn, D. and Zhai, X. and Unterthiner, T. and Dehghani, M. and Minderer, M. and Heigold, G. and Gelly, S. and others},
  title = {An image is worth 16x16 words: Transformers for image recognition at scale}
}
@inproceedings{chen2020simclr,
  organization = {PMLR},
  year = {2020},
  pages = {1597--1607},
  booktitle = {International Conference on Machine Learning},
  author = {Chen, T. and Kornblith, S. and Norouzi, M. and Hinton, G.},
  title = {A simple framework for contrastive learning of visual representations}
}
@inproceedings{radford2021clip,
  organization = {PMLR},
  year = {2021},
  pages = {8748--8763},
  booktitle = {International Conference on Machine Learning},
  author = {Radford, A. and Kim, J. W. and Hallacy, C. and Ramesh, A. and Goh, G. and Agarwal, S. and Sastry, G. and Askell, A. and Mishkin, P. and Clark, J. and others},
  title = {Learning transferable visual models from natural language supervision}
}
@article{ramesh2022dalle2,
  year = {2022},
  journal = {arXiv preprint arXiv:2204.06125},
  title = {Hierarchical Text-Conditional Image Generation with CLIP Latents},
  author = {Ramesh, A. and Dhariwal, P. and Nichol, A. and Chu, C. and Chen, M.}
}
@book{pearl1988probabilistic,
  isbn = {0934613737},
  address = {San Francisco, CA, USA},
  publisher = {Morgan Kaufmann Publishers Inc.},
  year = {1988},
  author = {Pearl, Judea},
  title = {{Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference}}
}
@book{pearl2009causality,
  edition = {2nd},
  isbn = {052189560X},
  address = {USA},
  publisher = {Cambridge University Press},
  year = {2009},
  author = {Pearl, Judea},
  title = {{Causality: Models, Reasoning and Inference}}
}
@article{peters2015structural,
  number = {3},
  volume = {27},
  journal = {Neural Computation},
  year = {2015},
  author = {Peters, Jonas and B{\"u}hlmann, Peters},
  title = {{Structural Intervention Distance (SID) for Evaluating Causal Graphs}}
}
@article{peters2016causal,
  publisher = {JSTOR},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  year = {2016},
  author = {Peters, Jonas and B{\"u}hlmann, Peter and Meinshausen, Nicolai},
  title = {{Causal inference by using invariant prediction: identification and confidence intervals}}
}
@book{peters2017elements,
  isbn = {0262037319},
  publisher = {The MIT Press},
  year = {2017},
  author = {Peters, Jonas and Janzing, Dominik and Schlkopf, Bernhard},
  title = {{Elements of Causal Inference: Foundations and Learning Algorithms}}
}
@inproceedings{pmlr-v115-tonolini20a,
  publisher = {PMLR},
  month = {22--25 Jul},
  series = {Proceedings of Machine Learning Research},
  volume = {115},
  editor = {Adams, Ryan P. and Gogate, Vibhav},
  year = {2020},
  pages = {690--700},
  booktitle = {Proceedings of The 35th Uncertainty in Artificial Intelligence Conference},
  author = {Tonolini, Francesco and Jensen, Bj{\o}rn Sand and Murray-Smith, Roderick},
  title = {Variational Sparse Coding}
}
@book{Spirtes_2000,
  keywords = {imported ml},
  intrahash = {e2b107e8fd3469c8b0e944ca37a559f3},
  interhash = {559e17fcd12a76214629ba6c4efe3f9a},
  edition = {2nd},
  date-modified = {2008-05-16 16:48:00 -0700},
  date-added = {2008-05-16 16:46:46 -0700},
  added-at = {2016-11-26T13:19:29.000+0100},
  address = {Cambridge MA},
  publisher = {MIT Press},
  year = {2000},
  author = {Spirtes, Peter and Glymour, Clark and Scheines, Richard},
  title = {{Causation, Prediction, and Search}}
}
@article{kivva2022identifiability,
  year = {2022},
  pages = {15687--15701},
  volume = {35},
  journal = {Advances in Neural Information Processing Systems},
  author = {Kivva, Bohdan and Rajendran, Goutham and Ravikumar, Pradeep and Aragam, Bryon},
  title = {Identifiability of deep generative models without auxiliary information}
}
@article{teicher1963identifiability,
  publisher = {JSTOR},
  year = {1963},
  pages = {1265--1269},
  journal = {The annals of Mathematical statistics},
  author = {Teicher, Henry},
  title = {Identifiability of finite mixtures}
}
@article{yakowitz1968identifiability,
  publisher = {Institute of Mathematical Statistics},
  year = {1968},
  pages = {209--214},
  number = {1},
  volume = {39},
  journal = {The Annals of Mathematical Statistics},
  author = {Yakowitz, Sidney J and Spragins, John D},
  title = {On the identifiability of finite mixtures}
}
@book{devlin2012joy,
  publisher = {Springer Science \& Business Media},
  year = {2012},
  author = {Devlin, Keith},
  title = {The joy of sets: fundamentals of contemporary set theory}
}
@article{tarieladze2007disintegration,
  publisher = {Elsevier},
  year = {2007},
  pages = {851--866},
  number = {4-6},
  volume = {23},
  journal = {Journal of Complexity},
  author = {Tarieladze, Vaja and Vakhania, Nicholas},
  title = {Disintegration of Gaussian measures and average-case optimal algorithms}
}
@article{scholkopf2021toward,
  publisher = {IEEE},
  year = {2021},
  pages = {612--634},
  number = {5},
  volume = {109},
  journal = {Proceedings of the IEEE},
  author = {Sch{\"o}lkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  title = {Toward causal representation learning}
}
@inproceedings{lachapelle2023synergies,
  organization = {PMLR},
  year = {2023},
  pages = {18171--18206},
  booktitle = {International Conference on Machine Learning},
  author = {Lachapelle, S{\'e}bastien and Deleu, Tristan and Mahajan, Divyat and Mitliagkas, Ioannis and Bengio, Yoshua and Lacoste-Julien, Simon and Bertrand, Quentin},
  title = {Synergies between disentanglement and sparsity: Generalization and identifiability in multi-task learning}
}
@inproceedings{oublal2024disentangling,
  url = {https://openreview.net/forum?id=iI7hZSczxE},
  year = {2024},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Khalid Oublal and Said Ladjal and David Benhaiem and Emmanuel LE BORGNE and Fran{\c{c}}ois Roueff},
  title = {Disentangling Time Series Representations via Contrastive Independence-of-Support on l-Variational Inference}
}
@article{tschannen2018recent,
  year = {2018},
  journal = {arXiv preprint arXiv:1812.05069},
  author = {Tschannen, Michael and Bachem, Olivier and Lucic, Mario},
  title = {Recent advances in autoencoder-based representation learning}
}
@inproceedings{Hyvrinen2017NonlinearIO,
  year = {2017},
  volume = {54},
  title = {Nonlinear {ICA} of Temporally Dependent Stationary Sources},
  series = {Proceedings of Machine Learning Research},
  pages = {460--469},
  booktitle = {{AISTATS}},
  author = {Aapo Hyv{\"{a}}rinen and
Hiroshi Morioka}
}
@inproceedings{klindt2020towards,
  year = {2021},
  title = {Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse
Coding},
  booktitle = {{ICLR}},
  author = {David A. Klindt and
Lukas Schott and
Yash Sharma and
Ivan Ustyuzhaninov and
Wieland Brendel and
Matthias Bethge and
Dylan M. Paiton}
}
@inproceedings{zimmermann2021contrastive,
  year = {2021},
  volume = {139},
  title = {Contrastive Learning Inverts the Data Generating Process},
  series = {Proceedings of Machine Learning Research},
  pages = {12979--12990},
  booktitle = {{ICML}},
  author = {Roland S. Zimmermann and
Yash Sharma and
Steffen Schneider and
Matthias Bethge and
Wieland Brendel}
}
@inproceedings{halva2020hidden,
  year = {2020},
  volume = {124},
  title = {Hidden Markov Nonlinear {ICA:} Unsupervised Learning from Nonstationary
Time Series},
  series = {Proceedings of Machine Learning Research},
  pages = {939--948},
  booktitle = {Proceedings of the Thirty-Sixth Conference on Uncertainty in Artificial
Intelligence, {UAI} 2020, virtual online, August 3-6, 2020},
  biburl = {https://dblp.org/rec/conf/uai/HalvaH20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  author = {Hermanni H{\"{a}}lv{\"{a}} and
Aapo Hyv{\"{a}}rinen}
}
@inproceedings{hyvarinen2019nonlinear,
  organization = {PMLR},
  year = {2019},
  pages = {859--868},
  booktitle = {The 22nd International Conference on Artificial Intelligence and Statistics},
  author = {Hyvarinen, Aapo and Sasaki, Hiroaki and Turner, Richard},
  title = {Nonlinear ICA using auxiliary variables and generalized contrastive learning}
}
@article{hyvarinen2016unsupervised,
  year = {2016},
  volume = {29},
  journal = {Advances in neural information processing systems},
  author = {Hyvarinen, Aapo and Morioka, Hiroshi},
  title = {Unsupervised feature extraction by time-contrastive learning and nonlinear ica}
}
@inproceedings{locatello2020weakly,
  organization = {PMLR},
  year = {2020},
  pages = {6348--6359},
  booktitle = {International Conference on Machine Learning},
  author = {Locatello, Francesco and Poole, Ben and R{\"a}tsch, Gunnar and Sch{\"o}lkopf, Bernhard and Bachem, Olivier and Tschannen, Michael},
  title = {Weakly-supervised disentanglement without compromises}
}
@article{gresele2021independent,
  year = {2021},
  pages = {28233--28248},
  volume = {34},
  journal = {Advances in neural information processing systems},
  author = {Gresele, Luigi and von K{\"u}gelgen, Julius and Stimper, Vincent and Sch{\"o}lkopf, Bernhard and Besserve, Michel},
  title = {Independent mechanism analysis, a new concept?}
}
@inproceedings{sparseCodingForMTL2013,
  series = {ICML'13},
  year = {2013},
  title = {Sparse Coding for Multitask and Transfer Learning},
  author = {Maurer, A. and Pontil, M. and Romera-Paredes, B.}
}
@article{marcus2022preliminary,
  year = {2022},
  journal = {arXiv preprint arXiv:2204.13807},
  title = {A very preliminary analysis of DALL-E 2},
  author = {Marcus, G. and Davis, E. and Aaronson, S.}
}
@article{devlin2018bert,
  year = {2018},
  journal = {arXiv preprint arXiv:1810.04805},
  author = {Devlin, J. and Chang, M.-W. and Lee, K. and Toutanova, K.},
  title = {Bert: Pre-training of deep bidirectional transformers for language understanding}
}
@article{andreassen2021robustness,
  year = {2021},
  journal = {arXiv preprint arXiv:2106.15831},
  author = {Andreassen, A. and Bahri, Y. and Neyshabur, B. and Roelofs, R.},
  title = {The evolution of out-of-distribution robustness throughout fine-tuning}
}
@inproceedings{wortsman2022wiseft,
  pages = {7959-7971},
  year = {2022},
  month = {June},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title = {Robust Fine-Tuning of Zero-Shot Models},
  author = {Wortsman, M. and Ilharco, G. and Kim, J. W. and Li, M. and Kornblith, S. and Roelofs, R. and Lopes, R. G. and Hajishirzi, H. and Farhadi, A. and Namkoong, H. and Schmidt, L.}
}
@article{miladinovic2019disentangledODE,
  year = {2019},
  journal = {arXiv preprint arXiv:1906.03255},
  title = {Disentangled State Space Representations},
  author = {Miladinović, D. and Gondal, M. W. and Schölkopf, B. and Buhmann, J. M. and Bauer, S.}
}
@inproceedings{steenkiste2019DisForAbstractReasoning,
  year = {2019},
  title = {Are Disentangled Representations Helpful for Abstract Visual Reasoning?},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {van Steenkiste, S. and Locatello, F. and Schmidhuber, J. and Bachem, O.}
}
@inproceedings{dittadi2021sim2real_dis,
  year = {2021},
  booktitle = {International Conference on Learning Representations},
  author = {A. Dittadi and F. Tr{\"a}uble and F. Locatello and M. Wuthrich and V. Agrawal and O. Winther and S. Bauer and B. Sch{\"o}lkopf},
  title = {On the Transfer of Disentangled Representations in Realistic Settings}
}
@inproceedings{montero2021disGen,
  year = {2021},
  booktitle = {International Conference on Learning Representations},
  author = {M. L. Montero and C. JH Ludwig and R. P. Costa and G. Malhotra and J. Bowers},
  title = {The role of Disentanglement in Generalisation}
}
@conference{Zhangetal22b,
  year = {2022},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  author = {Zhang, H. and Zhang, Y.-F. and Liu, W. and Weller, A. and Sch{\"o}lkopf, B. and Xing, E.},
  title = {Towards Principled Disentanglement for Domain Generalization}
}
@book{MohriRostamizadehTalwalkar18,
  year = {2018},
  publisher = {MIT Press},
  author = {Mohri, M. and Rostamizadeh, A. and Talwalkar, A.}
}
@misc{3dshapes18,
  year = {2018},
  howpublished = {https://github.com/deepmind/3dshapes-dataset/},
  author = {Burgess, Chris and Kim, Hyunjik},
  title = {3D Shapes Dataset}
}
@inproceedings{mikolov2010recurrent,
  year = {2010},
  title = {Recurrent neural network based language model},
  publisher = {ISCA},
  author = {Mikolov, T. and Karafiát, M. and Burget, L. and Cernocký, J. and Khudanpur, S.},
  added-at = {2015-04-09T00:00:00.000+0200}
}
@book{CaseBerg2001,
  year = {2001},
  title = {Statistical Inference},
  publisher = {{Duxbury Resource Center}},
  author = {Casella, G. and Berger, R.}
}
@book{Boyd_Vandenberghe2004,
  publisher = {Cambridge university press},
  year = {2004},
  author = {S. P. Boyd and and L. Vandenberghe},
  title = {Convex optimization}
}
@article{Chang_Lin2011,
  publisher = {Acm New York, NY, USA},
  year = {2011},
  pages = {1--27},
  number = {3},
  volume = {2},
  journal = {ACM transactions on intelligent systems and technology (TIST)},
  author = {C.-C. Chang and C.-L. Lin},
  title = {LIBSVM: a library for support vector machines}
}
@article{Chen_Lin_Scholkopf2005,
  publisher = {Wiley Online Library},
  year = {2005},
  pages = {111--136},
  number = {2},
  volume = {21},
  journal = {Applied Stochastic Models in Business and Industry},
  author = {P.-H. Chen and C.-J. Lin and B. Sch{\"o}lkopf},
  title = {A tutorial on $\nu$-support vector machines}
}
@article{Bottou_Lin2007,
  publisher = {MIT press Cambridge, MA},
  year = {2007},
  pages = {301--320},
  number = {1},
  volume = {3},
  journal = {Large scale kernel machines},
  author = {L. Bottou and C.-J. Lin},
  title = {Support vector machine solvers}
}
@inproceedings{Hsieh2008,
  year = {2008},
  pages = {408--415},
  booktitle = {Proceedings of the 25th international conference on Machine learning},
  author = {C.-J. Hsieh and K.-W. Chang and C.-J. Lin and S. S. Keerthi and S. Sundararajan},
  title = {A dual coordinate descent method for large-scale linear SVM}
}
@article{Shalev_Zhang2012,
  year = {2012},
  journal = {The Journal of Machine Learning Research},
  author = {S. Shalev-Shwartz and T. Zhang},
  title = {Stochastic dual coordinate ascent methods for regularized loss minimization}
}
@inproceedings{Finn_Abbel_Levine2017,
  organization = {PMLR},
  year = {2017},
  pages = {1126--1135},
  booktitle = {International conference on machine learning},
  author = {C. Finn and P. Abbeel and S. Levine},
  title = {Model-agnostic meta-learning for fast adaptation of deep networks}
}
@article{bengio2013representation,
  publisher = {IEEE},
  year = {2013},
  pages = {1798--1828},
  number = {8},
  volume = {35},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  title = {Representation learning: A review and new perspectives}
}
@inproceedings{locatello2019challenging,
  organization = {PMLR},
  year = {2019},
  pages = {4114--4124},
  booktitle = {international conference on machine learning},
  author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Raetsch, Gunnar and Gelly, Sylvain and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
  title = {Challenging common assumptions in the unsupervised learning of disentangled representations}
}
@inproceedings{hyvarinen2017nonlinear,
  organization = {PMLR},
  year = {2017},
  pages = {460--469},
  booktitle = {Artificial Intelligence and Statistics},
  author = {Hyvarinen, Aapo and Morioka, Hiroshi},
  title = {Nonlinear ICA of temporally dependent stationary sources}
}
@inproceedings{khemakhem2020variational,
  organization = {PMLR},
  year = {2020},
  pages = {2207--2217},
  booktitle = {International Conference on Artificial Intelligence and Statistics},
  author = {Khemakhem, Ilyes and Kingma, Diederik and Monti, Ricardo and Hyvarinen, Aapo},
  title = {Variational autoencoders and nonlinear ica: A unifying framework}
}
@inproceedings{lippe2022citris,
  organization = {PMLR},
  year = {2022},
  pages = {13557--13603},
  booktitle = {International Conference on Machine Learning},
  author = {Lippe, Phillip and Magliacane, Sara and L{\"o}we, Sindy and Asano, Yuki M and Cohen, Taco and Gavves, Stratis},
  title = {Citris: Causal identifiability from temporal intervened sequences}
}
@article{scholkopf2012causal,
  year = {2012},
  journal = {arXiv preprint arXiv:1206.6471},
  author = {Sch{\"o}lkopf, Bernhard and Janzing, Dominik and Peters, Jonas and Sgouritsa, Eleni and Zhang, Kun and Mooij, Joris},
  title = {On causal and anticausal learning}
}
@article{lippe2023biscuit,
  year = {2023},
  journal = {Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
  author = {Lippe, Phillip and Magliacane, Sara and L{\"o}we, Sindy and Asano, Yuki M and Cohen, Taco and Gavves, Efstratios},
  title = {BISCUIT: Causal Representation Learning from Binary Interactions}
}
@inproceedings{von2023nonparametric,
  year = {2023},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {von K{\"u}gelgen, Julius and Besserve, Michel and Liang, Wendong and Gresele, Luigi and Keki{\'c}, Armin and Bareinboim, Elias and Blei, David M and Sch{\"o}lkopf, Bernhard},
  title = {Nonparametric Identifiability of Causal Representations from Unknown Interventions}
}
@inproceedings{Liang2023cca,
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2023},
  author = {Liang Wendong and Armin Keki\'c and Julius von K\"ugelgen and Simon Buchholz and Michel Besserve and Luigi Gresele and Bernhard Sch\"olkopf},
  title = {Causal Component Analysis}
}
@article{perry2022causal,
  year = {2022},
  pages = {10904--10917},
  volume = {35},
  journal = {Advances in Neural Information Processing Systems},
  author = {Perry, Ronan and Von K{\"u}gelgen, Julius and Sch{\"o}lkopf, Bernhard},
  title = {Causal discovery in heterogeneous environments under the sparse mechanism shift hypothesis}
}
@article{fumero2023leveraging,
  year = {2023},
  journal = {arXiv preprint arXiv:2304.07939},
  author = {Fumero, Marco and Wenzel, Florian and Zancato, Luca and Achille, Alessandro and Rodol{\`a}, Emanuele and Soatto, Stefano and Sch{\"o}lkopf, Bernhard and Locatello, Francesco},
  title = {Leveraging sparse and shared feature activations for disentangled representation learning}
}
@inproceedings{zhang2023identifiability,
  year = {2023},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Jiaqi Zhang and Kristjan Greenewald and Chandler Squires and Akash Srivastava and Karthikeyan Shanmugam and Caroline Uhler},
  title = {Identifiability Guarantees for Causal Disentanglement from Soft Interventions}
}
@inproceedings{ahuja2023multi,
  year = {2023},
  booktitle = {Causal Representation Learning Workshop at NeurIPS 2023},
  author = {Ahuja, Kartik and Mansouri, Amin and Wang, Yixin},
  title = {Multi-Domain Causal Representation Learning via Weak Distributional Invariances}
}
@inproceedings{squires2023linear,
  booktitle = {40th International Conference on Machine Learning},
  year = {2023},
  author = {Chandler Squires and Anna Seigal and Salil Bhate and Caroline Uhler},
  title = {Linear Causal Disentanglement via Interventions}
}
@inproceedings{buchholz2023learning,
  year = {2023},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Buchholz, Simon and Rajendran, Goutham and Rosenfeld, Elan and Aragam, Bryon},
  title = {Learning Linear Causal Representations from Interventions under General Nonlinear Mixing}
}
@inproceedings{ke2023learning,
  organization = {PMLR},
  year = {2023},
  pages = {368--396},
  booktitle = {Conference on Causal Learning and Reasoning},
  author = {Ke, Nan Rosemary and Bengio, Yoshua and Goyal, Anirudh},
  title = {Learning Disentangled Causal Representations via Principal-Mechanism Analysis}
}
@inproceedings{kim_unsupervised_2011,
  pages = {747--758},
  year = {2011},
  month = {April},
  author = {Kim, Hyungsul and Marwah, Manish and Arlitt, Martin and Lyon, Geoff and Han, Jiawei},
  publisher = {Society for Industrial and Applied Mathematics},
  booktitle = {Proceedings of the 2011 {SIAM} {International} {Conference} on {Data} {Mining}},
  urldate = {2023-11-15},
  language = {en},
  doi = {10.1137/1.9781611972818.64},
  url = {https://epubs.siam.org/doi/10.1137/1.9781611972818.64},
  isbn = {978-0-89871-992-5 978-1-61197-281-8},
  title = {Unsupervised {Disaggregation} of {Low} {Frequency} {Power} {Measurements}}
}
@misc{noauthor_unsupervised_nodate,
  note = {ISBN: 9781611972818},
  urldate = {2023-11-14},
  language = {en},
  url = {https://epubs.siam.org/doi/epdf/10.1137/1.9781611972818.64},
  title = {Unsupervised {Disaggregation} of {Low} {Frequency} {Power} {Measurements}}
}
@inproceedings{trauble_disentangled_2021,
  pages = {10401--10412},
  note = {ISSN: 2640-3498},
  year = {2021},
  month = {July},
  author = {Träuble, Frederik and Creager, Elliot and Kilbertus, Niki and Locatello, Francesco and Dittadi, Andrea and Goyal, Anirudh and Schölkopf, Bernhard and Bauer, Stefan},
  publisher = {PMLR},
  booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
  urldate = {2023-11-13},
  language = {en},
  abstract = {The focus of disentanglement approaches has been on identifying independent factors of variation in data. However, the causal variables underlying real-world observations are often not statistically independent. In this work, we bridge the gap to real-world scenarios by analyzing the behavior of the most prominent disentanglement approaches on correlated data in a large-scale empirical study (including 4260 models). We show and quantify that systematically induced correlations in the dataset are being learned and reflected in the latent representations, which has implications for downstream applications of disentanglement such as fairness. We also demonstrate how to resolve these latent correlations, either using weak supervision during training or by post-hoc correcting a pre-trained model with a small number of labels.},
  url = {https://proceedings.mlr.press/v139/trauble21a.html},
  title = {On {Disentangled} {Representations} {Learned} from {Correlated} {Data}}
}
@misc{xia_causal-neural_2022,
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  note = {arXiv:2107.00793 [cs]},
  year = {2022},
  month = {October},
  author = {Xia, Kevin and Lee, Kai-Zhan and Bengio, Yoshua and Bareinboim, Elias},
  publisher = {arXiv},
  urldate = {2023-11-11},
  abstract = {One of the central elements of any causal inference is an object called structural causal model (SCM), which represents a collection of mechanisms and exogenous sources of random variation of the system under investigation (Pearl, 2000). An important property of many kinds of neural networks is universal approximability: the ability to approximate any function to arbitrary precision. Given this property, one may be tempted to surmise that a collection of neural nets is capable of learning any SCM by training on data generated by that SCM. In this paper, we show this is not the case by disentangling the notions of expressivity and learnability. Specifically, we show that the causal hierarchy theorem (Thm. 1, Bareinboim et al., 2020), which describes the limits of what can be learned from data, still holds for neural models. For instance, an arbitrarily complex and expressive neural net is unable to predict the effects of interventions given observational data alone. Given this result, we introduce a special type of SCM called a neural causal model (NCM), and formalize a new type of inductive bias to encode structural constraints necessary for performing causal inferences. Building on this new class of models, we focus on solving two canonical tasks found in the literature known as causal identification and estimation. Leveraging the neural toolbox, we develop an algorithm that is both sufficient and necessary to determine whether a causal effect can be learned from data (i.e., causal identifiability); it then estimates the effect whenever identifiability holds (causal estimation). Simulations corroborate the proposed approach.},
  url = {http://arxiv.org/abs/2107.00793},
  shorttitle = {The {Causal}-{Neural} {Connection}},
  title = {The {Causal}-{Neural} {Connection}: {Expressiveness}, {Learnability}, and {Inference}}
}
@misc{wang_desiderata_2022,
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
  note = {arXiv:2109.03795 [cs, stat]},
  year = {2022},
  month = {February},
  author = {Wang, Yixin and Jordan, Michael I.},
  publisher = {arXiv},
  urldate = {2023-11-10},
  abstract = {Representation learning constructs low-dimensional representations to summarize essential features of high-dimensional data. This learning problem is often approached by describing various desiderata associated with learned representations; e.g., that they be non-spurious, efficient, or disentangled. It can be challenging, however, to turn these intuitive desiderata into formal criteria that can be measured and enhanced based on observed data. In this paper, we take a causal perspective on representation learning, formalizing non-spuriousness and efficiency (in supervised representation learning) and disentanglement (in unsupervised representation learning) using counterfactual quantities and observable consequences of causal assertions. This yields computable metrics that can be used to assess the degree to which representations satisfy the desiderata of interest and learn non-spurious and disentangled representations from single observational datasets.},
  url = {http://arxiv.org/abs/2109.03795},
  shorttitle = {Desiderata for {Representation} {Learning}},
  title = {Desiderata for {Representation} {Learning}: {A} {Causal} {Perspective}}
}
@inproceedings{ahuja_interventional_2023,
  pages = {372--407},
  note = {ISSN: 2640-3498},
  year = {2023},
  month = {July},
  author = {Ahuja, Kartik and Mahajan, Divyat and Wang, Yixin and Bengio, Yoshua},
  publisher = {PMLR},
  booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
  urldate = {2023-11-10},
  language = {en},
  abstract = {Causal representation learning seeks to extract high-level latent factors from low-level sensory data. Most existing methods rely on observational data and structural assumptions (e.g., conditional independence) to identify the latent factors. However, interventional data is prevalent across applications. Can interventional data facilitate causal representation learning? We explore this question in this paper. The key observation is that interventional data often carries geometric signatures of the latent factors’ support (i.e. what values each latent can possibly take). For example, when the latent factors are causally connected, interventions can break the dependency between the intervened latents’ support and their ancestors’. Leveraging this fact, we prove that the latent causal factors can be identified up to permutation and scaling given data from perfect do interventions. Moreover, we can achieve block affine identification, namely the estimated latent factors are only entangled with a few other latents if we have access to data from imperfect interventions. These results highlight the unique power of interventional data in causal representation learning; they can enable provable identification of latent factors without any assumptions about their distributions or dependency structure.},
  url = {https://proceedings.mlr.press/v202/ahuja23a.html},
  title = {Interventional {Causal} {Representation} {Learning}}
}
@misc{chen_isolating_2019,
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
  note = {arXiv:1802.04942 [cs, stat]},
  year = {2019},
  month = {April},
  author = {Chen, Ricky T. Q. and Li, Xuechen and Grosse, Roger and Duvenaud, David},
  publisher = {arXiv},
  urldate = {2023-11-08},
  abstract = {We decompose the evidence lower bound to show the existence of a term measuring the total correlation between latent variables. We use this to motivate our \${\textbackslash}beta\$-TCVAE (Total Correlation Variational Autoencoder), a refinement of the state-of-the-art \${\textbackslash}beta\$-VAE objective for learning disentangled representations, requiring no additional hyperparameters during training. We further propose a principled classifier-free measure of disentanglement called the mutual information gap (MIG). We perform extensive quantitative and qualitative experiments, in both restricted and non-restricted settings, and show a strong relation between total correlation and disentanglement, when the latent variables model is trained using our framework.},
  url = {http://arxiv.org/abs/1802.04942},
  title = {Isolating {Sources} of {Disentanglement} in {Variational} {Autoencoders}}
}
@misc{zimmermann_contrastive_2022,
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
  note = {arXiv:2102.08850 [cs]},
  year = {2022},
  month = {April},
  author = {Zimmermann, Roland S. and Sharma, Yash and Schneider, Steffen and Bethge, Matthias and Brendel, Wieland},
  publisher = {arXiv},
  urldate = {2023-11-08},
  abstract = {Contrastive learning has recently seen tremendous success in self-supervised learning. So far, however, it is largely unclear why the learned representations generalize so effectively to a large variety of downstream tasks. We here prove that feedforward models trained with objectives belonging to the commonly used InfoNCE family learn to implicitly invert the underlying generative model of the observed data. While the proofs make certain statistical assumptions about the generative model, we observe empirically that our findings hold even if these assumptions are severely violated. Our theory highlights a fundamental connection between contrastive learning, generative modeling, and nonlinear independent component analysis, thereby furthering our understanding of the learned representations as well as providing a theoretical foundation to derive more effective contrastive losses.},
  url = {http://arxiv.org/abs/2102.08850},
  title = {Contrastive {Learning} {Inverts} the {Data} {Generating} {Process}}
}
@article{bai_contrastively_nodate,
  author = {Bai, Junwen and Wang, Weiran and Gomes, Carla},
  language = {en},
  abstract = {Self-supervised disentangled representation learning is a critical task in sequence modeling. The learnt representations contribute to better model interpretability as well as the data generation, and improve the sample efﬁciency for downstream tasks. We propose a novel sequence representation learning method, named Contrastively Disentangled Sequential Variational Autoencoder (C-DSVAE), to extract and separate the static (time-invariant) and dynamic (time-variant) factors in the latent space. Different from previous sequential variational autoencoder methods, we use a novel evidence lower bound which maximizes the mutual information between the input and the latent factors, while penalizes the mutual information between the static and dynamic factors. We leverage contrastive estimations of the mutual information terms in training, together with simple yet effective augmentation techniques, to introduce additional inductive biases. Our experiments show that C-DSVAE signiﬁcantly outperforms the previous state-of-the-art methods on multiple metrics.},
  title = {Contrastively {Disentangled} {Sequential} {Variational} {Autoencoder}}
}
@misc{kopiczko_vera_2023,
  keywords = {Computer Science - Computation and Language},
  note = {arXiv:2310.11454 [cs]},
  year = {2023},
  month = {October},
  author = {Kopiczko, Dawid Jan and Blankevoort, Tijmen and Asano, Yuki Markus},
  publisher = {arXiv},
  urldate = {2023-11-08},
  abstract = {Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous per-user or per-task adapted models. In this work, we present Vector-based Random Matrix Adaptation (VeRA), which reduces the number of trainable parameters by 10x compared to LoRA, yet maintains the same performance. It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead. We demonstrate its effectiveness on the GLUE and E2E benchmarks, and show its application in instruction-following with just 1.4M parameters using the Llama2 7B model.},
  url = {http://arxiv.org/abs/2310.11454},
  shorttitle = {{VeRA}},
  title = {{VeRA}: {Vector}-based {Random} {Matrix} {Adaptation}}
}
@misc{camps-valls_discovering_2023,
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability, Statistics - Methodology},
  note = {arXiv:2305.13341 [physics, stat]},
  year = {2023},
  month = {May},
  author = {Camps-Valls, Gustau and Gerhardus, Andreas and Ninad, Urmi and Varando, Gherardo and Martius, Georg and Balaguer-Ballester, Emili and Vinuesa, Ricardo and Diaz, Emiliano and Zanna, Laure and Runge, Jakob},
  publisher = {arXiv},
  urldate = {2023-11-08},
  abstract = {Physics is a field of science that has traditionally used the scientific method to answer questions about why natural phenomena occur and to make testable models that explain the phenomena. Discovering equations, laws and principles that are invariant, robust and causal explanations of the world has been fundamental in physical sciences throughout the centuries. Discoveries emerge from observing the world and, when possible, performing interventional studies in the system under study. With the advent of big data and the use of data-driven methods, causal and equation discovery fields have grown and made progress in computer science, physics, statistics, philosophy, and many applied fields. All these domains are intertwined and can be used to discover causal relations, physical laws, and equations from observational data. This paper reviews the concepts, methods, and relevant works on causal and equation discovery in the broad field of Physics and outlines the most important challenges and promising future lines of research. We also provide a taxonomy for observational causal and equation discovery, point out connections, and showcase a complete set of case studies in Earth and climate sciences, fluid dynamics and mechanics, and the neurosciences. This review demonstrates that discovering fundamental laws and causal relations by observing natural phenomena is being revolutionised with the efficient exploitation of observational data, modern machine learning algorithms and the interaction with domain knowledge. Exciting times are ahead with many challenges and opportunities to improve our understanding of complex systems.},
  url = {http://arxiv.org/abs/2305.13341},
  title = {Discovering {Causal} {Relations} and {Equations} from {Data}}
}
@misc{davidson_hyperspherical_2022,
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  note = {arXiv:1804.00891 [cs, stat]},
  year = {2022},
  month = {September},
  author = {Davidson, Tim R. and Falorsi, Luca and De Cao, Nicola and Kipf, Thomas and Tomczak, Jakub M.},
  publisher = {arXiv},
  urldate = {2023-11-08},
  abstract = {The Variational Auto-Encoder (VAE) is one of the most used unsupervised machine learning models. But although the default choice of a Gaussian distribution for both the prior and posterior represents a mathematically convenient distribution often leading to competitive results, we show that this parameterization fails to model data with a latent hyperspherical structure. To address this issue we propose using a von Mises-Fisher (vMF) distribution instead, leading to a hyperspherical latent space. Through a series of experiments we show how such a hyperspherical VAE, or \${\textbackslash}mathcal\{S\}\$-VAE, is more suitable for capturing data with a hyperspherical latent structure, while outperforming a normal, \${\textbackslash}mathcal\{N\}\$-VAE, in low dimensions on other data types. Code at http://github.com/nicola-decao/s-vae-tf and https://github.com/nicola-decao/s-vae-pytorch},
  url = {http://arxiv.org/abs/1804.00891},
  title = {Hyperspherical {Variational} {Auto}-{Encoders}}
}
@misc{mattei_miwae_2019,
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
  note = {arXiv:1812.02633 [cs, stat]},
  year = {2019},
  month = {February},
  author = {Mattei, Pierre-Alexandre and Frellsen, Jes},
  publisher = {arXiv},
  urldate = {2023-11-08},
  abstract = {We consider the problem of handling missing data with deep latent variable models (DLVMs). First, we present a simple technique to train DLVMs when the training set contains missing-at-random data. Our approach, called MIWAE, is based on the importance-weighted autoencoder (IWAE), and maximises a potentially tight lower bound of the log-likelihood of the observed data. Compared to the original IWAE, our algorithm does not induce any additional computational overhead due to the missing data. We also develop Monte Carlo techniques for single and multiple imputation using a DLVM trained on an incomplete data set. We illustrate our approach by training a convolutional DLVM on a static binarisation of MNIST that contains 50\% of missing pixels. Leveraging multiple imputation, a convolutional network trained on these incomplete digits has a test performance similar to one trained on complete data. On various continuous and binary data sets, we also show that MIWAE provides accurate single imputations, and is highly competitive with state-of-the-art methods.},
  url = {http://arxiv.org/abs/1812.02633},
  shorttitle = {{MIWAE}},
  title = {{MIWAE}: {Deep} {Generative} {Modelling} and {Imputation} of {Incomplete} {Data}}
}
@article{rahimpour_non-intrusive_2017,
  pages = {4430--4441},
  keywords = {Computer Science - Computational Engineering, Finance, and Science},
  note = {arXiv:1704.07308 [cs]},
  year = {2017},
  month = {November},
  author = {Rahimpour, Alireza and Qi, Hairong and Fugate, David and Kuruganti, Teja},
  journal = {IEEE Transactions on Power Systems},
  urldate = {2023-11-08},
  number = {6},
  abstract = {Energy disaggregation or Non-Intrusive Load Monitoring (NILM) addresses the issue of extracting device-level energy consumption information by monitoring the aggregated signal at one single measurement point without installing meters on each individual device. Energy disaggregation can be formulated as a source separation problem where the aggregated signal is expressed as linear combination of basis vectors in a matrix factorization framework. In this paper, an approach based on Sum-to-k constrained Non-negative Matrix Factorization (S2K-NMF) is proposed. By imposing the sum-to-k constraint and the non-negative constraint, S2K-NMF is able to effectively extract perceptually meaningful sources from complex mixtures. The strength of the proposed algorithm is demonstrated through two sets of experiments: Energy disaggregation in a residential smart home, and HVAC components energy monitoring in an industrial building testbed maintained at the Oak Ridge National Laboratory (ORNL). Extensive experimental results demonstrate the superior performance of S2K-NMF as compared to state-of-the-art decomposition-based disaggregation algorithms. The source code and our collected data (HVORUT) for studying NILM for HVAC units can be found at https://bitbucket.org/aicip/nonintrusive-load-monitoring.},
  doi = {10.1109/TPWRS.2017.2660246},
  url = {http://arxiv.org/abs/1704.07308},
  issn = {0885-8950, 1558-0679},
  volume = {32},
  title = {Non-{Intrusive} {Energy} {Disaggregation} {Using} {Non}-negative {Matrix} {Factorization} with {Sum}-to-k {Constraint}}
}
@misc{kingma_auto-encoding_2022,
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  note = {arXiv:1312.6114 [cs, stat]},
  year = {2022},
  month = {December},
  author = {Kingma, Diederik P. and Welling, Max},
  publisher = {arXiv},
  urldate = {2023-11-08},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  url = {http://arxiv.org/abs/1312.6114},
  title = {Auto-{Encoding} {Variational} {Bayes}}
}
@inproceedings{yeche_neighborhood_2021,
  pages = {11964--11974},
  note = {ISSN: 2640-3498},
  year = {2021},
  month = {July},
  author = {Yèche, Hugo and Dresdner, Gideon and Locatello, Francesco and Hüser, Matthias and Rätsch, Gunnar},
  publisher = {PMLR},
  booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
  urldate = {2023-11-08},
  language = {en},
  abstract = {Intensive care units (ICU) are increasingly looking towards machine learning for methods to provide online monitoring of critically ill patients. In machine learning, online monitoring is often formulated as a supervised learning problem. Recently, contrastive learning approaches have demonstrated promising improvements over competitive supervised benchmarks. These methods rely on well-understood data augmentation techniques developed for image data which do not apply to online monitoring. In this work, we overcome this limitation by supplementing time-series data augmentation techniques with a novel contrastive learning objective which we call neighborhood contrastive learning (NCL). Our objective explicitly groups together contiguous time segments from each patient while maintaining state-specific information. Our experiments demonstrate a marked improvement over existing work applying contrastive methods to medical time-series.},
  url = {https://proceedings.mlr.press/v139/yeche21a.html},
  title = {Neighborhood {Contrastive} {Learning} {Applied} to {Online} {Patient} {Monitoring}}
}
@article{cha_orthogonality-enforced_nodate,
  author = {Cha, Jaehoon and Thiyagalingam, Jeyan},
  language = {en},
  abstract = {Noting the importance of factorizing (or disentangling) the latent space, we propose a novel, non-probabilistic disentangling framework for autoencoders, based on the principles of symmetry transformations that are independent of one another. To the best of our knowledge, this is the first deterministic model that is aiming to achieve disentanglement based on autoencoders using only a reconstruction loss without pairs of images or labels, by explicitly introducing inductive biases into a model architecture through Euler encoding. The proposed model is then compared with a number of state-of-the-art models, relevant to disentanglement, including symmetry-based models and generative models. Our evaluation using six different disentanglement metrics, including the unsupervised disentanglement metric we propose here in this paper, shows that the proposed model can offer better disentanglement, especially when variances of the features are different, where other methods may struggle. We believe that this model opens several opportunities for linear disentangled representation learning based on deterministic autoencoders.},
  title = {Orthogonality-{Enforced} {Latent} {Space} in {Autoencoders}: {An} {Approach} to {Learning} {Disentangled} {Representations}}
}
@article{yao2022temporally,
  year = {2022},
  pages = {26492--26503},
  volume = {35},
  journal = {Advances in Neural Information Processing Systems},
  author = {Yao, Weiran and Chen, Guangyi and Zhang, Kun},
  title = {Temporally disentangled representation learning}
}
@inproceedings{yue2022ts2vec,
  year = {2022},
  pages = {8980--8987},
  number = {8},
  volume = {36},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author = {Yue, Zhihan and Wang, Yujing and Duan, Juanyong and Yang, Tianmeng and Huang, Congrui and Tong, Yunhai and Xu, Bixiong},
  title = {Ts2vec: Towards universal representation of time series}
}
@article{ruff2021unifying,
  publisher = {IEEE},
  year = {2021},
  pages = {756--795},
  number = {5},
  volume = {109},
  journal = {Proceedings of the IEEE},
  author = {Ruff, Lukas and Kauffmann, Jacob R and Vandermeulen, Robert A and Montavon, Gr{\'e}goire and Samek, Wojciech and Kloft, Marius and Dietterich, Thomas G and M{\"u}ller, Klaus-Robert},
  title = {A unifying review of deep and shallow anomaly detection}
}
@misc{davidson_hyperspherical_2022-1,
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  note = {arXiv:1804.00891 [cs, stat]},
  year = {2022},
  month = {September},
  author = {Davidson, Tim R. and Falorsi, Luca and De Cao, Nicola and Kipf, Thomas and Tomczak, Jakub M.},
  publisher = {arXiv},
  urldate = {2023-11-07},
  abstract = {The Variational Auto-Encoder (VAE) is one of the most used unsupervised machine learning models. But although the default choice of a Gaussian distribution for both the prior and posterior represents a mathematically convenient distribution often leading to competitive results, we show that this parameterization fails to model data with a latent hyperspherical structure. To address this issue we propose using a von Mises-Fisher (vMF) distribution instead, leading to a hyperspherical latent space. Through a series of experiments we show how such a hyperspherical VAE, or \${\textbackslash}mathcal\{S\}\$-VAE, is more suitable for capturing data with a hyperspherical latent structure, while outperforming a normal, \${\textbackslash}mathcal\{N\}\$-VAE, in low dimensions on other data types. Code at http://github.com/nicola-decao/s-vae-tf and https://github.com/nicola-decao/s-vae-pytorch},
  url = {http://arxiv.org/abs/1804.00891},
  title = {Hyperspherical {Variational} {Auto}-{Encoders}}
}
@misc{domke_importance_2018,
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  note = {arXiv:1808.09034 [cs, stat]},
  year = {2018},
  month = {October},
  author = {Domke, Justin and Sheldon, Daniel},
  publisher = {arXiv},
  urldate = {2023-11-07},
  abstract = {Recent work used importance sampling ideas for better variational bounds on likelihoods. We clarify the applicability of these ideas to pure probabilistic inference, by showing the resulting Importance Weighted Variational Inference (IWVI) technique is an instance of augmented variational inference, thus identifying the looseness in previous work. Experiments confirm IWVI's practicality for probabilistic inference. As a second contribution, we investigate inference with elliptical distributions, which improves accuracy in low dimensions, and convergence in high dimensions.},
  url = {http://arxiv.org/abs/1808.09034},
  title = {Importance {Weighting} and {Variational} {Inference}}
}
@inproceedings{alvarez_melis_towards_2018,
  year = {2018},
  author = {Alvarez Melis, David and Jaakkola, Tommi},
  publisher = {Curran Associates, Inc.},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  urldate = {2023-11-06},
  abstract = {Most recent work on interpretability of complex machine learning models has focused on estimating a-posteriori explanations for previously trained models around specific predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general -- explicitness, faithfulness, and stability -- and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html},
  volume = {31},
  title = {Towards {Robust} {Interpretability} with {Self}-{Explaining} {Neural} {Networks}}
}
@misc{noauthor_carbon_nodate,
  note = {ISBN: 9781922205117},
  urldate = {2023-10-21},
  language = {en},
  url = {https://www.pnas.org/doi/10.1073/pnas.1922205117},
  title = {The carbon footprint of household energy use in the {United} {States}}
}
@article{booth_us_2010,
  year = {2010},
  author = {Booth, Adrian and Greene, Mike and Tai, Humayun},
  language = {en},
  title = {U.{S}. smart grid value at stake: {The} \$130 billion question}
}
@article{noauthor_chapter_nodate,
  language = {en},
  title = {Chapter 5: {Increasing} {Efficiency} of {Building} {Systems} and {Technologies}}
}
@misc{noauthor_neurips_nodate,
  journal = {OpenReview},
  urldate = {2023-10-19},
  language = {en},
  abstract = {Welcome to the OpenReview homepage for NeurIPS 2023 Workshop UniReps Reviewers},
  url = {https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/UniReps/Reviewers},
  title = {{NeurIPS} 2023 {Workshop} {UniReps} {Reviewers}}
}
@misc{wang_desiderata_2022-1,
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
  note = {arXiv:2109.03795 [cs, stat]},
  year = {2022},
  month = {February},
  author = {Wang, Yixin and Jordan, Michael I.},
  publisher = {arXiv},
  urldate = {2023-10-19},
  abstract = {Representation learning constructs low-dimensional representations to summarize essential features of high-dimensional data. This learning problem is often approached by describing various desiderata associated with learned representations; e.g., that they be non-spurious, efficient, or disentangled. It can be challenging, however, to turn these intuitive desiderata into formal criteria that can be measured and enhanced based on observed data. In this paper, we take a causal perspective on representation learning, formalizing non-spuriousness and efficiency (in supervised representation learning) and disentanglement (in unsupervised representation learning) using counterfactual quantities and observable consequences of causal assertions. This yields computable metrics that can be used to assess the degree to which representations satisfy the desiderata of interest and learn non-spurious and disentangled representations from single observational datasets.},
  url = {http://arxiv.org/abs/2109.03795},
  shorttitle = {Desiderata for {Representation} {Learning}},
  title = {Desiderata for {Representation} {Learning}: {A} {Causal} {Perspective}}
}
@misc{chen_simple_2020,
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
  note = {arXiv:2002.05709 [cs, stat]},
  year = {2020},
  month = {June},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  publisher = {arXiv},
  urldate = {2023-10-17},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  url = {http://arxiv.org/abs/2002.05709},
  title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}}
}
@misc{oord_representation_2019,
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  note = {arXiv:1807.03748 [cs, stat]
version: 2},
  year = {2019},
  month = {January},
  author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  publisher = {arXiv},
  urldate = {2023-10-17},
  abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
  url = {http://arxiv.org/abs/1807.03748},
  title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}}
}
@misc{davidson_hyperspherical_2022-2,
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  note = {arXiv:1804.00891 [cs, stat]},
  year = {2022},
  month = {September},
  author = {Davidson, Tim R. and Falorsi, Luca and De Cao, Nicola and Kipf, Thomas and Tomczak, Jakub M.},
  publisher = {arXiv},
  urldate = {2023-10-13},
  abstract = {The Variational Auto-Encoder (VAE) is one of the most used unsupervised machine learning models. But although the default choice of a Gaussian distribution for both the prior and posterior represents a mathematically convenient distribution often leading to competitive results, we show that this parameterization fails to model data with a latent hyperspherical structure. To address this issue we propose using a von Mises-Fisher (vMF) distribution instead, leading to a hyperspherical latent space. Through a series of experiments we show how such a hyperspherical VAE, or \${\textbackslash}mathcal\{S\}\$-VAE, is more suitable for capturing data with a hyperspherical latent structure, while outperforming a normal, \${\textbackslash}mathcal\{N\}\$-VAE, in low dimensions on other data types. Code at http://github.com/nicola-decao/s-vae-tf and https://github.com/nicola-decao/s-vae-pytorch},
  doi = {10.48550/arXiv.1804.00891},
  url = {http://arxiv.org/abs/1804.00891},
  title = {Hyperspherical {Variational} {Auto}-{Encoders}}
}
@misc{jeon_gt-gan_2022,
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  note = {arXiv:2210.02040 [cs]},
  year = {2022},
  month = {October},
  author = {Jeon, Jinsung and Kim, Jeonghak and Song, Haryong and Cho, Seunghyeon and Park, Noseong},
  publisher = {arXiv},
  urldate = {2023-10-13},
  abstract = {Time series synthesis is an important research topic in the field of deep learning, which can be used for data augmentation. Time series data types can be broadly classified into regular or irregular. However, there are no existing generative models that show good performance for both types without any model changes. Therefore, we present a general purpose model capable of synthesizing regular and irregular time series data. To our knowledge, we are the first designing a general purpose time series synthesis model, which is one of the most challenging settings for time series synthesis. To this end, we design a generative adversarial network-based method, where many related techniques are carefully integrated into a single framework, ranging from neural ordinary/controlled differential equations to continuous time-flow processes. Our method outperforms all existing methods.},
  doi = {10.48550/arXiv.2210.02040},
  url = {http://arxiv.org/abs/2210.02040},
  shorttitle = {{GT}-{GAN}},
  title = {{GT}-{GAN}: {General} {Purpose} {Time} {Series} {Synthesis} with {Generative} {Adversarial} {Networks}}
}
@article{noauthor_monthly_2023,
  year = {2023},
  language = {en},
  title = {Monthly {Energy} {Review} - {September} 2023}
}
@article{makonin_rae_2018,
  pages = {8},
  keywords = {Computer Science - Other Computer Science},
  note = {arXiv:1705.05767 [cs]},
  year = {2018},
  month = {February},
  author = {Makonin, Stephen and Wang, Z. Jane and Tumpach, Chris},
  journal = {Data},
  urldate = {2023-10-12},
  number = {1},
  abstract = {Datasets are important for researchers to build models and test how well their machine learning algorithms perform. This paper presents the Rainforest Automation Energy (RAE) dataset to help smart grid researchers test their algorithms which make use of smart meter data. This initial release of RAE contains 1Hz data (mains and sub-meters) from two a residential house. In addition to power data, environmental and sensor data from the house's thermostat is included. Sub-meter data from one of the houses includes heat pump and rental suite captures which is of interest to power utilities. We also show and energy breakdown of each house and show (by example) how RAE can be used to test non-intrusive load monitoring (NILM) algorithms.},
  doi = {10.3390/data3010008},
  url = {http://arxiv.org/abs/1705.05767},
  shorttitle = {{RAE}},
  issn = {2306-5729},
  volume = {3},
  title = {{RAE}: {The} {Rainforest} {Automation} {Energy} {Dataset} for {Smart} {Grid} {Meter} {Data} {Analysis}}
}
@misc{wang_desiderata_2022-2,
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
  note = {arXiv:2109.03795 [cs, stat]},
  year = {2022},
  month = {February},
  author = {Wang, Yixin and Jordan, Michael I.},
  publisher = {arXiv},
  urldate = {2023-10-12},
  abstract = {Representation learning constructs low-dimensional representations to summarize essential features of high-dimensional data. This learning problem is often approached by describing various desiderata associated with learned representations; e.g., that they be non-spurious, efficient, or disentangled. It can be challenging, however, to turn these intuitive desiderata into formal criteria that can be measured and enhanced based on observed data. In this paper, we take a causal perspective on representation learning, formalizing non-spuriousness and efficiency (in supervised representation learning) and disentanglement (in unsupervised representation learning) using counterfactual quantities and observable consequences of causal assertions. This yields computable metrics that can be used to assess the degree to which representations satisfy the desiderata of interest and learn non-spurious and disentangled representations from single observational datasets.},
  url = {http://arxiv.org/abs/2109.03795},
  shorttitle = {Desiderata for {Representation} {Learning}},
  title = {Desiderata for {Representation} {Learning}: {A} {Causal} {Perspective}}
}
@misc{trauble_disentangled_2021-1,
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  note = {arXiv:2006.07886 [cs, stat]},
  year = {2021},
  month = {July},
  author = {Träuble, Frederik and Creager, Elliot and Kilbertus, Niki and Locatello, Francesco and Dittadi, Andrea and Goyal, Anirudh and Schölkopf, Bernhard and Bauer, Stefan},
  publisher = {arXiv},
  urldate = {2023-10-12},
  abstract = {The focus of disentanglement approaches has been on identifying independent factors of variation in data. However, the causal variables underlying real-world observations are often not statistically independent. In this work, we bridge the gap to real-world scenarios by analyzing the behavior of the most prominent disentanglement approaches on correlated data in a large-scale empirical study (including 4260 models). We show and quantify that systematically induced correlations in the dataset are being learned and reflected in the latent representations, which has implications for downstream applications of disentanglement such as fairness. We also demonstrate how to resolve these latent correlations, either using weak supervision during training or by post-hoc correcting a pre-trained model with a small number of labels.},
  url = {http://arxiv.org/abs/2006.07886},
  title = {On {Disentangled} {Representations} {Learned} {From} {Correlated} {Data}}
}
@inproceedings{trauble_disentangled_nodate,
  organization = {PMLR},
  year = {2021},
  pages = {10401--10412},
  booktitle = {International Conference on Machine Learning},
  author = {Tr{\"a}uble, Frederik and Creager, Elliot and Kilbertus, Niki and Locatello, Francesco and Dittadi, Andrea and Goyal, Anirudh and Sch{\"o}lkopf, Bernhard and Bauer, Stefan},
  title = {On disentangled representations learned from correlated data}
}
@article{oublal_temporal_2023,
  year = {2023},
  author = {Oublal, Khalid and Ladjal, Saïd and Roueff, François and Benhaiem, David},
  journal = {ICML 2023},
  language = {en},
  abstract = {Generative models have garnered significant attention for their ability to address the challenge of source separation in disaggregation tasks. Energy Disaggregation holds promise for promoting energy conservation by allowing homeowners to gain comprehensive insights into their energy consumption solely through the interpretation of aggregated load curves. Nevertheless, the model’s ability to generalize and its interpretability remain two major challenges. To tackle these challenges, we deploy a generative model called TAB-VAE (Temporal Attention Bottleneck for Variational Auto-encoder), based on hierarchical architecture, addresses signature variability, and provides a robust, interpretable separation through the design of its informative representation of latent space. Our implementation and evaluation guidelines are available at https://github.com/ oublalkhalid/TAB-VAE.},
  title = {Temporal {Attention} {Bottleneck} is informative?  {Interpretability} through {Disentangled} {Generative} {Representations} for {Time} {Series} {Disaggregation}},
  series = {Workshop}
}
@article{higgins_learning_2017,
  year = {2017},
  author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  language = {en},
  abstract = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artiﬁcial intelligence that is able to learn and reason in the same way that humans do. We introduce β-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modiﬁcation of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter β that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that β-VAE with appropriately tuned β {\textgreater} 1 qualitatively outperforms VAE (β = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also signiﬁcantly outperforms all baselines quantitatively. Unlike InfoGAN, β-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter β, which can be directly optimised through a hyperparameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.},
  title = {{LEARNING} {BASIC} {VISUAL} {CONCEPTS} {WITH} {A} {CONSTRAINED} {VARIATIONAL} {FRAMEWORK}}
}
@misc{le_parisien_italie_2023,
  year = {2023},
  month = {October},
  author = {{Le Parisien}},
  urldate = {2023-10-04},
  abstract = {Deux touristes suisses sont morts carbonisés au volant d’une Ferrari louée à l’occasion d’un rallye de voitures de luxe, en Sardaigne. L’accident s’est produit sur une route à double sens, en Sardaigne,  
alors que le couple tentait un dépassement,  il a heurté cette Lamborghini, qui a ensuite percuté un camping-car. L'accident s'est produit sur une route qui comporte plusieurs tronçons où les dépassements sont interdits.},
  url = {https://www.youtube.com/watch?v=c_R7uZCMCYU},
  shorttitle = {Italie},
  title = {Italie : un conducteur de {Ferrari} provoque un accident mortel lors d’un rallye de voitures de luxe}
}
@inproceedings{chung_recurrent_2015,
  year = {2015},
  author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron C and Bengio, Yoshua},
  publisher = {Curran Associates, Inc.},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  urldate = {2023-09-29},
  abstract = {In this paper, we explore the inclusion of latent random variables into the hidden state of a recurrent neural network (RNN) by combining the elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN) can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against other related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamics.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2015/hash/b618c3210e934362ac261db280128c22-Abstract.html},
  volume = {28},
  title = {A {Recurrent} {Latent} {Variable} {Model} for {Sequential} {Data}}
}
@misc{elfwing_sigmoid-weighted_2017,
  keywords = {Computer Science - Machine Learning},
  note = {arXiv:1702.03118 [cs]},
  year = {2017},
  month = {November},
  author = {Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
  publisher = {arXiv},
  urldate = {2023-09-29},
  abstract = {In recent years, neural networks have enjoyed a renaissance as function approximators in reinforcement learning. Two decades after Tesauro's TD-Gammon achieved near top-level human performance in backgammon, the deep reinforcement learning algorithm DQN achieved human-level performance in many Atari 2600 games. The purpose of this study is twofold. First, we propose two activation functions for neural network function approximation in reinforcement learning: the sigmoid-weighted linear unit (SiLU) and its derivative function (dSiLU). The activation of the SiLU is computed by the sigmoid function multiplied by its input. Second, we suggest that the more traditional approach of using on-policy learning with eligibility traces, instead of experience replay, and softmax action selection with simple annealing can be competitive with DQN, without the need for a separate target network. We validate our proposed approach by, first, achieving new state-of-the-art results in both stochastic SZ-Tetris and Tetris with a small 10\${\textbackslash}times\$10 board, using TD(\${\textbackslash}lambda\$) learning and shallow dSiLU network agents, and, then, by outperforming DQN in the Atari 2600 domain by using a deep Sarsa(\${\textbackslash}lambda\$) agent with SiLU and dSiLU hidden units.},
  doi = {10.48550/arXiv.1702.03118},
  url = {http://arxiv.org/abs/1702.03118},
  title = {Sigmoid-{Weighted} {Linear} {Units} for {Neural} {Network} {Function} {Approximation} in {Reinforcement} {Learning}}
}
@misc{lea_temporal_2016,
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {arXiv:1608.08242 [cs]},
  year = {2016},
  month = {August},
  author = {Lea, Colin and Vidal, Rene and Reiter, Austin and Hager, Gregory D.},
  publisher = {arXiv},
  urldate = {2023-09-29},
  abstract = {The dominant paradigm for video-based action segmentation is composed of two steps: first, for each frame, compute low-level features using Dense Trajectories or a Convolutional Neural Network that encode spatiotemporal information locally, and second, input these features into a classifier that captures high-level temporal relationships, such as a Recurrent Neural Network (RNN). While often effective, this decoupling requires specifying two separate models, each with their own complexities, and prevents capturing more nuanced long-range spatiotemporal relationships. We propose a unified approach, as demonstrated by our Temporal Convolutional Network (TCN), that hierarchically captures relationships at low-, intermediate-, and high-level time-scales. Our model achieves superior or competitive performance using video or sensor data on three public action segmentation datasets and can be trained in a fraction of the time it takes to train an RNN.},
  url = {http://arxiv.org/abs/1608.08242},
  shorttitle = {Temporal {Convolutional} {Networks}},
  title = {Temporal {Convolutional} {Networks}: {A} {Unified} {Approach} to {Action} {Segmentation}}
}
@inproceedings{yue_bert4nilm_2020,
  pages = {89--93},
  keywords = {Deep Learning, Energy Disaggregation, NILM, Neural Network, Non-Intrusive Load Monitoring, Transformer},
  year = {2020},
  month = {November},
  author = {Yue, Zhenrui and Witzig, Camilo Requena and Jorde, Daniel and Jacobsen, Hans-Arno},
  publisher = {Association for Computing Machinery},
  booktitle = {Proceedings of the 5th {International} {Workshop} on {Non}-{Intrusive} {Load} {Monitoring}},
  urldate = {2023-09-29},
  abstract = {Non-intrusive load monitoring (NILM) based energy disaggregation is the decomposition of a system's energy into the consumption of its individual appliances. Previous work on deep learning NILM algorithms has shown great potential in the field of energy management and smart grids. In this paper, we propose BERT4NILM, an architecture based on bidirectional encoder representations from transformers (BERT) and an improved objective function designed specifically for NILM learning. We adapt the bidirectional transformer architecture to the field of energy disaggregation and follow the pattern of sequence-to-sequence learning. With the improved loss function and masked training, BERT4NILM outperforms state-of-the-art models across various metrics on the two publicly available datasets UK-DALE and REDD.},
  doi = {10.1145/3427771.3429390},
  url = {https://dl.acm.org/doi/10.1145/3427771.3429390},
  shorttitle = {{BERT4NILM}},
  isbn = {978-1-4503-8191-8},
  title = {{BERT4NILM}: {A} {Bidirectional} {Transformer} {Model} for {Non}-{Intrusive} {Load} {Monitoring}},
  series = {{NILM}'20},
  address = {New York, NY, USA}
}
@article{kingma_adam_2014,
  year = {2014},
  author = {Kingma, Diederik P and Ba, Jimmy},
  journal = {arXiv preprint arXiv:1412.6980},
  title = {Adam: {A} method for stochastic optimization}
}
@article{yang_sequence_2021,
  year = {2021},
  author = {Yang, Mingzhi and Li, Xinchun and Liu, Yue},
  journal = {Electronics},
  title = {Sequence to {Point} {Learning} {Based} on an {Attention} {Neural} {Network} for {Nonintrusive} {Load} {Decomposition}}
}
@article{chen_convolutional_2018,
  pages = {1860--1864},
  note = {Publisher: Wiley Online Library},
  year = {2018},
  author = {Chen, Kunjin and Wang, Qin and He, Ziyu and Chen, Kunlong and Hu, Jun and He, Jinliang},
  journal = {the Journal of Engineering},
  number = {17},
  volume = {2018},
  title = {Convolutional sequence to sequence non-intrusive load monitoring}
}
@article{ciancetta_new_2021,
  year = {2021},
  author = {Ciancetta, Fabrizio and Bucci, Giovanni and Fiorucci, Edoardo and Mari, Simone and Fioravanti, Andrea},
  journal = {IEEE Transactions on Instrumentation and Measurement},
  doi = {10.1109/TIM.2020.3035193},
  title = {A {New} {Convolutional} {Neural} {Network}-{Based} {System} for {NILM} {Applications}}
}
@inproceedings{vahdat_nvae_2020,
  year = {2020},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  author = {Vahdat, Arash and Kautz, Jan},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  title = {{NVAE}: {A} {Deep} {Hierarchical} {Variational} {Autoencoder}}
}
@inproceedings{chen_isolating_2018,
  year = {2018},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  author = {Chen, Ricky T. Q. and Li, Xuechen and Grosse, Roger B and Duvenaud, David K},
  publisher = {Curran Associates, Inc.},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  volume = {31},
  title = {Isolating {Sources} of {Disentanglement} in {Variational} {Autoencoders}}
}
@article{kelly_uk-dale_2015,
  note = {Publisher: Nature Publishing Group},
  year = {2015},
  author = {Kelly, Jack and Knottenbelt, William},
  journal = {Scientific data},
  volume = {2},
  title = {The {UK}-{DALE} dataset, domestic appliance-level electricity demand and whole-house demand from five {UK} homes}
}
@inproceedings{valenti_exploiting_2018,
  year = {2018},
  author = {Valenti, Michele and Bonfigli, Roberto and Principi, Emanuele and Squartini, Stefano},
  publisher = {IEEE},
  booktitle = {2018 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
  title = {Exploiting the reactive power in deep neural models for non-intrusive load monitoring}
}
@misc{zimmermann_contrastive_2022-1,
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
  note = {arXiv:2102.08850 [cs]},
  year = {2022},
  month = {April},
  author = {Zimmermann, Roland S. and Sharma, Yash and Schneider, Steffen and Bethge, Matthias and Brendel, Wieland},
  publisher = {arXiv},
  urldate = {2023-09-29},
  abstract = {Contrastive learning has recently seen tremendous success in self-supervised learning. So far, however, it is largely unclear why the learned representations generalize so effectively to a large variety of downstream tasks. We here prove that feedforward models trained with objectives belonging to the commonly used InfoNCE family learn to implicitly invert the underlying generative model of the observed data. While the proofs make certain statistical assumptions about the generative model, we observe empirically that our findings hold even if these assumptions are severely violated. Our theory highlights a fundamental connection between contrastive learning, generative modeling, and nonlinear independent component analysis, thereby furthering our understanding of the learned representations as well as providing a theoretical foundation to derive more effective contrastive losses.},
  doi = {10.48550/arXiv.2102.08850},
  url = {http://arxiv.org/abs/2102.08850},
  title = {Contrastive {Learning} {Inverts} the {Data} {Generating} {Process}}
}
@article{zimmermann_contrastive_nodate,
  author = {Zimmermann, Roland S and Sharma, Yash and Schneider, Steffen and Bethge, Matthias and Brendel, Wieland},
  language = {en},
  title = {Contrastive {Learning} {Inverts} the {Data} {Generating} {Process}}
}
@misc{bengio_representation_2014,
  keywords = {Computer Science - Machine Learning},
  note = {arXiv:1206.5538 [cs]},
  year = {2014},
  month = {April},
  author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  publisher = {arXiv},
  urldate = {2023-09-28},
  abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
  doi = {10.48550/arXiv.1206.5538},
  url = {http://arxiv.org/abs/1206.5538},
  shorttitle = {Representation {Learning}},
  title = {Representation {Learning}: {A} {Review} and {New} {Perspectives}}
}
@misc{wang_correlated_2023,
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  note = {arXiv:2306.06994 [cs]},
  year = {2023},
  month = {June},
  author = {Wang, Luxuan and Bai, Lei and Li, Ziyue and Zhao, Rui and Tsung, Fugee},
  publisher = {arXiv},
  urldate = {2023-09-28},
  abstract = {Correlated time series analysis plays an important role in many real-world industries. Learning an efficient representation of this large-scale data for further downstream tasks is necessary but challenging. In this paper, we propose a time-step-level representation learning framework for individual instances via bootstrapped spatiotemporal representation prediction. We evaluated the effectiveness and flexibility of our representation learning framework on correlated time series forecasting and cold-start transferring the forecasting model to new instances with limited data. A linear regression model trained on top of the learned representations demonstrates our model performs best in most cases. Especially compared to representation learning models, we reduce the RMSE, MAE, and MAPE by 37\%, 49\%, and 48\% on the PeMS-BAY dataset, respectively. Furthermore, in real-world metro passenger flow data, our framework demonstrates the ability to transfer to infer future information of new cold-start instances, with gains of 15\%, 19\%, and 18\%. The source code will be released under the GitHub https://github.com/bonaldli/Spatiotemporal-TS-Representation-Learning},
  url = {http://arxiv.org/abs/2306.06994},
  title = {Correlated {Time} {Series} {Self}-{Supervised} {Representation} {Learning} via {Spatiotemporal} {Bootstrapping}}
}
@inproceedings{yang_causalvae_2021,
  pages = {9588--9597},
  year = {2021},
  month = {June},
  author = {Yang, Mengyue and Liu, Furui and Chen, Zhitang and Shen, Xinwei and Hao, Jianye and Wang, Jun},
  publisher = {IEEE},
  booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  urldate = {2023-09-28},
  language = {en},
  abstract = {Learning disentanglement aims at ﬁnding a low dimensional representation which consists of multiple explanatory and generative factors of the observational data. The framework of variational autoencoder (VAE) is commonly used to disentangle independent factors from observations. However, in real scenarios, factors with semantics are not necessarily independent. Instead, there might be an underlying causal structure which renders these factors dependent. We thus propose a new VAE based framework named CausalVAE, which includes a Causal Layer to transform independent exogenous factors into causal endogenous ones that correspond to causally related concepts in data. We further analyze the model identiﬁabitily, showing that the proposed model learned from observations recovers the true one up to a certain degree. Experiments are conducted on various datasets, including synthetic and real word benchmark CelebA. Results show that the causal representations learned by CausalVAE are semantically interpretable, and their causal relationship as a Directed Acyclic Graph (DAG) is identiﬁed with good accuracy. Furthermore, we demonstrate that the proposed CausalVAE model is able to generate counterfactual data through “do-operation” to the causal factors.},
  doi = {10.1109/CVPR46437.2021.00947},
  url = {https://ieeexplore.ieee.org/document/9578520/},
  shorttitle = {{CausalVAE}},
  isbn = {978-1-66544-509-2},
  title = {{CausalVAE}: {Disentangled} {Representation} {Learning} via {Neural} {Structural} {Causal} {Models}},
  address = {Nashville, TN, USA}
}
@misc{scholkopf_towards_2021,
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  note = {arXiv:2102.11107 [cs]},
  year = {2021},
  month = {February},
  author = {Schölkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  publisher = {arXiv},
  urldate = {2023-09-28},
  abstract = {The two fields of machine learning and graphical causality arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.},
  url = {http://arxiv.org/abs/2102.11107},
  title = {Towards {Causal} {Representation} {Learning}}
}
@article{shanmugam_elements_2018,
  pages = {3248--3248},
  year = {2018},
  month = {November},
  author = {Shanmugam, Ramalingam},
  journal = {Journal of Statistical Computation and Simulation},
  urldate = {2023-09-27},
  number = {16},
  language = {en},
  doi = {10.1080/00949655.2018.1505197},
  url = {https://www.tandfonline.com/doi/full/10.1080/00949655.2018.1505197},
  shorttitle = {Elements of causal inference},
  issn = {0094-9655, 1563-5163},
  volume = {88},
  title = {Elements of causal inference: foundations and learning algorithms}
}
@article{nalmpantis_time_2020,
  year = {2020},
  author = {Nalmpantis, Christoforos and Vrakas, Dimitris},
  journal = {Neural Computing and Applications},
  urldate = {2023-09-27},
  number = {23},
  language = {en},
  abstract = {Given only the main power consumption of a household, a non-intrusive load monitoring (NILM) system identifies which appliances are operating. With the rise of Internet of things, running energy disaggregation models on the edge is more and more essential for privacy concerns and economic reasons. However, current NILM solutions use data-hungry deep learning models that can recognize only one device and are impossible to run on a device with limited resources. This research investigates in-depth multi-label NILM systems and suggests a novel framework which enables a cost-effective solution. It can be deployed on an embedded device, and thus, privacy can be preserved. The proposed system leverages dimensionality reduction using Signal2Vec, is evaluated on two popular public datasets and outperforms another state-of-the-art multi-label NILM system.},
  doi = {10.1007/s00521-020-04916-5},
  url = {https://link.springer.com/epdf/10.1007/s00521-020-04916-5},
  issn = {0941-0643},
  volume = {32},
  title = {On time series representations for multi-label {NILM}}
}
@article{yang_semisupervised_2020,
  pages = {6892--6902},
  year = {2020},
  month = {November},
  author = {Yang, Yandong and Zhong, Jing and Li, Wei and Gulliver, T. Aaron and Li, Shufang},
  journal = {IEEE Transactions on Industrial Informatics},
  urldate = {2023-09-27},
  number = {11},
  language = {en},
  abstract = {Nonintrusive load monitoring (NILM) is a technique that infers appliance-level energy consumption patterns and operation state changes based on feeder power signals. With the availability of ﬁne-grained electric load proﬁles, there has been increasing interest in using this approach for demand-side energy management in smart grids. NILM is a multilabel classiﬁcation problem due to the simultaneous operation of multiple appliances. Recently, deep learning based techniques have been shown to be a promising approach to solving this problem, but annotating the huge volume of load proﬁle data with multiple active appliances for learning is very challenging and impractical. In this article, a new semisupervised multilabel deep learning based framework is proposed to address this problem with the goal of mitigating the reliance on large labeled datasets. Speciﬁcally, a temporal convolutional neural network is used to automatically extract high-level load signatures for individual appliances. These signatures can be efﬁciently used to improve the feature representation capability of the framework. Case studies conducted on two open-access NILM datasets demonstrate the effectiveness and superiority of the proposed approach.},
  doi = {10.1109/TII.2019.2955470},
  url = {https://ieeexplore.ieee.org/document/8911216/},
  issn = {1551-3203, 1941-0050},
  volume = {16},
  title = {Semisupervised {Multilabel} {Deep} {Learning} {Based} {Nonintrusive} {Load} {Monitoring} in {Smart} {Grids}}
}
@misc{noauthor_zimbra_nodate,
  urldate = {2023-09-27},
  url = {https://z.imt.fr/zimbra/mail#1},
  title = {Zimbra: {Réception}}
}
@inproceedings{bai_contrastively_2021,
  year = {2021},
  month = {November},
  author = {Bai, Junwen and Wang, Weiran and Gomes, Carla P.},
  urldate = {2023-09-27},
  language = {en},
  abstract = {Self-supervised disentangled representation learning is a critical task in sequence modeling. The learnt representations contribute to better model interpretability as well as the data generation, and improve the sample efficiency for downstream tasks. We propose a novel sequence representation learning method, named Contrastively Disentangled Sequential Variational Autoencoder (C-DSVAE), to extract and separate the static (time-invariant) and dynamic (time-variant) factors in the latent space. Different from previous sequential variational autoencoder methods, we use a novel evidence lower bound which maximizes the mutual information between the input and the latent factors, while penalizes the mutual information between the static and dynamic factors. We leverage contrastive estimations of the mutual information terms in training, together with simple yet effective augmentation techniques, to introduce additional inductive biases. Our experiments show that C-DSVAE significantly outperforms the previous state-of-the-art methods on multiple metrics.},
  url = {https://openreview.net/forum?id=rWPxhfz2_S},
  title = {Contrastively {Disentangled} {Sequential} {Variational} {Autoencoder}}
}
@misc{carbonneau_measuring_2022,
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  note = {arXiv:2012.09276 [cs]},
  year = {2022},
  month = {May},
  author = {Carbonneau, Marc-André and Zaidi, Julian and Boilard, Jonathan and Gagnon, Ghyslain},
  publisher = {arXiv},
  urldate = {2023-09-26},
  abstract = {Learning to disentangle and represent factors of variation in data is an important problem in AI. While many advances have been made to learn these representations, it is still unclear how to quantify disentanglement. While several metrics exist, little is known on their implicit assumptions, what they truly measure, and their limits. In consequence, it is difficult to interpret results when comparing different representations. In this work, we survey supervised disentanglement metrics and thoroughly analyze them. We propose a new taxonomy in which all metrics fall into one of three families: intervention-based, predictor-based and information-based. We conduct extensive experiments in which we isolate properties of disentangled representations, allowing stratified comparison along several axes. From our experiment results and analysis, we provide insights on relations between disentangled representation properties. Finally, we share guidelines on how to measure disentanglement.},
  url = {http://arxiv.org/abs/2012.09276},
  shorttitle = {Measuring {Disentanglement}},
  title = {Measuring {Disentanglement}: {A} {Review} of {Metrics}}
}
@inproceedings{vaswani_attention_2017,
  year = {2017},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  publisher = {Curran Associates, Inc.},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  urldate = {2023-09-26},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  volume = {30},
  title = {Attention is {All} you {Need}}
}
@inproceedings{apostolopoulou_deep_2022,
  year = {2021},
  booktitle = {International Conference on Learning Representations},
  author = {Apostolopoulou, Ifigeneia and Char, Ian and Rosenfeld, Elan and Dubrawski, Artur},
  title = {Deep Attentive Variational Inference}
}
@misc{franceschi_unsupervised_2020,
  keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
  note = {arXiv:1901.10738 [cs, stat]},
  year = {2020},
  month = {January},
  author = {Franceschi, Jean-Yves and Dieuleveut, Aymeric and Jaggi, Martin},
  publisher = {arXiv},
  urldate = {2023-09-26},
  abstract = {Time series constitute a challenging data type for machine learning algorithms, due to their highly variable lengths and sparse labeling in practice. In this paper, we tackle this challenge by proposing an unsupervised method to learn universal embeddings of time series. Unlike previous works, it is scalable with respect to their length and we demonstrate the quality, transferability and practicability of the learned representations with thorough experiments and comparisons. To this end, we combine an encoder based on causal dilated convolutions with a novel triplet loss employing time-based negative sampling, obtaining general-purpose representations for variable length and multivariate time series.},
  url = {http://arxiv.org/abs/1901.10738},
  title = {Unsupervised {Scalable} {Representation} {Learning} for {Multivariate} {Time} {Series}}
}
@misc{paul_psa-gan_2022,
  keywords = {Computer Science - Machine Learning},
  note = {arXiv:2108.00981 [cs]},
  year = {2022},
  month = {March},
  author = {Paul, Jeha and Michael, Bohlke-Schneider and Pedro, Mercado and Shubham, Kapoor and Rajbir, Singh Nirwan and Valentin, Flunkert and Jan, Gasthaus and Tim, Januschowski},
  publisher = {arXiv},
  urldate = {2023-09-26},
  abstract = {Realistic synthetic time series data of sufficient length enables practical applications in time series modeling tasks, such as forecasting, but remains a challenge. In this paper we present PSA-GAN, a generative adversarial network (GAN) that generates long time series samples of high quality using progressive growing of GANs and self-attention. We show that PSA-GAN can be used to reduce the error in two downstream forecasting tasks over baselines that only use real data. We also introduce a Frechet-Inception Distance-like score, Context-FID, assessing the quality of synthetic time series samples. In our downstream tasks, we find that the lowest scoring models correspond to the best-performing ones. Therefore, Context-FID could be a useful tool to develop time series GAN models.},
  url = {http://arxiv.org/abs/2108.00981},
  shorttitle = {{PSA}-{GAN}},
  title = {{PSA}-{GAN}: {Progressive} {Self} {Attention} {GANs} for {Synthetic} {Time} {Series}}
}
@misc{zimmermann_contrastive_2022-2,
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
  note = {arXiv:2102.08850 [cs]},
  year = {2022},
  month = {April},
  author = {Zimmermann, Roland S. and Sharma, Yash and Schneider, Steffen and Bethge, Matthias and Brendel, Wieland},
  publisher = {arXiv},
  urldate = {2023-09-25},
  abstract = {Contrastive learning has recently seen tremendous success in self-supervised learning. So far, however, it is largely unclear why the learned representations generalize so effectively to a large variety of downstream tasks. We here prove that feedforward models trained with objectives belonging to the commonly used InfoNCE family learn to implicitly invert the underlying generative model of the observed data. While the proofs make certain statistical assumptions about the generative model, we observe empirically that our findings hold even if these assumptions are severely violated. Our theory highlights a fundamental connection between contrastive learning, generative modeling, and nonlinear independent component analysis, thereby furthering our understanding of the learned representations as well as providing a theoretical foundation to derive more effective contrastive losses.},
  url = {http://arxiv.org/abs/2102.08850},
  title = {Contrastive {Learning} {Inverts} the {Data} {Generating} {Process}}
}
@misc{li_generative_2023,
  keywords = {Computer Science - Machine Learning},
  note = {arXiv:2301.03028 [cs]},
  year = {2023},
  month = {January},
  author = {Li, Yan and Lu, Xinjiang and Wang, Yaqing and Dou, Dejing},
  publisher = {arXiv},
  urldate = {2023-09-25},
  abstract = {Time series forecasting has been a widely explored task of great importance in many applications. However, it is common that real-world time series data are recorded in a short time period, which results in a big gap between the deep model and the limited and noisy time series. In this work, we propose to address the time series forecasting problem with generative modeling and propose a bidirectional variational auto-encoder (BVAE) equipped with diffusion, denoise, and disentanglement, namely D3VAE. Specifically, a coupled diffusion probabilistic model is proposed to augment the time series data without increasing the aleatoric uncertainty and implement a more tractable inference process with BVAE. To ensure the generated series move toward the true target, we further propose to adapt and integrate the multiscale denoising score matching into the diffusion process for time series forecasting. In addition, to enhance the interpretability and stability of the prediction, we treat the latent variable in a multivariate manner and disentangle them on top of minimizing total correlation. Extensive experiments on synthetic and real-world data show that D3VAE outperforms competitive algorithms with remarkable margins. Our implementation is available at https://github.com/PaddlePaddle/PaddleSpatial/tree/main/research/D3VAE.},
  url = {http://arxiv.org/abs/2301.03028},
  title = {Generative {Time} {Series} {Forecasting} with {Diffusion}, {Denoise}, and {Disentanglement}}
}
@misc{noauthor_zimbra_nodate-2,
  urldate = {2023-09-24},
  url = {https://z.imt.fr/zimbra/mail#1},
  title = {Zimbra: {Réception}}
}
@misc{maaloe_biva_2019,
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
  note = {arXiv:1902.02102 [cs, stat]},
  year = {2019},
  month = {November},
  author = {Maaløe, Lars and Fraccaro, Marco and Liévin, Valentin and Winther, Ole},
  publisher = {arXiv},
  urldate = {2023-09-24},
  abstract = {With the introduction of the variational autoencoder (VAE), probabilistic latent variable models have received renewed attention as powerful generative models. However, their performance in terms of test likelihood and quality of generated samples has been surpassed by autoregressive models without stochastic units. Furthermore, flow-based models have recently been shown to be an attractive alternative that scales well to high-dimensional data. In this paper we close the performance gap by constructing VAE models that can effectively utilize a deep hierarchy of stochastic variables and model complex covariance structures. We introduce the Bidirectional-Inference Variational Autoencoder (BIVA), characterized by a skip-connected generative model and an inference network formed by a bidirectional stochastic inference path. We show that BIVA reaches state-of-the-art test likelihoods, generates sharp and coherent natural images, and uses the hierarchy of latent variables to capture different aspects of the data distribution. We observe that BIVA, in contrast to recent results, can be used for anomaly detection. We attribute this to the hierarchy of latent variables which is able to extract high-level semantic features. Finally, we extend BIVA to semi-supervised classification tasks and show that it performs comparably to state-of-the-art results by generative adversarial networks.},
  url = {http://arxiv.org/abs/1902.02102},
  shorttitle = {{BIVA}},
  title = {{BIVA}: {A} {Very} {Deep} {Hierarchy} of {Latent} {Variables} for {Generative} {Modeling}}
}
@misc{do_theory_2021,
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  note = {arXiv:1908.09961 [cs, stat]},
  year = {2021},
  month = {March},
  author = {Do, Kien and Tran, Truyen},
  publisher = {arXiv},
  urldate = {2023-09-23},
  abstract = {We make two theoretical contributions to disentanglement learning by (a) defining precise semantics of disentangled representations, and (b) establishing robust metrics for evaluation. First, we characterize the concept "disentangled representations" used in supervised and unsupervised methods along three dimensions-informativeness, separability and interpretability - which can be expressed and quantified explicitly using information-theoretic constructs. This helps explain the behaviors of several well-known disentanglement learning models. We then propose robust metrics for measuring informativeness, separability and interpretability. Through a comprehensive suite of experiments, we show that our metrics correctly characterize the representations learned by different methods and are consistent with qualitative (visual) results. Thus, the metrics allow disentanglement learning methods to be compared on a fair ground. We also empirically uncovered new interesting properties of VAE-based methods and interpreted them with our formulation. These findings are promising and hopefully will encourage the design of more theoretically driven models for learning disentangled representations.},
  url = {http://arxiv.org/abs/1908.09961},
  title = {Theory and {Evaluation} {Metrics} for {Learning} {Disentangled} {Representations}}
}
@inproceedings{higgins_beta-vae_2016,
  year = {2016},
  booktitle = {International conference on learning representations},
  author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  title = {beta-vae: Learning basic visual concepts with a constrained variational framework}
}
@misc{kim_disentangling_2019,
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  note = {arXiv:1802.05983 [cs, stat]},
  year = {2019},
  month = {July},
  author = {Kim, Hyunjik and Mnih, Andriy},
  publisher = {arXiv},
  urldate = {2023-09-22},
  abstract = {We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon \${\textbackslash}beta\$-VAE by providing a better trade-off between disentanglement and reconstruction quality. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.},
  url = {http://arxiv.org/abs/1802.05983},
  title = {Disentangling by {Factorising}}
}
@misc{kingma_auto-encoding_2022-1,
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  note = {arXiv:1312.6114 [cs, stat]},
  year = {2022},
  month = {December},
  author = {Kingma, Diederik P. and Welling, Max},
  publisher = {arXiv},
  urldate = {2023-09-22},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  doi = {10.48550/arXiv.1312.6114},
  url = {http://arxiv.org/abs/1312.6114},
  title = {Auto-{Encoding} {Variational} {Bayes}}
}
@article{bucci_new_2021,
  year = {2021},
  author = {Bucci, Giovanni and Fiorucci, Edoardo and Mari, Simone and Fioravanti, Andrea},
  journal = {IEEE Transactions on Instrumentation and Measurement},
  doi = {10.1109/TIM.2020.3035193},
  title = {A {New} {Convolutional} {Neural} {Network}-{Based} {System} for {NILM} {Applications}}
}
@article{koublal_xgen_2023,
  year = {2023},
  author = {Koublal, Ladjal S, Benhaiem D, le-borgne E and Roueff, F.},
  url = {https://xgentimeseries.github.io},
  title = {{XGen}: {A} {Comprehensive} {Archive} and an {eXplainable} {Time} {Series} {Generation} {Framework} for {Energy}}
}
@article{chen_isolating_2018-1,
  year = {2018},
  author = {Chen, Ricky TQ and Li, Xuechen and Grosse, Roger B and Duvenaud, David K},
  journal = {Advances in neural information processing systems},
  volume = {31},
  title = {Isolating sources of disentanglement in variational autoencoders}
}
@article{davidson_hyperspherical_2018,
  year = {2018},
  author = {Davidson, Tim R and Falorsi, Luca and De Cao, Nicola and Kipf, Thomas and Tomczak, Jakub M},
  journal = {arXiv preprint arXiv:1804.00891},
  title = {Hyperspherical variational auto-encoders}
}
@inproceedings{kolter_redd_2011,
  note = {Issue: Citeseer},
  year = {2011},
  author = {Kolter, J Zico and Johnson, Matthew J},
  booktitle = {Workshop on data mining applications in sustainability ({SIGKDD}), {San} {Diego}, {CA}},
  volume = {25},
  title = {{REDD}: {A} public data set for energy disaggregation research}
}
@article{bardes_vicreg_2022,
  year = {2022},
  author = {Bardes, Adrien and Ponce, Jean and LeCun, Yann},
  language = {en},
  abstract = {Recent self-supervised methods for image representation learning maximize the agreement between embedding vectors produced by encoders fed with different views of the same image. The main challenge is to prevent a collapse in which the encoders produce constant or non-informative vectors. We introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with two regularizations terms applied to both embeddings separately: (1) a term that maintains the variance of each embedding dimension above a threshold, (2) a term that decorrelates each pair of variables. Unlike most other approaches to the same problem, VICReg does not require techniques such as: weight sharing between the branches, batch normalization, feature-wise normalization, output quantization, stop gradient, memory banks, etc., and achieves results on par with the state of the art on several downstream tasks. In addition, we show that our variance regularization term stabilizes the training of other methods and leads to performance improvements.},
  title = {{VICREG}: {VARIANCE}-{INVARIANCE}-{COVARIANCE} {RE}- {GULARIZATION} {FOR} {SELF}-{SUPERVISED} {LEARNING}}
}
@inproceedings{bardes_vicreg_2021,
  year = {2021},
  month = {October},
  author = {Bardes, Adrien and Ponce, Jean and LeCun, Yann},
  urldate = {2023-09-21},
  language = {en},
  abstract = {Recent self-supervised methods for image representation learning maximize the agreement between embedding vectors produced by encoders fed with different views of the same image. The main challenge is to prevent a collapse in which the encoders produce constant or non-informative vectors. We introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with two regularizations terms applied to both embeddings separately: (1) a term that maintains the variance of each embedding dimension above a threshold, (2) a term that decorrelates each pair of variables. Unlike most other approaches to the same problem, VICReg does not require techniques such as: weight sharing between the branches, batch normalization, feature-wise normalization, output quantization, stop gradient, memory banks, etc., and achieves results on par with the state of the art on several downstream tasks. In addition, we show that our variance regularization term stabilizes the training of other methods and leads to performance improvements.},
  url = {https://openreview.net/forum?id=xm6YD62D1Ub},
  shorttitle = {{VICReg}},
  title = {{VICReg}: {Variance}-{Invariance}-{Covariance} {Regularization} for {Self}-{Supervised} {Learning}}
}
@misc{roth_disentanglement_2023,
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  note = {arXiv:2210.07347 [cs, stat]},
  year = {2023},
  month = {February},
  author = {Roth, Karsten and Ibrahim, Mark and Akata, Zeynep and Vincent, Pascal and Bouchacourt, Diane},
  publisher = {arXiv},
  urldate = {2023-09-21},
  abstract = {A grand goal in deep learning research is to learn representations capable of generalizing across distribution shifts. Disentanglement is one promising direction aimed at aligning a model's representation with the underlying factors generating the data (e.g. color or background). Existing disentanglement methods, however, rely on an often unrealistic assumption: that factors are statistically independent. In reality, factors (like object color and shape) are correlated. To address this limitation, we consider the use of a relaxed disentanglement criterion -- the Hausdorff Factorized Support (HFS) criterion -- that encourages only pairwise factorized {\textbackslash}emph\{support\}, rather than a factorial distribution, by minimizing a Hausdorff distance. This allows for arbitrary distributions of the factors over their support, including correlations between them. We show that the use of HFS consistently facilitates disentanglement and recovery of ground-truth factors across a variety of correlation settings and benchmarks, even under severe training correlations and correlation shifts, with in parts over \$+60{\textbackslash}\%\$ in relative improvement over existing disentanglement methods. In addition, we find that leveraging HFS for representation learning can even facilitate transfer to downstream tasks such as classification under distribution shifts. We hope our original approach and positive empirical results inspire further progress on the open problem of robust generalization. Code available at https://github.com/facebookresearch/disentangling-correlated-factors.},
  url = {http://arxiv.org/abs/2210.07347},
  title = {Disentanglement of {Correlated} {Factors} via {Hausdorff} {Factorized} {Support}}
}
@article{ren2021learning,
  year = {2021},
  journal = {arXiv preprint arXiv:2102.10543},
  author = {Ren, Xuanchi and Yang, Tao and Wang, Yuwang and Zeng, Wenjun},
  title = {Learning disentangled representation by exploiting pretrained generative models: A contrastive learning view}
}
@article{klindt_towards_2021,
  year = {2020},
  journal = {arXiv preprint arXiv:2007.10930},
  author = {Klindt, David and Schott, Lukas and Sharma, Yash and Ustyuzhaninov, Ivan and Brendel, Wieland and Bethge, Matthias and Paiton, Dylan},
  title = {Towards nonlinear disentanglement in natural data with temporal sparse coding}
}
@article{zhao2019deep,
  publisher = {IEEE},
  year = {2019},
  pages = {114496--114507},
  volume = {7},
  journal = {Ieee Access},
  author = {Zhao, Wentian and Gao, Yanyun and Ji, Tingxiang and Wan, Xili and Ye, Feng and Bai, Guangwei},
  title = {Deep temporal convolutional networks for short-term traffic flow forecasting}
}
@article{murray2017electrical,
  publisher = {Nature Publishing Group},
  year = {2017},
  pages = {1--12},
  number = {1},
  volume = {4},
  journal = {Scientific data},
  author = {Murray, David and Stankovic, Lina and Stankovic, Vladimir},
  title = {An electrical load measurements dataset of United Kingdom households from a two-year longitudinal study}
}
@article{paszke2019pytorch,
  year = {2019},
  volume = {32},
  journal = {Advances in neural information processing systems},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  title = {Pytorch: An imperative style, high-performance deep learning library}
}
@book{ash2012information,
  publisher = {Courier Corporation},
  year = {2012},
  author = {Ash, Robert B},
  title = {Information theory}
}
@inproceedings{zhang_use_2022,
  pages = {16639--16648},
  year = {2022},
  month = {June},
  author = {Zhang, Shu and Xu, Ran and Xiong, Caiming and Ramaiah, Chetan},
  publisher = {IEEE},
  booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  urldate = {2023-09-20},
  language = {en},
  abstract = {Current contrastive learning frameworks focus on leveraging a single supervisory signal to learn representations, which limits the efficacy on unseen data and downstream tasks. In this paper, we present a hierarchical multi-label representation learning framework that can leverage all available labels and preserve the hierarchical relationship between classes. We introduce novel hierarchy preserving losses, which jointly apply a hierarchical penalty to the contrastive loss, and enforce the hierarchy constraint. The loss function is data driven and automatically adapts to arbitrary multi-label structures. Experiments on several datasets show that our relationship-preserving embedding performs well on a variety of tasks and outperform the baseline supervised and self-supervised approaches. Code is available at https://github.com/salesforce/ hierarchicalContrastiveLearning.},
  doi = {10.1109/CVPR52688.2022.01616},
  url = {https://ieeexplore.ieee.org/document/9880213/},
  shorttitle = {Use {All} {The} {Labels}},
  isbn = {978-1-66546-946-3},
  title = {Use {All} {The} {Labels}: {A} {Hierarchical} {Multi}-{Label} {Contrastive} {Learning} {Framework}},
  address = {New Orleans, LA, USA}
}
@article{do_theory_2020,
  year = {2020},
  author = {Do, Kien and Tran, Truyen},
  language = {en},
  abstract = {We make two theoretical contributions to disentanglement learning by (a) deﬁning precise semantics of disentangled representations, and (b) establishing robust metrics for evaluation. First, we characterize the concept “disentangled representations” used in supervised and unsupervised methods along three dimensions–informativeness, separability and interpretability–which can be expressed and quantiﬁed explicitly using information-theoretic constructs. This helps explain the behaviors of several well-known disentanglement learning models. We then propose robust metrics for measuring informativeness, separability, and interpretability. Through a comprehensive suite of experiments, we show that our metrics correctly characterize the representations learned by different methods and are consistent with qualitative (visual) results. Thus, the metrics allow disentanglement learning methods to be compared on a fair ground. We also empirically uncovered new interesting properties of VAE-based methods and interpreted them with our formulation. These ﬁndings are promising and hopefully will encourage the design of more theoretically driven models for learning disentangled representations.},
  title = {{THEORY} {AND} {EVALUATION} {METRICS} {FOR} {LEARNING} {DISENTANGLED} {REPRESENTATIONS}}
}
@article{daudel_alpha-divergence_nodate,
  author = {Daudel, Kamelia and Benton, Joe and Shi, Yuyang and Doucet, Arnaud},
  language = {en},
  abstract = {Several algorithms involving the Variational R´enyi (VR) bound have been proposed to minimize an alpha-divergence between a target posterior distribution and a variational distribution. Despite promising empirical results, those algorithms resort to biased stochastic gradient descent procedures and thus lack theoretical guarantees. In this paper, we formalize and study the VR-IWAE bound, a generalization of the importance weighted auto-encoder (IWAE) bound. We show that the VR-IWAE bound enjoys several desirable properties and notably leads to the same stochastic gradient descent procedure as the VR bound in the reparameterized case, but this time by relying on unbiased gradient estimators. We then provide two complementary theoretical analyses of the VR-IWAE bound and thus of the standard IWAE bound. Those analyses shed light on the beneﬁts or lack thereof of these bounds. Lastly, we illustrate our theoretical claims over toy and real-data examples.},
  title = {Alpha-divergence {Variational} {Inference} {Meets} {Importance} {Weighted} {Auto}-{Encoders}: {Methodology} and {Asymptotics}}
}
@misc{pham_pcaae_2020,
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {arXiv:2006.07827 [cs]},
  year = {2020},
  month = {June},
  author = {Pham, Chi-Hieu and Ladjal, Saïd and Newson, Alasdair},
  publisher = {arXiv},
  urldate = {2023-09-20},
  abstract = {Autoencoders and generative models produce some of the most spectacular deep learning results to date. However, understanding and controlling the latent space of these models presents a considerable challenge. Drawing inspiration from principal component analysis and autoencoder, we propose the Principal Component Analysis Autoencoder (PCAAE). This is a novel autoencoder whose latent space verifies two properties. Firstly, the dimensions are organised in decreasing importance with respect to the data at hand. Secondly, the components of the latent space are statistically independent. We achieve this by progressively increasing the latent space during training, and with a covariance loss applied to the latent codes. The resulting autoencoder produces a latent space which separates the intrinsic attributes of the data into different components of the latent space, in a completely unsupervised manner. We also describe an extension of our approach to the case of powerful, pre-trained GANs. We show results on both synthetic examples of shapes and on a state-of-the-art GAN. For example, we are able to separate the color shade scale of hair and skin, pose of faces and the gender in the CelebA, without accessing any labels. We compare the PCAAE with other state-of-the-art approaches, in particular with respect to the ability to disentangle attributes in the latent space. We hope that this approach will contribute to better understanding of the intrinsic latent spaces of powerful deep generative models.},
  url = {http://arxiv.org/abs/2006.07827},
  shorttitle = {{PCAAE}},
  title = {{PCAAE}: {Principal} {Component} {Analysis} {Autoencoder} for organising the latent space of generative networks}
}
@inproceedings{eastwood_framework_2018,
  year = {2018},
  booktitle = {International conference on learning representations},
  author = {Eastwood, Cian and Williams, Christopher KI},
  title = {A framework for the quantitative evaluation of disentangled representations}
}
@misc{chen_simple_2020,
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
  note = {arXiv:2002.05709 [cs, stat]},
  year = {2020},
  month = {June},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  publisher = {arXiv},
  urldate = {2023-09-20},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  url = {http://arxiv.org/abs/2002.05709},
  title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}}
}
@misc{noauthor_iclr-2023-self-supervised-attention-based-variational-autoencoder-for-appliance-usage_nodate,
  urldate = {2023-09-19},
  language = {en},
  abstract = {An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
  url = {https://www.overleaf.com/project/64d55386b646ac8e003d85ac},
  title = {{ICLR}-2023-{SELF}-{SUPERVISED}-{ATTENTION}-{BASED}-{VARIATIONAL}-{AUTOENCODER}-{FOR}-{APPLIANCE}-{USAGE}}
}
@misc{khosla_supervised_2021,
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
  note = {arXiv:2004.11362 [cs, stat]},
  year = {2021},
  month = {March},
  author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
  publisher = {arXiv},
  urldate = {2023-09-19},
  abstract = {Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches subsume or significantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4\% on the ImageNet dataset, which is 0.8\% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows benefits for robustness to natural corruptions and is more stable to hyperparameter settings such as optimizers and data augmentations. Our loss function is simple to implement, and reference TensorFlow code is released at https://t.ly/supcon.},
  url = {http://arxiv.org/abs/2004.11362},
  title = {Supervised {Contrastive} {Learning}}
}
@misc{luo_time_2023,
  keywords = {Computer Science - Machine Learning},
  note = {arXiv:2303.11911 [cs]},
  year = {2023},
  month = {March},
  author = {Luo, Dongsheng and Cheng, Wei and Wang, Yingheng and Xu, Dongkuan and Ni, Jingchao and Yu, Wenchao and Zhang, Xuchao and Liu, Yanchi and Chen, Yuncong and Chen, Haifeng and Zhang, Xiang},
  publisher = {arXiv},
  urldate = {2023-09-19},
  abstract = {Various contrastive learning approaches have been proposed in recent years and achieve significant empirical success. While effective and prevalent, contrastive learning has been less explored for time series data. A key component of contrastive learning is to select appropriate augmentations imposing some priors to construct feasible positive samples, such that an encoder can be trained to learn robust and discriminative representations. Unlike image and language domains where ``desired'' augmented samples can be generated with the rule of thumb guided by prefabricated human priors, the ad-hoc manual selection of time series augmentations is hindered by their diverse and human-unrecognizable temporal structures. How to find the desired augmentations of time series data that are meaningful for given contrastive learning tasks and datasets remains an open question. In this work, we address the problem by encouraging both high {\textbackslash}textit\{fidelity\} and {\textbackslash}textit\{variety\} based upon information theory. A theoretical analysis leads to the criteria for selecting feasible data augmentations. On top of that, we propose a new contrastive learning approach with information-aware augmentations, InfoTS, that adaptively selects optimal augmentations for time series representation learning. Experiments on various datasets show highly competitive performance with up to 12.0{\textbackslash}\% reduction in MSE on forecasting tasks and up to 3.7{\textbackslash}\% relative improvement in accuracy on classification tasks over the leading baselines.},
  url = {http://arxiv.org/abs/2303.11911},
  title = {Time {Series} {Contrastive} {Learning} with {Information}-{Aware} {Augmentations}}
}
@misc{goodfellow_generative_2014,
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  note = {arXiv:1406.2661 [cs, stat]},
  year = {2014},
  month = {June},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  publisher = {arXiv},
  urldate = {2023-09-19},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  url = {http://arxiv.org/abs/1406.2661},
  title = {Generative {Adversarial} {Networks}}
}
@misc{li_dual-stream_2021,
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
  note = {arXiv:2011.08939 [cs]},
  year = {2021},
  month = {April},
  author = {Li, Bin and Li, Yin and Eliceiri, Kevin W.},
  publisher = {arXiv},
  urldate = {2023-09-19},
  abstract = {We address the challenging problem of whole slide image (WSI) classification. WSIs have very high resolutions and usually lack localized annotations. WSI classification can be cast as a multiple instance learning (MIL) problem when only slide-level labels are available. We propose a MIL-based method for WSI classification and tumor detection that does not require localized annotations. Our method has three major components. First, we introduce a novel MIL aggregator that models the relations of the instances in a dual-stream architecture with trainable distance measurement. Second, since WSIs can produce large or unbalanced bags that hinder the training of MIL models, we propose to use self-supervised contrastive learning to extract good representations for MIL and alleviate the issue of prohibitive memory cost for large bags. Third, we adopt a pyramidal fusion mechanism for multiscale WSI features, and further improve the accuracy of classification and localization. Our model is evaluated on two representative WSI datasets. The classification accuracy of our model compares favorably to fully-supervised methods, with less than 2\% accuracy gap across datasets. Our results also outperform all previous MIL-based methods. Additional benchmark results on standard MIL datasets further demonstrate the superior performance of our MIL aggregator on general MIL problems. GitHub repository: https://github.com/binli123/dsmil-wsi},
  url = {http://arxiv.org/abs/2011.08939},
  title = {Dual-stream {Multiple} {Instance} {Learning} {Network} for {Whole} {Slide} {Image} {Classification} with {Self}-supervised {Contrastive} {Learning}}
}
@misc{zbontar_barlow_2021,
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
  note = {arXiv:2103.03230 [cs, q-bio]},
  year = {2021},
  month = {June},
  author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stéphane},
  publisher = {arXiv},
  urldate = {2023-09-19},
  abstract = {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.},
  url = {http://arxiv.org/abs/2103.03230},
  shorttitle = {Barlow {Twins}},
  title = {Barlow {Twins}: {Self}-{Supervised} {Learning} via {Redundancy} {Reduction}}
}
@article{woo_cost_2022,
  year = {2022},
  journal = {arXiv preprint arXiv:2202.01575},
  author = {Woo, Gerald and Liu, Chenghao and Sahoo, Doyen and Kumar, Akshat and Hoi, Steven},
  title = {CoST: Contrastive learning of disentangled seasonal-trend representations for time series forecasting}
}
@inproceedings{liu2022multivariate,
  year = {2022},
  booktitle = {The Eleventh International Conference on Learning Representations},
  author = {LIU, SHUAI and Li, Xiucheng and Cong, Gao and Chen, Yile and JIANG, YUE},
  title = {Multivariate Time-series Imputation with Disentangled Temporal Representations}
}
@misc{vahdat_nvae_2021,
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
  note = {arXiv:2007.03898 [cs, stat]},
  year = {2021},
  month = {January},
  author = {Vahdat, Arash and Kautz, Jan},
  publisher = {arXiv},
  urldate = {2023-09-19},
  abstract = {Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256\${\textbackslash}times\$256 pixels. The source code is available at https://github.com/NVlabs/NVAE .},
  url = {http://arxiv.org/abs/2007.03898},
  shorttitle = {{NVAE}},
  title = {{NVAE}: {A} {Deep} {Hierarchical} {Variational} {Autoencoder}}
}
@article{kumar_variational_2018,
  year = {2017},
  journal = {arXiv preprint arXiv:1711.00848},
  author = {Kumar, Abhishek and Sattigeri, Prasanna and Balakrishnan, Avinash},
  title = {Variational inference of disentangled latent concepts from unlabeled observations}
}
@article{liu_multivariate_2023-1,
  year = {2023},
  author = {Liu, Shuai and Li, Xiucheng and Cong, Gao and Chen, Yile and Jiang, Yue},
  language = {en},
  abstract = {Multivariate time series often faces the problem of missing value. Many time series imputation methods have been developed in literature. However, they all rely on an entangled representation to model dynamics of time series, which may fail to fully exploit the multiple factors (e.g., periodic patterns) presented in the data. Moreover, the entangled representations usually have no semantic meaning, and thus they often lack interpretability. In addition, many recent models are proposed to deal with the whole time series to identify temporal dynamics, but they are not scalable to long time series. Different from existing approaches, we propose TIDER, a novel matrix factorization-based method with disentangled temporal representations that account for multiple factors, namely trend, seasonality, and local bias, to model complex dynamics. The learned disentanglement makes the imputation process more reliable and offers explainability for imputation results. Moreover, TIDER is scalable to long time series. Empirical results show that our method outperforms existing approaches on three typical real-world datasets, especially on long time series, reducing mean absolute error by up to 50\%. It also scales well to long datasets on which existing deep learning based methods struggle. Disentanglement validation experiments further highlight the robustness and accuracy of our model.},
  title = {{MULTIVARIATE} {TIME}-{SERIES} {IMPUTATION} {WITH} {DIS}- {ENTANGLED} {TEMPORAL} {REPRESENTATIONS}}
}
@misc{bouchacourt_multi-level_2017,
  keywords = {Computer Science - Machine Learning, ML-VAE, Statistics - Machine Learning},
  note = {arXiv:1705.08841 [cs, stat]},
  year = {2017},
  month = {May},
  author = {Bouchacourt, Diane and Tomioka, Ryota and Nowozin, Sebastian},
  publisher = {arXiv},
  urldate = {2023-09-18},
  abstract = {We would like to learn a representation of the data which decomposes an observation into factors of variation which we can independently control. Specifically, we want to use minimal supervision to learn a latent representation that reflects the semantics behind a specific grouping of the data, where within a group the samples share a common factor of variation. For example, consider a collection of face images grouped by identity. We wish to anchor the semantics of the grouping into a relevant and disentangled representation that we can easily exploit. However, existing deep probabilistic models often assume that the observations are independent and identically distributed. We present the Multi-Level Variational Autoencoder (ML-VAE), a new deep probabilistic model for learning a disentangled representation of a set of grouped observations. The ML-VAE separates the latent representation into semantically meaningful parts by working both at the group level and the observation level, while retaining efficient test-time inference. Quantitative and qualitative evaluations show that the ML-VAE model (i) learns a semantically meaningful disentanglement of grouped data, (ii) enables manipulation of the latent representation, and (iii) generalises to unseen groups.},
  url = {http://arxiv.org/abs/1705.08841},
  shorttitle = {Multi-{Level} {Variational} {Autoencoder}},
  title = {Multi-{Level} {Variational} {Autoencoder}: {Learning} {Disentangled} {Representations} from {Grouped} {Observations}}
}
@article{jutten2004advances,
  publisher = {World Scientific},
  year = {2004},
  pages = {267--292},
  number = {05},
  volume = {14},
  journal = {International journal of neural systems},
  author = {Jutten, Christian and Karhunen, Juha},
  title = {Advances in blind source separation (BSS) and independent component analysis (ICA) for nonlinear mixtures}
}
@book{peters2017elements,
  publisher = {The MIT Press},
  year = {2017},
  author = {Peters, Jonas and Janzing, Dominik and Sch{\"o}lkopf, Bernhard},
  title = {Elements of causal inference: foundations and learning algorithms}
}
@inproceedings{haber2018learning,
  year = {2018},
  number = {1},
  volume = {32},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author = {Haber, Eldad and Ruthotto, Lars and Holtham, Elliot and Jun, Seong-Hwan},
  title = {Learning across scales---multiscale methods for convolution neural networks}
}
@article{autoformer,
  year = {2021},
  pages = {22419--22430},
  volume = {34},
  journal = {Advances in neural information processing systems},
  author = {Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
  title = {Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting}
}
@inproceedings{chen2023learning,
  year = {2023},
  pages = {5896--5905},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  author = {Chen, Xiang and Li, Hao and Li, Mingqiang and Pan, Jinshan},
  title = {Learning a sparse transformer network for effective image deraining}
}
@inproceedings{fedformer,
  organization = {PMLR},
  year = {2022},
  pages = {27268--27286},
  booktitle = {International conference on machine learning},
  author = {Zhou, Tian and Ma, Ziqing and Wen, Qingsong and Wang, Xue and Sun, Liang and Jin, Rong},
  title = {Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting}
}
@inproceedings{timesnet,
  year = {2022},
  booktitle = {The eleventh international conference on learning representations},
  author = {Wu, Haixu and Hu, Tengge and Liu, Yong and Zhou, Hang and Wang, Jianmin and Long, Mingsheng},
  title = {Timesnet: Temporal 2d-variation modeling for general time series analysis}
}
@inproceedings{Dlinear,
  year = {2023},
  pages = {11121--11128},
  number = {9},
  volume = {37},
  booktitle = {Proceedings of the AAAI conference on artificial intelligence},
  author = {Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  title = {Are transformers effective for time series forecasting?}
}
@article{bai2021contrastively,
  year = {2021},
  pages = {10105--10118},
  volume = {34},
  journal = {Advances in Neural Information Processing Systems},
  author = {Bai, Junwen and Wang, Weiran and Gomes, Carla P},
  title = {Contrastively disentangled sequential variational autoencoder}
}
@incollection{jutten2010nonlinear,
  publisher = {Elsevier},
  year = {2010},
  pages = {549--592},
  booktitle = {Handbook of Blind Source Separation},
  author = {Jutten, Christian and Babaie-Zadeh, Massoud and Karhunen, Juha},
  title = {Nonlinear mixtures}
}
@article{hyvarinen2013independent,
  publisher = {The Royal Society Publishing},
  year = {2013},
  pages = {20110534},
  number = {1984},
  volume = {371},
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  author = {Hyv{\"a}rinen, Aapo},
  title = {Independent component analysis: recent advances}
}
@article{comon1994independent,
  publisher = {Elsevier},
  year = {1994},
  pages = {287--314},
  number = {3},
  volume = {36},
  journal = {Signal processing},
  author = {Comon, Pierre},
  title = {Independent component analysis, a new concept?}
}
@article{nguyen2019approximations,
  publisher = {Taylor \& Francis},
  year = {2019},
  pages = {3945--3955},
  number = {16},
  volume = {48},
  journal = {Communications in Statistics-Theory and Methods},
  author = {Nguyen, Hien D and McLachlan, Geoffrey},
  title = {On approximations via convolution-defined mixture models}
}
@article{yao2021learning,
  year = {2021},
  journal = {arXiv preprint arXiv:2110.05428},
  author = {Yao, Weiran and Sun, Yuewen and Ho, Alex and Sun, Changyin and Zhang, Kun},
  title = {Learning temporally causal latent processes from general temporal data}
}
@article{ng2023identifiability,
  year = {2023},
  pages = {47960--47990},
  volume = {36},
  journal = {Advances in Neural Information Processing Systems},
  author = {Ng, Ignavier and Zheng, Yujia and Dong, Xinshuai and Zhang, Kun},
  title = {On the identifiability of sparse ICA without assuming non-Gaussianity}
}
@inproceedings{brady2023provably,
  series = {ICML'23},
  location = {Honolulu, Hawaii, USA},
  numpages = {25},
  articleno = {126},
  booktitle = {
Proceedings of the 40th International
Conference on Machine Learning
},
  year = {2023},
  title = {
Provably Learning
Object-Centric Representations
},
  author = {
Brady, Jack and
Zimmermann, Roland S. and
Sharma, Yash and
Sch{\"o}lkopf, Bernhard and
von K{\"u}gelgen, Julius
Brendel, Wieland and
}
}
@book{hyndman2018forecasting,
  publisher = {OTexts},
  year = {2018},
  author = {Hyndman, Rob J and Athanasopoulos, George},
  title = {Forecasting: principles and practice}
}
@misc{wen2018robuststl,
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  eprint = {1812.01767},
  year = {2018},
  author = {Qingsong Wen and Jingkun Gao and Xiaomin Song and Liang Sun and Huan Xu and Shenghuo Zhu},
  title = {RobustSTL: A Robust Seasonal-Trend Decomposition Algorithm for Long Time Series}
}
@misc{wen2019robusttrend,
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  eprint = {1906.03751},
  year = {2019},
  author = {Qingsong Wen and Jingkun Gao and Xiaomin Song and Liang Sun and Jian Tan},
  title = {RobustTrend: A Huber Loss with a Combined First and Second Order Difference Regularization for Time Series Trend Filtering}
}
@inproceedings{wen2020fast,
  series = {KDD '20},
  location = {Virtual Event, CA, USA},
  keywords = {time series, generalized ADMM, seasonal-trend decomposition, multiple seasonality},
  numpages = {11},
  pages = {2203–2213},
  booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  doi = {10.1145/3394486.3403271},
  url = {https://doi.org/10.1145/3394486.3403271},
  address = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  isbn = {9781450379984},
  year = {2020},
  title = {Fast RobustSTL: Efficient and Robust Seasonal-Trend Decomposition for Time Series with Complex Patterns},
  author = {Wen, Qingsong and Zhang, Zhe and Li, Yan and Sun, Liang}
}
@inproceedings{yang2021multiscale,
  doi = {10.1109/ICASSP39728.2021.9413939},
  pages = {5085-5089},
  number = {},
  volume = {},
  year = {2021},
  title = {A Robust and Efficient Multi-Scale Seasonal-Trend Decomposition},
  booktitle = {ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  author = {Yang, Linxiao and Wen, Qingsong and Yang, Bo and Sun, Liang}
}
@article{godfrey2017decomposition,
  pages = {1–13},
  year = {2017},
  author = {Godfrey, Luke B. and Gashler, Michael S.},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  doi = {10.1109/tnnls.2017.2709324},
  url = {http://dx.doi.org/10.1109/TNNLS.2017.2709324},
  issn = {2162-2388},
  title = {Neural Decomposition of Time-Series Data for Effective Generalization}
}
@misc{yue2021ts2vec,
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  eprint = {2106.10466},
  year = {2021},
  author = {Zhihan Yue and Yujing Wang and Juanyong Duan and Tianmeng Yang and Congrui Huang and Yunhai Tong and Bixiong Xu},
  title = {TS2Vec: Towards Universal Representation of Time Series}
}
@inproceedings{tonekaboni2021unsupervised,
  url = {https://openreview.net/forum?id=8qDwejCuCN},
  year = {2021},
  booktitle = {International Conference on Learning Representations},
  author = {Sana Tonekaboni and Danny Eytan and Anna Goldenberg},
  title = {Unsupervised Representation Learning for Time Series with Temporal Neighborhood Coding}
}
@inproceedings{eldele2021time,
  year = {2021},
  pages = {2352--2359},
  booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, {IJCAI-21}},
  author = {Eldele, Emadeldeen and Ragab, Mohamed and Chen, Zhenghua and Wu, Min and Kwoh, Chee Keong and Li, Xiaoli and Guan, Cuntai},
  title = {Time-Series Representation Learning via Temporal and Contextual Contrasting}
}
@misc{franceschi2020unsupervised,
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  eprint = {1901.10738},
  year = {2020},
  author = {Jean-Yves Franceschi and Aymeric Dieuleveut and Martin Jaggi},
  title = {Unsupervised Scalable Representation Learning for Multivariate Time Series}
}
@inproceedings{zerveas2021transformer,
  series = {KDD '21},
  numpages = {11},
  pages = {2114–2124},
  booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  doi = {10.1145/3447548.3467401},
  url = {https://doi.org/10.1145/3447548.3467401},
  address = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  isbn = {9781450383325},
  year = {2021},
  title = {A Transformer-Based Framework for Multivariate Time Series Representation Learning},
  author = {Zerveas, George and Jayaraman, Srideepika and Patel, Dhaval and Bhamidipaty, Anuradha and Eickhoff, Carsten}
}
@misc{oord2019representation,
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  eprint = {1807.03748},
  year = {2019},
  author = {Aaron van den Oord and Yazhe Li and Oriol Vinyals},
  title = {Representation Learning with Contrastive Predictive Coding}
}
@misc{he2020momentum,
  primaryclass = {cs.CV},
  archiveprefix = {arXiv},
  eprint = {1911.05722},
  year = {2020},
  author = {Kaiming He and Haoqi Fan and Yuxin Wu and Saining Xie and Ross Girshick},
  title = {Momentum Contrast for Unsupervised Visual Representation Learning}
}
@article{wen2017multi,
  year = {2017},
  journal = {arXiv preprint arXiv:1711.11053},
  author = {Wen, Ruofeng and Torkkola, Kari and Narayanaswamy, Balakrishnan and Madeka, Dhruv},
  title = {A multi-horizon quantile recurrent forecaster}
}
@inproceedings{lai2018modeling,
  year = {2018},
  pages = {95--104},
  booktitle = {The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval},
  author = {Lai, Guokun and Chang, Wei-Cheng and Yang, Yiming and Liu, Hanxiao},
  title = {Modeling long-and short-term temporal patterns with deep neural networks}
}
@misc{salinas2019deepar,
  primaryclass = {cs.AI},
  archiveprefix = {arXiv},
  eprint = {1704.04110},
  year = {2019},
  author = {David Salinas and Valentin Flunkert and Jan Gasthaus},
  title = {DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks}
}
@article{rangapuram2018deep,
  year = {2018},
  pages = {7785--7794},
  volume = {31},
  journal = {Advances in neural information processing systems},
  author = {Rangapuram, Syama Sundar and Seeger, Matthias W and Gasthaus, Jan and Stella, Lorenzo and Wang, Yuyang and Januschowski, Tim},
  title = {Deep state space models for time series forecasting}
}
@inproceedings{wang2019deep,
  organization = {PMLR},
  year = {2019},
  pages = {6607--6617},
  booktitle = {International conference on machine learning},
  author = {Wang, Yuyang and Smola, Alex and Maddix, Danielle and Gasthaus, Jan and Foster, Dean and Januschowski, Tim},
  title = {Deep factors for forecasting}
}
@misc{li2020enhancing,
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  eprint = {1907.00235},
  year = {2020},
  author = {Shiyang Li and Xiaoyong Jin and Yao Xuan and Xiyou Zhou and Wenhu Chen and Yu-Xiang Wang and Xifeng Yan},
  title = {Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting}
}
@inproceedings{zhou2021informer,
  year = {2021},
  booktitle = {Proceedings of AAAI},
  author = {Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  title = {Informer: Beyond efficient transformer for long sequence time-series forecasting}
}
@misc{oreshkin2020nbeats,
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  eprint = {1905.10437},
  year = {2020},
  author = {Boris N. Oreshkin and Dmitri Carpov and Nicolas Chapados and Yoshua Bengio},
  title = {N-BEATS: Neural basis expansion analysis for interpretable time series forecasting}
}
@article{cuaresma2004forecasting,
  publisher = {Elsevier},
  year = {2004},
  pages = {87--106},
  number = {1},
  volume = {77},
  journal = {Applied Energy},
  author = {Cuaresma, Jes{\'u}s Crespo and Hlouskova, Jaroslava and Kossmeier, Stephan and Obersteiner, Michael},
  title = {Forecasting electricity spot-prices using linear univariate time-series models}
}
@article{carbonneau2008application,
  publisher = {Elsevier},
  year = {2008},
  pages = {1140--1154},
  number = {3},
  volume = {184},
  journal = {European Journal of Operational Research},
  author = {Carbonneau, Real and Laframboise, Kevin and Vahidov, Rustam},
  title = {Application of machine learning techniques for supply chain demand forecasting}
}
@article{kim2003financial,
  publisher = {Elsevier},
  year = {2003},
  pages = {307--319},
  number = {1-2},
  volume = {55},
  journal = {Neurocomputing},
  author = {Kim, Kyoung-jae},
  title = {Financial time series forecasting using support vector machines}
}
@inproceedings{laptev2017time,
  year = {2017},
  pages = {1--5},
  volume = {34},
  booktitle = {International conference on machine learning},
  author = {Laptev, Nikolay and Yosinski, Jason and Li, Li Erran and Smyl, Slawek},
  title = {Time-series extreme event forecasting with neural networks at uber}
}
@article{bai2018empirical,
  year = {2018},
  journal = {arXiv preprint arXiv:1803.01271},
  author = {Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  title = {An empirical evaluation of generic convolutional and recurrent networks for sequence modeling}
}
@article{makridakis2020m5,
  year = {2020},
  journal = {Int J Forecast},
  author = {Makridakis, S and Spiliotis, E and Assimakopoulos, V},
  title = {The M5 accuracy competition: Results, findings and conclusions}
}
@inproceedings{khurana2016cognito,
  organization = {IEEE},
  year = {2016},
  pages = {1304--1307},
  booktitle = {2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)},
  author = {Khurana, Udayan and Turaga, Deepak and Samulowitz, Horst and Parthasrathy, Srinivasan},
  title = {Cognito: Automated feature engineering for supervised learning}
}
@article{qiu2018multivariate,
  year = {2018},
  pages = {2744--2776},
  number = {1},
  volume = {19},
  journal = {J. Mach. Learn. Res.},
  author = {Qiu, Jinwen and Jammalamadaka, S Rao and Ning, Ning},
  title = {Multivariate Bayesian Structural Time Series Model.}
}
@book{scott20154,
  publisher = {University of Chicago Press},
  year = {2015},
  author = {Scott, Steven L and Varian, Hal R},
  title = {4. Bayesian Variable Selection for Nowcasting Economic Time Series}
}
@inproceedings{parascandolo2018learning,
  organization = {PMLR},
  year = {2018},
  pages = {4036--4044},
  booktitle = {International Conference on Machine Learning},
  author = {Parascandolo, Giambattista and Kilbertus, Niki and Rojas-Carulla, Mateo and Sch{\"o}lkopf, Bernhard},
  title = {Learning independent causal mechanisms}
}
@article{mitrovic2020representation,
  year = {2020},
  journal = {arXiv preprint arXiv:2010.07922},
  author = {Mitrovic, Jovana and McWilliams, Brian and Walker, Jacob and Buesing, Lars and Blundell, Charles},
  title = {Representation learning via invariant causal mechanisms}
}
@article{von2021self,
  year = {2021},
  journal = {arXiv preprint arXiv:2106.04619},
  author = {von K{\"u}gelgen, Julius and Sharma, Yash and Gresele, Luigi and Brendel, Wieland and Sch{\"o}lkopf, Bernhard and Besserve, Michel and Locatello, Francesco},
  title = {Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style}
}
@article{hyndman2008automatic,
  year = {2008},
  pages = {1--22},
  number = {1},
  volume = {27},
  journal = {Journal of statistical software},
  author = {Hyndman, Rob J and Khandakar, Yeasmin},
  title = {Automatic time series forecasting: the forecast package for R}
}
@book{shumway2000time,
  publisher = {Springer},
  year = {2000},
  volume = {3},
  author = {Shumway, Robert H and Stoffer, David S and Stoffer, David S},
  title = {Time series analysis and its applications}
}
@article{van2008visualizing,
  year = {2008},
  number = {11},
  volume = {9},
  journal = {Journal of machine learning research},
  author = {Van der Maaten, Laurens and Hinton, Geoffrey},
  title = {Visualizing data using t-SNE.}
}
@article{de2011forecasting,
  publisher = {Taylor \& Francis},
  year = {2011},
  pages = {1513--1527},
  number = {496},
  volume = {106},
  journal = {Journal of the American statistical association},
  author = {De Livera, Alysha M and Hyndman, Rob J and Snyder, Ralph D},
  title = {Forecasting time series with complex seasonal patterns using exponential smoothing}
}
@article{cordeiro2009forecasting,
  year = {2009},
  pages = {135--149},
  number = {2},
  volume = {7},
  journal = {REVSTAT-Statistical Journal},
  author = {Cordeiro, Clara and Neves, M},
  title = {Forecasting time series with BOOT. EXPOS procedure}
}
@inproceedings{Pearl2012TheDR,
  year = {2012},
  booktitle = {UAI},
  author = {Judea Pearl},
  title = {The Do-Calculus Revisited}
}
@misc{he2021fastmoe,
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  eprint = {2103.13262},
  year = {2021},
  author = {Jiaao He and Jiezhong Qiu and Aohan Zeng and Zhilin Yang and Jidong Zhai and Jie Tang},
  title = {FastMoE: A Fast Mixture-of-Expert Training System}
}
@article{bengio2013representation,
  year = {2013},
  title = {Representation Learning: A Review and New Perspectives},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  author = {Bengio, Y. and Courville, A. and Vincent, P.}
}
@inproceedings{lachapelle2023synergies,
  organization = {PMLR},
  year = {2023},
  pages = {18171--18206},
  booktitle = {International Conference on Machine Learning},
  author = {Lachapelle, S{\'e}bastien and Deleu, Tristan and Mahajan, Divyat and Mitliagkas, Ioannis and Bengio, Yoshua and Lacoste-Julien, Simon and Bertrand, Quentin},
  title = {Synergies between Disentanglement and Sparsity: Generalization and Identifiability in Multi-Task Learning}
}
@inproceedings{ahuja2023interventional,
  organization = {PMLR},
  year = {2023},
  pages = {372--407},
  booktitle = {International Conference on Machine Learning},
  author = {Ahuja, Kartik and Mahajan, Divyat and Wang, Yixin and Bengio, Yoshua},
  title = {Interventional causal representation learning}
}
@article{moran2022identifiable,
  year = {2022},
  journal = {Transactions on machine learning research},
  author = {Moran, G and Sridhar, D and Wang, Y and Blei, D},
  title = {Identifiable Deep Generative Models via Sparse Decoding}
}
@inproceedings{Dlinear,
  year = {2023},
  pages = {11121--11128},
  volume = {34},
  number = {9},
  booktitle = {Proceedings of the AAAI conference on artificial intelligence},
  author = {Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  title = {Are transformers effective for time series forecasting?}
}
@inproceedings{Engelcke2020GENESIS:,
  url = {https://openreview.net/forum?id=BkxfaTVFwH},
  year = {2020},
  booktitle = {International Conference on Learning Representations},
  author = {Martin Engelcke and Adam R. Kosiorek and Oiwi Parker Jones and Ingmar Posner},
  title = {GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations}
}
@inproceedings{Lin2020SPACE:,
  year = {2020},
  booktitle = {International Conference on Learning Representations},
  author = {Zhixuan Lin and Yi-Fu Wu and Skand Vishwanath Peri and Weihao Sun and Gautam Singh and Fei Deng and Jindong Jiang and Sungjin Ahn},
  title = {SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition}
}
@inproceedings{jia2023improving,
  url = {https://openreview.net/forum?id=_-FN9mJsgg},
  year = {2023},
  booktitle = {The Eleventh International Conference on Learning Representations },
  author = {Baoxiong Jia and Yu Liu and Siyuan Huang},
  title = {Improving Object-centric Learning with Query Optimization}
}
@inproceedings{seitzer2023bridging,
  year = {2023},
  booktitle = {The Eleventh International Conference on Learning Representations },
  author = {Maximilian Seitzer and Max Horn and Andrii Zadaianchuk and Dominik Zietlow and Tianjun Xiao and Carl-Johann Simon-Gabriel and Tong He and Zheng Zhang and Bernhard Sch{\"o}lkopf and Thomas Brox and Francesco Locatello},
  title = {Bridging the Gap to Real-World Object-Centric Learning}
}
@article{vankovTrainingNeuralNetworks2020,
  file = {C:\Users\thadd\OneDrive\Dokumente\zotero\vankovTrainingNeuralNetworks2020.pdf},
  keywords = {combinatorial generalization,neural networks,symbols},
  urldate = {2022-12-08},
  doi = {10.1098/rstb.2019.0309},
  publisher = {{Royal Society}},
  pages = {20190309},
  number = {1791},
  volume = {375},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  month = {February},
  year = {2020},
  author = {Vankov, Ivan I. and Bowers, Jeffrey S.},
  title = {Training Neural Networks to Encode Symbols Enables Combinatorial Generalization}
}
@article{greff2020binding,
  year = {2020},
  journal = {arXiv preprint arXiv:2012.05208},
  author = {Greff, Klaus and Van Steenkiste, Sjoerd and Schmidhuber, J{\"u}rgen},
  title = {On the binding problem in artificial neural networks}
}
@article{lake_ullman_tenenbaum_gershman_2017,
  pages = {e253},
  year = {2017},
  author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
  publisher = {Cambridge University Press},
  journal = {Behavioral and Brain Sciences},
  doi = {10.1017/S0140525X16001837},
  volume = {40},
  title = {Building machines that learn and think like people}
}
@inproceedings{lachapelle2021disentanglement,
  year = {2021},
  title = {Disentanglement via Mechanism Sparsity Regularization: A New Principle for Nonlinear ICA},
  booktitle = {First Conference on Causal Learning and Reasoning},
  author = {Lachapelle, S{\'e}bastien and Rodriguez, Pau and Sharma, Yash and Everett, Katie E and Le Priol, R{\'e}mi and Lacoste, Alexandre and Lacoste-Julien, Simon}
}
@inproceedings{gresele2020incomplete,
  year = {2019},
  volume = {115},
  title = {The Incomplete Rosetta Stone problem: Identifiability results for
Multi-view Nonlinear {ICA}},
  series = {Proceedings of Machine Learning Research},
  pages = {217--227},
  booktitle = {Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial
Intelligence, {UAI} 2019, Tel Aviv, Israel, July 22-25, 2019},
  biburl = {https://dblp.org/rec/conf/uai/GreseleRMLS19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  author = {Luigi Gresele and
Paul K. Rubenstein and
Arash Mehrjou and
Francesco Locatello and
Bernhard Sch{\"{o}}lkopf}
}
@inproceedings{shu2019weakly,
  year = {2020},
  title = {Weakly Supervised Disentanglement with Guarantees},
  booktitle = {{ICLR}},
  author = {Rui Shu and
Yining Chen and
Abhishek Kumar and
Stefano Ermon and
Ben Poole}
}
@inproceedings{khemakhem2020ice,
  year = {2020},
  title = {ICE-BeeM: Identifiable Conditional Energy-Based Deep Models Based
on Nonlinear {ICA}},
  timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
  biburl = {https://dblp.org/rec/conf/nips/KhemakhemMKH20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  author = {Ilyes Khemakhem and
Ricardo Pio Monti and
Diederik P. Kingma and
Aapo Hyv{\"{a}}rinen}
}
@article{Hyvrinen2023NonlinearIC,
  volume = {abs/2303.16535},
  year = {2023},
  journal = {ArXiv},
  author = {Aapo Hyv{\"a}rinen and Ilyes Khemakhem and Hiroshi Morioka},
  title = {Nonlinear Independent Component Analysis for Principled Disentanglement in Unsupervised Deep Learning}
}
@inproceedings{Brady2023ProvablyLO,
  publisher = {PMLR},
  month = {23--29 Jul},
  series = {Proceedings of Machine Learning Research},
  volume = {202},
  year = {2023},
  pages = {3038--3062},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  author = {Brady, Jack and Zimmermann, Roland S. and Sharma, Yash and Sch\"{o}lkopf, Bernhard and Von K\"{u}gelgen, Julius and Brendel, Wieland},
  title = {Provably Learning Object-Centric Representations}
}
@inproceedings{Halva2021,
  year = {2021},
  title = {Disentangling Identifiable Features from Noisy Data with Structured
Nonlinear {ICA}},
  pages = {1624--1633},
  booktitle = {NeurIPS},
  author = {Hermanni H{\"{a}}lv{\"{a}} and
Sylvain Le Corff and
Luc Leh{\'{e}}ricy and
Jonathan So and
Yongjie Zhu and
Elisabeth Gassiat and
Aapo Hyv{\"{a}}rinen}
}
@article{moran2021identifiable,
  note = {},
  year = {2022},
  issn = {2835-8856},
  journal = {Transactions on Machine Learning Research},
  author = {Gemma Elyse Moran and Dhanya Sridhar and Yixin Wang and David Blei},
  title = {Identifiable Deep Generative Models via Sparse Decoding}
}
@inproceedings{assouel2022objectcentric,
  year = {2022},
  booktitle = {ICLR2022 Workshop on the Elements of Reasoning: Objects, Structure and Causality},
  author = {Rim Assouel and Pau Rodriguez and Perouz Taslakian and David Vazquez and Yoshua Bengio},
  title = {Object-centric Compositional Imagination for Visual Abstract Reasoning}
}
@article{ellis2023dreamcoder,
  publisher = {The Royal Society},
  year = {2023},
  pages = {20220050},
  number = {2251},
  volume = {381},
  journal = {Philosophical Transactions of the Royal Society A},
  author = {Ellis, Kevin and Wong, Lionel and Nye, Maxwell and Sable-Meyer, Mathias and Cary, Luc and Anaya Pozo, Lore and Hewitt, Luke and Solar-Lezama, Armando and Tenenbaum, Joshua B},
  title = {DreamCoder: growing generalizable, interpretable knowledge with wake--sleep Bayesian program learning}
}
@article{cemgil2020autoencoding,
  year = {2020},
  pages = {15077--15087},
  volume = {33},
  journal = {Advances in Neural Information Processing Systems},
  author = {Cemgil, Taylan and Ghaisas, Sumedh and Dvijotham, Krishnamurthy and Gowal, Sven and Kohli, Pushmeet},
  title = {The autoencoding variational autoencoder}
}
@article{sinha2021consistency,
  year = {2021},
  pages = {12943--12954},
  volume = {34},
  journal = {Advances in Neural Information Processing Systems},
  author = {Sinha, Samarth and Dieng, Adji Bousso},
  title = {Consistency regularization for variational auto-encoders}
}
@inproceedings{leeb2021assays,
  year = {2022},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Felix Leeb and Stefan Bauer and Michel Besserve and Bernhard Sch{\"o}lkopf},
  title = {Exploring the Latent Space of Autoencoders with Interventional Assays}
}
@article{lachapelle2023additive,
  year = {2023},
  journal = {arXiv preprint arXiv:2307.02598},
  author = {Lachapelle, S{\'e}bastien and Mahajan, Divyat and Mitliagkas, Ioannis and Lacoste-Julien, Simon},
  title = {Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation}
}
@inproceedings{zhao2022toward,
  organization = {PMLR},
  year = {2022},
  pages = {26841--26864},
  booktitle = {International Conference on Machine Learning},
  author = {Zhao, Linfeng and Kong, Lingzhi and Walters, Robin and Wong, Lawson LS},
  title = {Toward Compositional Generalization in Object-Oriented World Modeling}
}
@article{wiedemer2023compositional,
  year = {2023},
  journal = {arXiv preprint arXiv:2307.05596},
  author = {Wiedemer, Thadd{\"a}us and Mayilvahanan, Prasanna and Bethge, Matthias and Brendel, Wieland},
  title = {Compositional Generalization from First Principles}
}
@article{zhao2018bias,
  year = {2018},
  volume = {31},
  journal = {Advances in Neural Information Processing Systems},
  author = {Zhao, Shengjia and Ren, Hongyu and Yuan, Arianna and Song, Jiaming and Goodman, Noah and Ermon, Stefano},
  title = {Bias and generalization in deep generative models: An empirical study}
}
@inproceedings{Dittadi2021GeneralizationAR,
  year = {2021},
  booktitle = {International Conference on Machine Learning},
  author = {Andrea Dittadi and Samuele Papa and Michele De Vita and Bernhard Sch{\"o}lkopf and Ole Winther and Francesco Locatello},
  title = {Generalization and Robustness Implications in Object-Centric Learning}
}
@article{Liang2023CausalCA,
  volume = {abs/2305.17225},
  year = {2023},
  journal = {ArXiv},
  author = {Wendong Liang and Armin Keki'c and Julius von K{\"u}gelgen and Simon Buchholz and Michel Besserve and Luigi Gresele and Bernhard Sch{\"o}lkopf},
  title = {Causal Component Analysis}
}
@inproceedings{horan2021when,
  year = {2021},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Daniella Horan and Eitan Richardson and Yair Weiss},
  title = {When Is Unsupervised Disentanglement Possible?}
}
@inproceedings{Buchholz2022FunctionCF,
  year = {2022},
  title = {Function Classes for Identifiable Nonlinear Independent Component
Analysis},
  booktitle = {NeurIPS},
  author = {Simon Buchholz and
Michel Besserve and
Bernhard Sch{\"{o}}lkopf}
}
@article{rezende2018taming,
  year = {2018},
  journal = {arXiv preprint arXiv:1810.00597},
  author = {Rezende, Danilo Jimenez and Viola, Fabio},
  title = {Taming vaes}
}
@inproceedings{Zheng2022OnTI,
  year = {2022},
  title = {On the Identifiability of Nonlinear {ICA:} Sparsity and Beyond},
  booktitle = {NeurIPS},
  author = {Yujia Zheng and
Ignavier Ng and
Kun Zhang}
}
@inproceedings{montero022lost,
  year = {2022},
  volume = {35},
  title = {Lost in Latent Space: Examining failures of disentangled models at combinatorial generalisation},
  publisher = {Curran Associates, Inc.},
  pages = {10136--10149},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Montero, Milton and Bowers, Jeffrey and Ponte Costa, Rui  and Ludwig, Casimir and Malhotra, Gaurav}
}
@inproceedings{schott2022visual,
  year = {2022},
  booktitle = {International Conference on Learning Representations},
  author = {Lukas Schott and Julius Von K{\"u}gelgen and Frederik Tr{\"a}uble and Peter Vincent Gehler and Chris Russell and Matthias Bethge and Bernhard Sch{\"o}lkopf and Francesco Locatello and Wieland Brendel},
  title = {Visual Representation Learning Does Not Generalize Strongly Within the Same Domain}
}
@inproceedings{montero2021the,
  year = {2021},
  booktitle = {International Conference on Learning Representations},
  author = {Milton Llera Montero and Casimir JH Ludwig and Rui Ponte Costa and Gaurav Malhotra and Jeffrey Bowers},
  title = {The role of Disentanglement in Generalisation}
}
@article{KurthNelson2022ReplayAC,
  pages = {454-469},
  volume = {111},
  year = {2022},
  journal = {Neuron},
  author = {Zeb Kurth-Nelson and Timothy Edward John Behrens and Greg Wayne and Kevin J. Miller and Lennart Luettgau and Raymond Dolan and Yunzhe Liu and Philipp Schwartenbeck},
  title = {Replay and compositional computation}
}
@article{Bakermans2023,
  journal = {bioRxiv},
  eprint = {https://www.biorxiv.org/content/early/2023/04/07/2023.04.07.536053.full.pdf},
  publisher = {Cold Spring Harbor Laboratory},
  doi = {10.1101/2023.04.07.536053},
  year = {2023},
  elocation-id = {2023.04.07.536053},
  title = {Constructing future behaviour in the hippocampal formation through composition and replay},
  author = {Jacob J.W. Bakermans and Joseph Warren and James C.R. Whittington and Timothy E.J. Behrens}
}
@article{Schwartenbeck2021.06.06.447249,
  journal = {bioRxiv},
  eprint = {https://www.biorxiv.org/content/early/2021/06/06/2021.06.06.447249.full.pdf},
  publisher = {Cold Spring Harbor Laboratory},
  doi = {10.1101/2021.06.06.447249},
  year = {2021},
  elocation-id = {2021.06.06.447249},
  title = {Generative replay for compositional visual understanding in the prefrontal-hippocampal circuit},
  author = {Philipp Schwartenbeck and Alon Baram and Yunzhe Liu and Shirley Mark and Timothy Muller and Raymond Dolan and Matthew Botvinick and Zeb Kurth-Nelson and Timothy Behrens}
}
@misc{multiobjectdatasets19,
  year = {2019},
  howpublished = {https://github.com/deepmind/multi-object-datasets/},
  author = {Kabra, Rishabh and Burgess, Chris and Matthey, Loic and
Kaufman, Raphael Lopez and Greff, Klaus and Reynolds, Malcolm and
Lerchner, Alexander},
  title = {Multi-Object Datasets}
}
@misc{kuhnle2017shapeworld,
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  eprint = {1704.04517},
  year = {2017},
  author = {Alexander Kuhnle and Ann Copestake},
  title = {ShapeWorld - A new test methodology for multimodal language understanding}
}
@inproceedings{greff2019multi,
  year = {2019},
  volume = {97},
  title = {Multi-Object Representation Learning with Iterative Variational Inference},
  series = {Proceedings of Machine Learning Research},
  pages = {2424--2433},
  booktitle = {{ICML}},
  author = {Klaus Greff and
Rapha{\"{e}}l Lopez Kaufman and
Rishabh Kabra and
Nick Watters and
Chris Burgess and
Daniel Zoran and
Loic Matthey and
Matthew M. Botvinick and
Alexander Lerchner}
}
@inproceedings{singh2021illiterate,
  year = {2022},
  booktitle = {International Conference on Learning Representations},
  author = {Gautam Singh and Fei Deng and Sungjin Ahn},
  title = {Illiterate {DALL}-E Learns to Compose}
}
@article{elsayed2022savi++,
  year = {2022},
  pages = {28940--28954},
  volume = {35},
  journal = {Advances in Neural Information Processing Systems},
  author = {Elsayed, Gamaleldin and Mahendran, Aravindh and van Steenkiste, Sjoerd and Greff, Klaus and Mozer, Michael C and Kipf, Thomas},
  title = {Savi++: Towards end-to-end object-centric learning from real-world videos}
}
@article{fradyLearningGeneralizationCompositional2023,
  year = {2023},
  journal = {arXiv preprint arXiv:2303.13691},
  author = {Frady, E Paxon and Kent, Spencer and Tran, Quinn and Kanerva, Pentti and Olshausen, Bruno A and Sommer, Friedrich T},
  title = {Learning and generalization of compositional representations of visual scenes}
}
@inproceedings{dongFirstStepsUnderstanding2022,
  langid = {english},
  urldate = {2023-08-30},
  month = {September},
  year = {2022},
  author = {Dong, Kefan and Ma, Tengyu},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  title = {First {{Steps Toward Understanding}} the {{Extrapolation}} of {{Nonlinear Models}} to {{Unseen Domains}}}
}
@inproceedings{kipf2019contrastive,
  year = {2020},
  booktitle = {International Conference on Learning Representations},
  author = {Thomas Kipf and Elise van der Pol and Max Welling},
  title = {Contrastive Learning of Structured World Models}
}
@article{madanWhenHowCNNs2021,
  year = {2020},
  journal = {arXiv preprint arXiv:2007.08032},
  author = {Madan, Spandan and Henry, Timothy and Dozier, Jamell and Ho, Helen and Bhandari, Nishchal and Sasaki, Tomotake and Durand, Fr{\'e}do and Pfister, Hanspeter and Boix, Xavier},
  title = {When and how CNNs generalize to out-of-distribution category-viewpoint combinations}
}
@misc{burgess2018understanding,
  primaryclass = {stat.ML},
  archiveprefix = {arXiv},
  eprint = {1804.03599},
  year = {2018},
  author = {Christopher P. Burgess and Irina Higgins and Arka Pal and Loic Matthey and Nick Watters and Guillaume Desjardins and Alexander Lerchner},
  title = {Understanding disentangling in $\beta$-VAE}
}
@misc{burgessMONetUnsupervisedScene2019,
  archiveprefix = {arxiv},
  urldate = {2023-05-16},
  doi = {10.48550/arXiv.1901.11390},
  publisher = {{arXiv}},
  primaryclass = {cs, stat},
  eprint = {1901.11390},
  number = {arXiv:1901.11390},
  month = {January},
  year = {2019},
  author = {Burgess, Christopher P. and Matthey, Loic and Watters, Nicholas and Kabra, Rishabh and Higgins, Irina and Botvinick, Matt and Lerchner, Alexander},
  shorttitle = {{{MONet}}},
  title = {{{MONet}}: {{Unsupervised Scene Decomposition}} and {{Representation}}}
}
@misc{spriteworld19,
  year = {2019},
  howpublished = {https://github.com/deepmind/spriteworld/},
  title = {Spriteworld: A Flexible, Configurable Reinforcement Learning Environment},
  author = {Nicholas Watters and Loic Matthey and Sebastian Borgeaud and Rishabh Kabra and Alexander Lerchner}
}
@inproceedings{loshchilov2019decoupled,
  year = {2019},
  booktitle = {International Conference on Learning Representations},
  author = {Ilya Loshchilov and Frank Hutter},
  title = {Decoupled Weight Decay Regularization}
}
@article{Tenenbaum2011HowTG,
  year = {2011},
  volume = {331},
  title = {How to Grow a Mind: Statistics, Structure, and Abstraction},
  pages = {1279 - 1285},
  journal = {Science},
  author = {Joshua B. Tenenbaum and Charles Kemp and Thomas L. Griffiths and Noah D. Goodman}
}
@article{goyal2020inductive,
  publisher = {The Royal Society},
  year = {2022},
  pages = {20210068},
  number = {2266},
  volume = {478},
  journal = {Proceedings of the Royal Society A},
  author = {Goyal, Anirudh and Bengio, Yoshua},
  title = {Inductive biases for deep learning of higher-level cognition}
}
@article{BEHRENS2018490,
  author = {Timothy E.J. Behrens and Timothy H. Muller and James C.R. Whittington and Shirley Mark and Alon B. Baram and Kimberly L. Stachenfeld and Zeb Kurth-Nelson},
  doi = {https://doi.org/10.1016/j.neuron.2018.10.002},
  issn = {0896-6273},
  year = {2018},
  pages = {490-509},
  number = {2},
  volume = {100},
  journal = {Neuron},
  title = {What Is a Cognitive Map? Organizing Knowledge for Flexible Behavior}
}
@article{Battaglia2018RelationalIB,
  year = {2018},
  volume = {abs/1806.01261},
  title = {Relational inductive biases, deep learning, and graph networks},
  journal = {ArXiv},
  author = {Peter W. Battaglia and Jessica B. Hamrick and Victor Bapst and Alvaro Sanchez-Gonzalez and Vin{\'i}cius Flores Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Çaglar G{\"u}lçehre and H. Francis Song and Andrew J. Ballard and Justin Gilmer and George E. Dahl and Ashish Vaswani and Kelsey R. Allen and Charlie Nash and Victoria Langston and Chris Dyer and Nicolas Manfred Otto Heess and Daan Wierstra and Pushmeet Kohli and Matthew M. Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu}
}
@article{Fodor1988ConnectionismAC,
  pages = {3-71},
  volume = {28},
  year = {1988},
  journal = {Cognition},
  author = {Jerry A. Fodor and Zenon W. Pylyshyn},
  title = {Connectionism and cognitive architecture: A critical analysis}
}
@article{yuan2023compositional,
  publisher = {IEEE},
  year = {2023},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Yuan, Jinyang and Chen, Tonglin and Li, Bin and Xue, Xiangyang},
  title = {Compositional Scene Representation Learning via Reconstruction: A Survey}
}
@inproceedings{Paszke2019PyTorchAI,
  year = {2019},
  title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  pages = {8024--8035},
  booktitle = {NeurIPS},
  author = {Adam Paszke and
Sam Gross and
Francisco Massa and
Adam Lerer and
James Bradbury and
Gregory Chanan and
Trevor Killeen and
Zeming Lin and
Natalia Gimelshein and
Luca Antiga and
Alban Desmaison and
Andreas K{\"{o}}pf and
Edward Z. Yang and
Zachary DeVito and
Martin Raison and
Alykhan Tejani and
Sasank Chilamkurthy and
Benoit Steiner and
Lu Fang and
Junjie Bai and
Soumith Chintala}
}
