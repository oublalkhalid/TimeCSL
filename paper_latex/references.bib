@article{Hoerl1970,
  title={Ridge regression: Biased estimation for nonorthogonal problems},
  author={A. E. Hoerl and R. W. Kennard},
  journal={Technometrics},
  volume={12},
  number={1},
  pages={55--67},
  year={1970},
  publisher={Taylor \& Francis}
}


@inproceedings{lenc2015understanding,
  title={Understanding image representations by measuring their equivariance and equivalence},
  author={Lenc, Karel and Vedaldi, Andrea},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={991--999},
  year={2015}
}

@article{wiedemer2023provable,
  title={Provable Compositional Generalization for Object-Centric Learning},
  author={Wiedemer, Thadd{\"a}us and Brady, Jack and Panfilov, Alexander and Juhos, Attila and Bethge, Matthias and Brendel, Wieland},
  journal={arXiv preprint arXiv:2310.05327},
  year={2023}
}
@article{jiang2016variational,
  title={Variational deep embedding: An unsupervised and generative approach to clustering},
  author={Jiang, Zhuxi and Zheng, Yin and Tan, Huachun and Tang, Bangsheng and Zhou, Hanning},
  journal={arXiv preprint arXiv:1611.05148},
  year={2016}
}


@article{bach2011convex,
  title={Optimization with sparsity-inducing penalties},
  author={Bach, Francis and Jenatton, Rodolphe and Mairal, Julien and Obozinski, Guillaume and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={4},
  number={1},
  pages={1--106},
  year={2012},
  publisher={Now Publishers, Inc.}
}


@inproceedings{locatelloObjectCentricLearningSlot2020,
  title = {Object-{{Centric Learning}} with {{Slot Attention}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Locatello, Francesco and Weissenborn, Dirk and Unterthiner, Thomas and Mahendran, Aravindh and Heigold, Georg and Uszkoreit, Jakob and Dosovitskiy, Alexey and Kipf, Thomas},
  year = {2020},
  volume = {33},
  pages = {11525--11538},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-02-28}
}

@article{kuhnHungarianMethodAssignment1955,
  title = {The {{Hungarian}} Method for the Assignment Problem},
  author = {Kuhn, H. W.},
  year = {1955},
  month = mar,
  journal = {Naval Research Logistics Quarterly},
  volume = {2},
  number = {1-2},
  pages = {83--97},
  issn = {00281441, 19319193},
  doi = {10.1002/nav.3800020109},
  urldate = {2023-08-21},
  langid = {english}
}

@inproceedings{eastwood2018framework,
  title={A framework for the quantitative evaluation of disentangled representations},
  author={Eastwood, Cian and Williams, Christopher KI},
  booktitle={International Conference on Learning Representations},
  year={2018}
}


@article{Tibshirani1996,
  title={Regression shrinkage and selection via the lasso},
  author={R. Tibshirani},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={58},
  number={1},
  pages={267--288},
  year={1996},
  publisher={Wiley Online Library}
}


@article{Lounici_Pontil_Tsybakov2010,
  title={Oracle inequalities and optimal inference under group sparsity},
  author={K. Lounici and M. Pontil and A. B. Tsybakov},
  journal={The Annals of statistics},
  year={2011}
}

@article{Bickel_Ritov_Tsybakov2009,
  title={Simultaneous analysis of Lasso and {Dantzig} selector},
  author={P. J. Bickel and Y. Ritov and A. B. Tsybakov},
  journal={The Annals of statistics},
  volume={37},
  number={4},
  pages={1705--1732},
  year={2009},
  publisher={Institute of Mathematical Statistics}
}

@article{Mairal_Ponce_Sapiro_Zisserman_Bach2008,
  title={Supervised dictionary learning},
  author={J. Mairal and J. Ponce and G. Sapiro and A. Zisserman and F. Bach},
  journal={Advances in neural information processing systems},
  volume={21},
  year={2008}
}

@article{Chen1998atomic,
author = {Chen, S. S. and Donoho, D. L. and Saunders, M. A.},
title = {Atomic Decomposition by Basis Pursuit},
journal = {SIAM Journal on Scientific Computing},
year = {1998}
}



@article{Mairal2011,
  title={Task-driven dictionary learning},
  author={J. Mairal and F. Bach and J. Ponce},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={34},
  number={4},
  pages={791--804},
  year={2011},
  publisher={IEEE}
}

@inproceedings{sparseComponentAnalysisSurvey,
  TITLE = {A survey of Sparse Component Analysis for blind source separation: principles, perspectives, and new challenges},
  AUTHOR = {Gribonval, R. and Lesage, S.},
  BOOKTITLE = {{ESANN'06 proceedings - 14th European Symposium on Artificial Neural Networks}},
  YEAR = {2006}
}

@article{Richtarik2014,
  title={Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function},
  author={P. Richt{\'a}rik and M. Tak{\'a}{\v{c}}},
  journal={Mathematical Programming},
  volume={144},
  number={1},
  pages={1--38},
  year={2014},
  publisher={Springer}
}


@article{Lecun2015,
  title={Deep learning},
  author={Y. LeCun and Y. Bengio and G. Hinton},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}


@inproceedings{He2016,
  title={Deep residual learning for image recognition},
  author={K. He and X. Zhang and S. Ren and J. Sun},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}


@article{Wright_Nocedal1999,
  title={Numerical optimization},
  author={S. Wright and J. Nocedal},
  journal={Springer Science},
  volume={35},
  number={67-68},
  pages={7},
  year={1999}
}


@article{kreutz2003dictionary,
  title={Dictionary learning algorithms for sparse representation},
  author={K. Kreutz-Delgado and J. F. Murray and B. D. Rao and K. Engan and T.-W. Lee and T. J. Sejnowski},
  journal={Neural computation},
  volume={15},
  number={2},
  pages={349--396},
  year={2003},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}


@inproceedings{Mairal_Bach_Ponce2009,
  title={Online dictionary learning for sparse coding},
  author={J. Mairal and F. Bach and J. Ponce and G. Sapiro},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={689--696},
  year={2009}
}


@article{layernorm,
  title={Layer normalization},
  author={J. L. Ba and J. R. Kiros and G. E. Hinton},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}


@article{Malezieux2022,
  title={Dictionary and prior learning with unrolled algorithms for unsupervised inverse problems},
  author={B. Mal{\'e}zieux and T. Moreau and M. Kowalski},
  journal={ICLR},
  year={2022}
}


@article{Kingma_Ba2014,
  title={Adam: A method for stochastic optimization},
  author={D. P. Kingma and J. Ba},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}


@article{Obozinski2006,
  title={Multi-task feature selection},
  author={G. Obozinski and B. Taskar and M. Jordan},
  journal={Statistics Department, UC Berkeley, Tech. Rep},
  volume={2},
  number={2.2},
  pages={2},
  year={2006},
  publisher={Citeseer}
}


@article{Lounici2009,
  title={Taking advantage of sparsity in multi-task learning},
  author={K. Lounici and M. Pontil and A. B. Tsybakov and S. Van De Geer},
  journal={arXiv preprint arXiv:0903.1468},
  year={2009}
}


@article{Argyriou2008,
  title={Convex multi-task feature learning},
  author={A. Argyriou and T. Evgeniou and M. Pontil},
  journal={Machine learning},
  volume={73},
  number={3},
  pages={243--272},
  year={2008},
  publisher={Springer}
}

@article{benefitOfMTRL2016,
author = {Maurer, A. and Pontil, M. and Romera-Paredes, B.},
title = {The Benefit of Multitask Representation Learning},
year = {2016},
journal = {J. Mach. Learn. Res.}
}


@article{lounici2011oracle,
  title={Oracle inequalities and optimal inference under group sparsity},
  author={Lounici, K. and Pontil, M. and Van De Geer, S. and Tsybakov, A. B},
  journal={The annals of statistics},
  year={2011}
}


@book{Goodfellow2016,
  title={Deep learning},
  author={I. Goodfellow and Y. Bengio and A. Courville},
  year={2016},
  publisher={MIT press}
}

@inproceedings{Bertrand2020,
  title={Implicit differentiation of Lasso-type models for hyperparameter optimization},
  author={Q. Bertrand and Q. Klopfenstein and M. Blondel and S. Vaiter and A. Gramfort and J. Salmon},
  booktitle={International Conference on Machine Learning},
  pages={810--821},
  year={2020},
  organization={PMLR}
}


@article{Bolte2022,
  title={Automatic differentiation of nonsmooth iterative algorithms},
  author={J. Bolte and E. Pauwels and S. Vaiter},
  journal={NeurIPS},
  year={2022}
}


@article{Bolte2021,
  title={Nonsmooth implicit differentiation for machine-learning and optimization},
  author={J. Bolte and T. Le and E. and Pauwels and T. Silveti-Falls},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={13537--13549},
  year={2021}
}


@article{Bertrand2022,
  title={Implicit differentiation for fast hyperparameter selection in non-smooth convex learning},
  author={Q. Bertrand and Q. Klopfenstein and M. Massias and M. Blondel and S. Vaiter and A. Gramfort and J. Salmon},
  journal={JMLR},
  year={2022}
}

@inproceedings{Franceschi2018,
  title={Bilevel programming for hyperparameter optimization and meta-learning},
  author={L. Franceschi and P. Frasconi and S. Salzo and R. Grazzi and M. Pontil},
  booktitle={International Conference on Machine Learning},
  pages={1568--1577},
  year={2018},
  organization={PMLR}
}

@inproceedings{Pedregosa2016,
  title={Hyperparameter optimization with approximate gradient},
  author={F. Pedregosa},
  booktitle={International conference on machine learning},
  pages={737--746},
  year={2016},
  organization={PMLR}
}


@article{bengio2000,
  title={Gradient-based optimization of hyperparameters},
  author={Y. Bengio},
  journal={Neural computation},
  volume={12},
  number={8},
  pages={1889--1900},
  year={2000},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@incollection{Bengio+chapter2007,
author = {Y. Bengio and Y. LeCun},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {G. E.Hinton and S. Osindero and Y. W. Teh},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={I. Goodfellow and A. Courville and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

% linear & nonlinear ICA
@article{Darmois1953AnalyseGD,
  title={Analyse g{\'e}n{\'e}rale des liaisons stochastiques: etude particuli{\`e}re de l'analyse factorielle lin{\'e}aire},
  journal={Revue de l’Institut International
de Statistique},
  author={G. Darmois},
  year={1953}
}

 (1953). "" Dokl. Akad. Nauk SSSR (N.S.) (89): 217—219 (in Russian).

@article{Skitovich1953,
  title={On a property of the normal distribution.},
  author={Skitivic, V. P.},
  journal={Izvestiya Akademii Nauk SSSR. Seriya
Matematicheskaya},
  year={1953}
}

@article{comon1992,
  title={Independent component analysis.},
  author={Comon, P.},
  journal={Higher-Order Statistics},
  year={1992}
}

@book{ICAbook,
author = {Hyvärinen, A. and Karhunen, J. and Oja, E.},
year = {2001},
title = {Independent Component Analysis},
publisher = {Wiley}
}

@INPROCEEDINGS{AmuseICA90,
  author={Tong, L. and Soon, V.C. and Huang, Y.F. and Liu, R.},
  booktitle={IEEE International Symposium on Circuits and Systems},
  title={AMUSE: a new blind identification algorithm},
  year={1990}}

@article{HYVARINEN1999429,
title = {Nonlinear independent component analysis: Existence and uniqueness results},
journal = {Neural Networks},
year = {1999},
author = {Hyvärinen, A. and Pajunen, P.}
}

@article{Pavan2018OnTD,
  title={On the Darmois-Skitovich Theorem and Spatial Independence in Blind Source Separation},
  author={F. R. M. Pavan and M. D. Miranda},
  journal={Journal of Communication and Information Systems},
  year={2018}
}

@inproceedings{TCL2016,
 author = {Hyvärinen, A. and Morioka, H.},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA},
 year = {2016}
}

@InProceedings{PCL17,
  title = 	 {{Nonlinear ICA of Temporally Dependent Stationary Sources}},
  author = 	 {Hyvärinen, A. and Morioka, H.},
  booktitle = 	 {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  year = 	 {2017}
}

@inproceedings{HyvarinenST19,
  author = {Hyvärinen, A. and Sasaki, H. and Turner, R. E.},
  booktitle = {AISTATS},
  publisher = {PMLR},
  title = {Nonlinear ICA Using Auxiliary Variables and Generalized Contrastive Learning},
  year = 2019
}

@article{Gutmann12JMLR,
 author = {Gutmann, M. U. and Hyvärinen, A.},
 journal = {The Journal of Machine Learning Research},
 title = {Noise-contrastive estimation of unnormalized statistical models, with
applications to natural image statistics.},
 year = {2012}
}

@inproceedings{iVAEkhemakhem20a,
  title = 	 {Variational Autoencoders and Nonlinear ICA: A Unifying Framework},
  author =       {Khemakhem, I. and Kingma, D. and Monti, R. and Hyvärinen, A.},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  year = 	 {2020}
}

@inproceedings{Sorrenson2020Disentanglement,
title={Disentanglement by Nonlinear ICA with General Incompressible-flow Networks (GIN)},
author={Sorrenson, P. and Rother, C. and Köthe, U.},
booktitle={International Conference on Learning Representations},
year={2020}
}

@inproceedings{ice-beem20,
 author = {Khemakhem, I. and Monti, R. and Kingma, D. and Hyvärinen, A.},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {ICE-BeeM: Identifiable Conditional Energy-Based Deep Models Based on Nonlinear ICA},
 year = {2020}
}

% Disentanglement in DL


@article{oord2018representation,
 author = {Oord, A. and Li, Y. and Vinyals, O.},
 journal = {Advances in Neural Information Processing Systems},
 title = {Representation learning with contrastive predictive coding},
 year = {2018}
}


@inproceedings{chen2021exploring,
  title={Exploring simple siamese representation learning},
  author={Chen, Xinlei and He, Kaiming},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={15750--15758},
  year={2021}
}

@article{grill2020bootstrap,
  title={Bootstrap your own latent-a new approach to self-supervised learning},
  author={Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21271--21284},
  year={2020}
}

@inproceedings{chen2020simple,
 author = {Chen, T. and
Kornblith, S. and
Norouzi, M. and
Hinton, G. E.},
 booktitle = {Proceedings of the 37th International Conference on Machine Learning},
 title = {A Simple Framework for Contrastive Learning of Visual Representations},
 year = {2020}
}


@InProceedings{pmlr-v80-kim18b,
  title = 	 {Disentangling by Factorising},
  author =       {Kim, H. and Mnih, A.},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  year = 	 {2018}
}

@inproceedings{
kumar2018variational,
title={VARIATIONAL INFERENCE OF DISENTANGLED LATENT CONCEPTS FROM UNLABELED OBSERVATIONS},
author={A. Kumar and P. Sattigeri and A. Balakrishnan},
booktitle={International Conference on Learning Representations},
year={2018}
}

@InProceedings{roeder2020linear,
  title = 	 {On Linear Identifiability of Learned Representations},
  author =       {Roeder, G. and Metz, L. and Kingma, D. P.},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  year = 	 {2021}
}

@conference{GreRubMehLocSch19,
  title = {The Incomplete Rosetta Stone problem: Identifiability results for Multi-view Nonlinear {ICA}},
  author = {Gresele, L. and Rubenstein, P. K. and Mehrjou, A. and Locatello, F. and Sch{\"{o}}lkopf, B.},
  booktitle = {Proceedings of the 35th Conference on Uncertainty in Artificial Intelligence (UAI)},
  year = {2019},
}

@inproceedings{ahuja2023interventional,
  title={Interventional causal representation learning},
  author={Ahuja, Kartik and Mahajan, Divyat and Wang, Yixin and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={372--407},
  year={2023},
  organization={PMLR}
}

@inproceedings{vonkugelgen2021selfsupervised,
title={Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style},
author={Von K{\"u}gelgen, J. and Sharma, Y. and Gresele, L. and Brendel, W. and Sch{\"o}lkopf, B. and Besserve, M. and Locatello, F.},
booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
year={2021}
}

@inproceedings{tcvae,
author = {Chen, R. T. Q. and Li, X. and G., R. and Duvenaud, D.},
title = {Isolating Sources of Disentanglement in VAEs},
year = {2018},
booktitle = {Advances in Neural Information Processing Systems}
}

@InProceedings{pmlr-v97-locatello19a,
title = {Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations},
author = {Locatello, F. and Bauer, S. and Lucic, M. and Raetsch, G. and Gelly, S. and Sch{\"o}lkopf, B. and Bachem, O.},
booktitle = {Proceedings of the 36th International Conference on Machine Learning},
year = {2019}
}

@inproceedings{
Locatello2020Disentangling,
title={Disentangling Factors of Variations Using Few Labels},
author={F. Locatello and M. Tschannen and S. Bauer and G. Rätsch and B. Schölkopf and O. Bachem},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SygagpEKwB}
}

@article{Bouchacourt_Tomioka_Nowozin_2018, title={Multi-Level Variational Autoencoder: Learning Disentangled Representations From Grouped Observations},
journal={Proceedings of the AAAI Conference on Artificial Intelligence},
author={Bouchacourt, D. and Tomioka, R. and Nowozin, S.},
year={2018}
}

@ARTICLE{TalebJutten1999,
  author={Taleb, A. and Jutten, C.},
  journal={IEEE Transactions on Signal Processing},
  title={Source separation in post-nonlinear mixtures},
  year={1999}
  }

@InProceedings{pmlr-v119-locatello20a,
  title = 	 {Weakly-Supervised Disentanglement Without Compromises},
  author =       {Locatello, F. and Poole, B. and Raetsch, G. and Sch{\"o}lkopf, B. and Bachem, O. and Tschannen, M.},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  year = {2020}
}

@InProceedings{zimmermann2021cl,
  title = 	 {Contrastive Learning Inverts the Data Generating Process},
  author =       {Zimmermann, R. S. and Sharma, Y. and Schneider, S. and Bethge, M. and Brendel, W.},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  year = 	 {2021}
}

@InProceedings{Yang_2021_CVPR,
    author    = {Yang, M. and Liu, F. and Chen, Z. and Shen, X. and Hao, J. and Wang, J.},
    title     = {{CausalVAE}: Disentangled Representation Learning via Neural Structural Causal Models},
    booktitle = {Proceedings of the {IEEE/CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
    year      = {2021}
}

@misc{shen2021disentangled,
      title={Disentangled Generative Causal Representation Learning},
      author={Shen, X. and Liu, F. and Dong, H. and Lian, Q. and Chen, Z. and Zhang, T.},
      year={2021},
      journal={arXiv preprint arXiv:2010.02637}
}

@inproceedings{
kocaoglu2018causalgan,
title={Causal{GAN}: Learning Causal Implicit Generative Models with Adversarial Training},
author={Kocaoglu, M. and Snyder, C. and Dimakis, A. G. and Vishwanath, S.},
booktitle={International Conference on Learning Representations},
year={2018}
}

@misc{nair2019causal,
      title={Causal Induction from Visual Observations for Goal Directed Tasks},
      author={Nair, S. and Zhu, Y. and Savarese, S. and Fei-Fei, L.},
      year={2019},
      journal={arXiv preprint arXiv:1910.01751}
}

@article{peters2015causal,
author = {Peters, J. and Bühlmann, P. and Meinshausen, N.},
title = {Causal inference by using invariant prediction: identification and confidence intervals},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
year = {2016}
}

@misc{arjovsky2020invariant,
      title={Invariant Risk Minimization},
      author={Arjovsky, M. and Bottou, L. and Gulrajani, I. and Lopez-Paz, D.},
      year={2020},
      journal={arXiv preprint arXiv:1907.02893}
}

@inproceedings{IRMgames,
author = {Ahuja, K. and Shanmugam, K. and Varshney, K. R. and Dhurandhar, A.},
title = {Invariant Risk Minimization Games},
year = {2020},
booktitle = {Proceedings of the 37th International Conference on Machine Learning}
}

@misc{
krueger2021outofdistribution,
title={Out-of-Distribution Generalization via Risk Extrapolation ({\{}RE{\}}x)},
author={D. Krueger and E. Caballero and J.-H. Jacobsen and A. Zhang and J. Binas and R. Le Priol and D. Zhang and A. Courville},
year={2021}
}


@misc{lu2021nonlinear,
      title={Nonlinear Invariant Risk Minimization: A Causal Approach},
      author={Chaochao Lu and Yuhuai Wu and Jośe Miguel Hernández-Lobato and Bernhard Schölkopf},
      year={2021},
      journal={arXiv preprint arXiv:2102.12353}
}

@inproceedings{
ahuja2022towards,
title={Towards efficient representation identification in supervised learning},
author={K. Ahuja and D. Mahajan and V. Syrgkanis and I. Mitliagkas},
booktitle={First Conference on Causal Learning and Reasoning},
year={2022}
}

@inproceedings{gulrajani2020search,
title={In Search of Lost Domain Generalization},
author={Gulrajani, I. and Lopez-Paz, D.},
booktitle={International Conference on Learning Representations},
year={2021}
}

@inproceedings{slowVAE,
  author    = {Klindt, D. A. and
               Schott, L. and
               Sharma, Y  and
               Ustyuzhaninov, I and
               Brendel, W. and
               Bethge, M. and
               Paiton, D. M.},
  title     = {Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse
               Coding},
  booktitle = {9th International Conference on Learning Representations},
  year      = {2021}
}

@inproceedings{
Duan2020UDR,
title={Unsupervised Model Selection for Variational Disentangled Representation Learning},
author={Duan, S. and Matthey, L. and Saraiva, A. and Watters, N. and Burgess, C. and Lerchner, A. and Higgins, I.},
booktitle={International Conference on Learning Representations},
year={2020}
}

@article{consciousBengio,
  author    = {Bengio, Y.},
  title     = {The Consciousness Prior},
  year      = {2017},
  journal={arXiv preprint arXiv:1709.08568}
}

% Causal Feature Learning
@article{Chalupka2017CausalFL,
  title={Causal feature learning: an overview},
  author={Chalupka, K. and Eberhardt, F. and Perona, P.},
  journal={Behaviormetrika},
  year={2017}
}


% Causal discovery
@article{ng2019masked,
  title={Masked Gradient-Based Causal Structure Learning},
  author={Ng, I. and Fang, Z. and Zhu, S. and Chen, Z. and Wang, J.},
  journal={arXiv preprint arXiv:1910.08527},
  year={2019}
}


@inproceedings{
Bengio2020A,
title={A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms},
author={Bengio, Y. and Deleu, T. and Rahaman, N. and Ke, N. R. and Lachapelle, S. and Bilaniuk, O. and Goyal, A. and Pal, C.},
booktitle={International Conference on Learning Representations},
year={2020}
}

@inproceedings{dcdi,
 author = {Brouillard, P. and Lachapelle, S. and Lacoste, A. and Lacoste-Julien, S. and Drouin, A.},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Differentiable Causal Discovery from Interventional Data},
 year = {2020}
}

@article{ut_igsp,
  title={Permutation-Based Causal Structure Learning with Unknown Intervention Targets},
  author={Squires, C. and Wang, Y. and Uhler, C.},
  journal={Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence},
  year={2020}
}

@article{JCI_jmlr,
  author  = {Mooij, J. M. and Magliacane, S. and Claassen, T.},
  title   = {Joint Causal Inference from Multiple Contexts},
  journal = {Journal of Machine Learning Research},
  year    = {2020}
}

@inproceedings{eaton2007exact,
  title={Exact Bayesian structure learning from uncertain interventions},
  author={Eaton, D. and Murphy, K.},
  booktitle={Artificial Intelligence and Statistics},
  year={2007}
}

@incollection{NIPS2019_9581,
title = {Characterization and Learning of Causal Graphs with Latent Variables from Soft Interventions},
author = {Kocaoglu, M. and Jaber, A. and Shanmugam, K. and Bareinboim, E.},
booktitle = {Advances in Neural Information Processing Systems 32},
year = {2019}
}

@inproceedings{NEURIPS2020_6cd9313e,
 author = {Jaber, A. and Kocaoglu, M. and Shanmugam, K. and Bareinboim, E.},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Causal Discovery from Soft Interventions with Unknown Targets: Characterization and Learning},
 year = {2020}
}

% causality
@book{pearl2009causality,
  title={Causality},
  author={Pearl, J.},
  year={2009},
  publisher={Cambridge university press}
}

@book{peters2017elements,
  title = {Elements of Causal Inference - Foundations and Learning Algorithms},
  author = {Peters, J. and Janzing, D. and Sch{\"o}lkopf, B.},
  publisher = {MIT Press},
  year = {2017}
}

@article{Pearl2018TheSP,
 title = {The Seven Tools of Causal Inference, with Reflections on Machine Learning},
 author = {Pearl, J.},
 journal = {Commun. ACM},
 year = {2019}
}

% Causality + disentanglement
@misc{scholkopf2019causality,
      title={Causality for Machine Learning},
      author={Schölkopf, B.},
      year={2019},
      journal={arXiv preprint arXiv:1911.10500}
}

@article{scholkopf2021causal,
  title = {Toward Causal Representation Learning},
  author = {Sch{\"o}lkopf, B. and Locatello, F. and Bauer, S. and Ke, N. R. and Kalchbrenner, N. and Goyal, A. and Bengio, Y.},
  journal = {Proceedings of the IEEE - Advances in Machine Learning and Deep Neural Networks},
  year = {2021}
}

@article{InducGoyal2021,
  author    = {Goyal, A. and
               Bengio, Y.},
  title     = {Inductive Biases for Deep Learning of Higher-Level Cognition},
  journal    = {Proc. R. Soc. A 478: 20210068},
  year      = {2022}
}

@misc{CITRIS,
  author = {Lippe, P. and Magliacane, S. and Löwe, S. and Asano, Y. M. and Cohen, T. and Gavves, E.},
  title = {{CITRIS}: Causal Identifiability from Temporal Intervened Sequences},
  year = {2022},
  journal={arXiv preprint arXiv:2202.03169}
}

@inproceedings{
lippe2022icitris,
title={i{CITRIS}: Causal Representation Learning for Instantaneous Temporal Effects},
author={P. Lippe and S. Magliacane and S. L{\"o}we and Y. M Asano and T. Cohen and E. Gavves},
booktitle={UAI 2022 Workshop on Causal Representation Learning},
year={2022}
}


% measure theory
@book{pollard_2001,
title={A User's Guide to Measure Theoretic Probability},
publisher={Cambridge University Press},
author={Pollard, D.},
year={2001}
}

% Variational autoencoders
@inproceedings{Kingma2014,
  booktitle = {2nd International Conference on Learning Representations},
  author = {Kingma, D. P. and Welling, M.},
  title = {Auto-Encoding Variational Bayes},
  year = 2014
}

@inproceedings{Higgins2017betaVAELB,
  title={beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework},
  author={Higgins, I. and Matthey, L. and Pal, A. and Burgess, C. P. and Glorot, X. and Botvinick, M. and Mohamed, S. and Lerchner, A.},
  booktitle={ICLR},
  year={2017}
}

% Optimization
@inproceedings{Adam,
  author    = {Kingma, D. P. and
               Ba, J.},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations},
  year      = {2015}
}

% Gumbel softmax
@article{jang2016categorical,
  title={Categorical Reparameterization with Gumbel-Softmax},
  author={Jang, E. and Gu, S. and Poole, B.},
  journal={Proceedings of the 34th International Conference on Machine Learning},
  year={2017}
}

@article{maddison2016concrete,
  title={The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables},
  author={Maddison, C. J. and Mnih, A. and Teh, Y. W.},
  journal={Proceedings of the 34th International Conference on Machine Learning},
  year={2017}
}

% NN stuff
@InProceedings{pmlr-v9-glorot10a,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, X. and Bengio, Y.},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  year = 	 {2010}
}

% Graphical models
@article{WainwrightJordan08,
author = {Wainwright, M. J. and Jordan, M. I.},
title = {Graphical Models, Exponential Families, and Variational Inference},
year = {2008},
journal = {Found. Trends Mach. Learn.}
}

@article{ke2021systematic,
  title={Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning},
  author={Ke, Nan Rosemary and Didolkar, Aniket Rajiv and Mittal, Sarthak and Goyal, Anirudh and Lajoie, Guillaume and Bauer, Stefan and Rezende, Danilo Jimenez and Mozer, Michael Curtis and Bengio, Yoshua and Pal, Christopher},
  year={2021}
}

@article{goyal2019recurrent,
  title={Recurrent independent mechanisms},
  author={A. Goyal and A. Lamb and J. Hoffmann and S. Sodhani and S. Levine and Y. Bengio and B. Sch{\"o}lkopf},
  journal={arXiv preprint arXiv:1909.10893},
  year={2019}
}

% Deep Learning
@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Goodfellow, I and Bengio, Y. and Courville, A.},
    publisher={MIT Press},
    year={2016}
}

@InProceedings{batchnorm2015,
  title = 	 {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author = 	 {Ioffe, S. and Szegedy, C.},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  year = 	 {2015}
}



@article{Bohning1992,
  title={Multinomial logistic regression algorithm},
  author={B{\"o}hning, D},
  journal={Annals of the institute of Statistical Mathematics},
  volume={44},
  number={1},
  pages={197--200},
  year={1992},
  publisher={Springer}
}

@inproceedings{Lee_Maji_Ravichandran_Soatto2019meta,
  title={Meta-learning with differentiable convex optimization},
  author={K. Lee and S.Maji and A. Ravichandran and S. Soatto},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10657--10665},
  year={2019}
}

@article{Crammer_Singer2001,
  title={On the algorithmic implementation of multiclass kernel-based vector machines},
  author={K. Crammer and Y. Singer},
  journal={Journal of machine learning research},
  volume={2},
  number={Dec},
  pages={265--292},
  year={2001}
}

@article{Tseng2001,
  title={Convergence of a block coordinate descent method for nondifferentiable minimization},
  author={P. Tseng},
  journal={Journal of optimization theory and applications},
  volume={109},
  number={3},
  pages={475--494},
  year={2001},
  publisher={Springer}
}

@inproceedings{Bertrand_Massias2021,
  title={Anderson acceleration of coordinate descent},
  author={Q. Bertrand and M. Massias},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1288--1296},
  year={2021},
  organization={PMLR}
}

@article{jaxopt,
  title={Efficient and modular implicit differentiation},
  author={M. Blondel and Q. Berthet and M. Cuturi and R. Frostig and S. Hoyer and F. Llinares-L{\'o}pez and F. Pedregosa and J.-P. Vert},
  journal={NeurIPS},
  year={2022}
}

@software{jax2018github,
  author = {J. Bradbury and R. Frostig and P. Hawkins and M. James Johnson and C. Leary and D. Maclaurin and G. Necula and A. Paszke and J. Vander{P}las and S. Wanderman-{M}ilne and Q. Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  year = {2018},
}

@inproceedings{Tiomoko_Couillet2019,
  title={M. Tiomoko and R. Couillet and F. Bouchard and G. Ginolhac},
  booktitle={ICML},
  pages={6254--6263},
  year={2019},
  organization={PMLR}
}

@inproceedings{
lachapelle2022disentanglement,
title={Disentanglement via Mechanism Sparsity Regularization: A New Principle for Nonlinear {ICA}},
author={Lachapelle, S. and Rodriguez Lopez, P. and Sharma, Y. and Everett, K. E. and {Le Priol}, R. and Lacoste, A. and Lacoste-Julien, S.},
booktitle={First Conference on Causal Learning and Reasoning},
year={2022}
}

@inproceedings{
lachapelle2022partial,
title={Partial Disentanglement via Mechanism Sparsity},
author={S. Lachapelle and S. Lacoste-Julien},
booktitle={UAI 2022 Workshop on Causal Representation Learning},
year={2022}
}

@inproceedings{
zheng2022on,
title={On the Identifiability of Nonlinear {ICA}: Sparsity and Beyond},
author={Y. Zheng and I. Ng and K. Zhang},
booktitle={Advances in Neural Information Processing Systems},
year={2022}
}

@article{
moran2022identifiable,
title={Identifiable Deep Generative Models via Sparse Decoding},
author={G. E. Moran and D. Sridhar and Y. Wang and D. Blei},
journal={Transactions on Machine Learning Research},
year={2022}
}



@misc{ahuja2022sparse,
  author = {Ahuja, K. and Hartford, J. and Bengio, Y.},
  title = {Weakly Supervised Representation Learning with Sparse Perturbations},
  journal={arXiv preprint arXiv:2206.01101},
  year = {2022}
}

@inproceedings{ahuja2022properties,
  title = {Properties from mechanisms: an equivariance perspective on identifiable representation learning},
  author = {Ahuja, K. and Hartford, J. and Bengio, Y.},
  booktitle = {International Conference on Learning Representations},
  year = {2022}
}


@inproceedings{bertinetto2018r2d2,
  title={Meta-learning with differentiable closed-form solvers},
  author={L. Bertinetto and J. F. Henriques and P. HS Torr and Vedaldi, Andrea},
  journal={International Conference on Learning Representations},
  year={2019}
}

@article{vinyals2016matchingnet,
  title={Matching networks for one shot learning},
  author={Vinyals, O. and Blundell, C. and Lillicrap, T. and Wierstra, D. and others},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{bengio2000implicitdiff,
  title={Gradient-based optimization of hyperparameters},
  author={Y. Bengio},
  journal={Neural computation},
  year={2000},
  publisher={MIT Press}
}

@inproceedings{lorraine2020million,
  title={Optimizing millions of hyperparameters by implicit differentiation},
  author={J. Lorraine and P. Vicol and D. Duvenaud},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1540--1552},
  year={2020},
  organization={PMLR}
}

@article{rajeswaran2019imaml,
  title={Meta-learning with implicit gradients},
  author={A. Rajeswaran and C. Finn and S. M. Kakade and S. Levine},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{snell2017protonet,
  title={Prototypical networks for few-shot learning},
  author={J. Snell and K. Swersky and R. Zemel},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{hospedales2021metalearningsurvey,
  title={Meta-learning in neural networks: A survey},
  author={Hospedales, T. and Antoniou, A. and Micaelli, P. and Storkey, A.},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={44},
  number={9},
  pages={5149--5169},
  year={2021},
  publisher={IEEE}
}

@article{brown2020gpt3,
  title={Language models are few-shot learners},
  author={Brown, T. and Mann, B. and Ryder, N. and Subbiah, M. and Kaplan, J. D and Dhariwal, P. and Neelakantan, A. and Shyam, P. and Sastry, G. and Askell, A. and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{dosovitskiy2020vit,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, A. and Beyer, L. and Kolesnikov, A. and Weissenborn, D. and Zhai, X. and Unterthiner, T. and Dehghani, M. and Minderer, M. and Heigold, G. and Gelly, S. and others},
  journal={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{chen2020simclr,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, T. and Kornblith, S. and Norouzi, M. and Hinton, G.},
  booktitle={International Conference on Machine Learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@inproceedings{radford2021clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, A. and Kim, J. W. and Hallacy, C. and Ramesh, A. and Goh, G. and Agarwal, S. and Sastry, G. and Askell, A. and Mishkin, P. and Clark, J. and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{ramesh2022dalle2,
  author = {Ramesh, A. and Dhariwal, P. and Nichol, A. and Chu, C. and Chen, M.},
  title = {Hierarchical Text-Conditional Image Generation with CLIP Latents},
  journal={arXiv preprint arXiv:2204.06125},
  year = {2022}
}


@book{pearl1988probabilistic,
	title        = {{Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference}},
	author       = {Pearl, Judea},
	year         = 1988,
	publisher    = {Morgan Kaufmann Publishers Inc.},
	address      = {San Francisco, CA, USA},
	isbn         = {0934613737}
}
@book{pearl2009causality,
	title        = {{Causality: Models, Reasoning and Inference}},
	author       = {Pearl, Judea},
	year         = 2009,
	publisher    = {Cambridge University Press},
	address      = {USA},
	isbn         = {052189560X},
	edition      = {2nd}
}
@article{peters2015structural,
	title        = {{Structural Intervention Distance (SID) for Evaluating Causal Graphs}},
	author       = {Peters, Jonas and B{\"u}hlmann, Peters},
	year         = 2015,
	journal      = {Neural Computation},
	volume       = 27,
	number       = 3
}
@article{peters2016causal,
	title        = {{Causal inference by using invariant prediction: identification and confidence intervals}},
	author       = {Peters, Jonas and B{\"u}hlmann, Peter and Meinshausen, Nicolai},
	year         = 2016,
	journal      = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
	publisher    = {JSTOR}
}
@book{peters2017elements,
	title        = {{Elements of Causal Inference: Foundations and Learning Algorithms}},
	author       = {Peters, Jonas and Janzing, Dominik and Schlkopf, Bernhard},
	year         = 2017,
	publisher    = {The MIT Press},
	isbn         = {0262037319},
	
}


@InProceedings{pmlr-v115-tonolini20a,
  title = 	 {Variational Sparse Coding},
  author =       {Tonolini, Francesco and Jensen, Bj{\o}rn Sand and Murray-Smith, Roderick},
  booktitle = 	 {Proceedings of The 35th Uncertainty in Artificial Intelligence Conference},
  pages = 	 {690--700},
  year = 	 {2020},
  editor = 	 {Adams, Ryan P. and Gogate, Vibhav},
  volume = 	 {115},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {22--25 Jul},
  publisher =    {PMLR},
 
}

@book{Spirtes_2000,
	title        = {{Causation, Prediction, and Search}},
	author       = {Spirtes, Peter and Glymour, Clark and Scheines, Richard},
	year         = 2000,
	publisher    = {MIT Press},
	address      = {Cambridge MA},
	added-at     = {2016-11-26T13:19:29.000+0100},
	date-added   = {2008-05-16 16:46:46 -0700},
	date-modified = {2008-05-16 16:48:00 -0700},
	edition      = {2nd},
	interhash    = {559e17fcd12a76214629ba6c4efe3f9a},
	intrahash    = {e2b107e8fd3469c8b0e944ca37a559f3},
	keywords     = {imported ml}
}


@article{kivva2022identifiability,
  title={Identifiability of deep generative models without auxiliary information},
  author={Kivva, Bohdan and Rajendran, Goutham and Ravikumar, Pradeep and Aragam, Bryon},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15687--15701},
  year={2022}
}


@article{teicher1963identifiability,
  title={Identifiability of finite mixtures},
  author={Teicher, Henry},
  journal={The annals of Mathematical statistics},
  pages={1265--1269},
  year={1963},
  publisher={JSTOR}
}

@article{yakowitz1968identifiability,
  title={On the identifiability of finite mixtures},
  author={Yakowitz, Sidney J and Spragins, John D},
  journal={The Annals of Mathematical Statistics},
  volume={39},
  number={1},
  pages={209--214},
  year={1968},
  publisher={Institute of Mathematical Statistics}
}

@book{devlin2012joy,
  title={The joy of sets: fundamentals of contemporary set theory},
  author={Devlin, Keith},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{tarieladze2007disintegration,
  title={Disintegration of Gaussian measures and average-case optimal algorithms},
  author={Tarieladze, Vaja and Vakhania, Nicholas},
  journal={Journal of Complexity},
  volume={23},
  number={4-6},
  pages={851--866},
  year={2007},
  publisher={Elsevier}
}



@article{scholkopf2021toward,
  title={Toward causal representation learning},
  author={Sch{\"o}lkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  journal={Proceedings of the IEEE},
  volume={109},
  number={5},
  pages={612--634},
  year={2021},
  publisher={IEEE}
}



@inproceedings{lachapelle2023synergies,
  title={Synergies between disentanglement and sparsity: Generalization and identifiability in multi-task learning},
  author={Lachapelle, S{\'e}bastien and Deleu, Tristan and Mahajan, Divyat and Mitliagkas, Ioannis and Bengio, Yoshua and Lacoste-Julien, Simon and Bertrand, Quentin},
  booktitle={International Conference on Machine Learning},
  pages={18171--18206},
  year={2023},
  organization={PMLR}
}



@inproceedings{
oublal2024disentangling,
title={Disentangling Time Series Representations via Contrastive Independence-of-Support on l-Variational Inference},
author={Khalid Oublal and Said Ladjal and David Benhaiem and Emmanuel LE BORGNE and Fran{\c{c}}ois Roueff},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=iI7hZSczxE}
}

@article{tschannen2018recent,
  title={Recent advances in autoencoder-based representation learning},
  author={Tschannen, Michael and Bachem, Olivier and Lucic, Mario},
  journal={arXiv preprint arXiv:1812.05069},
  year={2018}
}

@inproceedings{Hyvrinen2017NonlinearIO,
 author = {Aapo Hyv{\"{a}}rinen and
Hiroshi Morioka},
 booktitle = {{AISTATS}},
 pages = {460--469},
 series = {Proceedings of Machine Learning Research},
 title = {Nonlinear {ICA} of Temporally Dependent Stationary Sources},
 volume = {54},
 year = {2017}
}

@inproceedings{klindt2020towards,
 author = {David A. Klindt and
Lukas Schott and
Yash Sharma and
Ivan Ustyuzhaninov and
Wieland Brendel and
Matthias Bethge and
Dylan M. Paiton},
 booktitle = {{ICLR}},
 title = {Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse
Coding},
 year = {2021}
}


@inproceedings{zimmermann2021contrastive,
 author = {Roland S. Zimmermann and
Yash Sharma and
Steffen Schneider and
Matthias Bethge and
Wieland Brendel},
 booktitle = {{ICML}},
 pages = {12979--12990},
 series = {Proceedings of Machine Learning Research},
 title = {Contrastive Learning Inverts the Data Generating Process},
 volume = {139},
 year = {2021}
}


@inproceedings{halva2020hidden,
 author = {Hermanni H{\"{a}}lv{\"{a}} and
Aapo Hyv{\"{a}}rinen},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/uai/HalvaH20.bib},
 booktitle = {Proceedings of the Thirty-Sixth Conference on Uncertainty in Artificial
Intelligence, {UAI} 2020, virtual online, August 3-6, 2020},
 pages = {939--948},
 series = {Proceedings of Machine Learning Research},
 title = {Hidden Markov Nonlinear {ICA:} Unsupervised Learning from Nonstationary
Time Series},
 volume = {124},
 year = {2020}
}


@inproceedings{hyvarinen2019nonlinear,
  title={Nonlinear ICA using auxiliary variables and generalized contrastive learning},
  author={Hyvarinen, Aapo and Sasaki, Hiroaki and Turner, Richard},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={859--868},
  year={2019},
  organization={PMLR}
}



@article{hyvarinen2016unsupervised,
  title={Unsupervised feature extraction by time-contrastive learning and nonlinear ica},
  author={Hyvarinen, Aapo and Morioka, Hiroshi},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{locatello2020weakly,
  title={Weakly-supervised disentanglement without compromises},
  author={Locatello, Francesco and Poole, Ben and R{\"a}tsch, Gunnar and Sch{\"o}lkopf, Bernhard and Bachem, Olivier and Tschannen, Michael},
  booktitle={International Conference on Machine Learning},
  pages={6348--6359},
  year={2020},
  organization={PMLR}
}

@article{gresele2021independent,
  title={Independent mechanism analysis, a new concept?},
  author={Gresele, Luigi and von K{\"u}gelgen, Julius and Stimper, Vincent and Sch{\"o}lkopf, Bernhard and Besserve, Michel},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={28233--28248},
  year={2021}
}








@inproceedings{sparseCodingForMTL2013,
author = {Maurer, A. and Pontil, M. and Romera-Paredes, B.},
title = {Sparse Coding for Multitask and Transfer Learning},
year = {2013},
series = {ICML'13}
}



@article{marcus2022preliminary,
  author = {Marcus, G. and Davis, E. and Aaronson, S.},
  title = {A very preliminary analysis of DALL-E 2},
  journal={arXiv preprint arXiv:2204.13807},
  year = {2022}
}


@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, J. and Chang, M.-W. and Lee, K. and Toutanova, K.},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{andreassen2021robustness,
  title={The evolution of out-of-distribution robustness throughout fine-tuning},
  author={Andreassen, A. and Bahri, Y. and Neyshabur, B. and Roelofs, R.},
  journal={arXiv preprint arXiv:2106.15831},
  year={2021}
}

@InProceedings{wortsman2022wiseft,
    author    = {Wortsman, M. and Ilharco, G. and Kim, J. W. and Li, M. and Kornblith, S. and Roelofs, R. and Lopes, R. G. and Hajishirzi, H. and Farhadi, A. and Namkoong, H. and Schmidt, L.},
    title     = {Robust Fine-Tuning of Zero-Shot Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {7959-7971}
}

% Usefulness of disentanglement
@article{miladinovic2019disentangledODE,
  author = {Miladinović, D. and Gondal, M. W. and Schölkopf, B. and Buhmann, J. M. and Bauer, S.},
  title = {Disentangled State Space Representations},
  journal = {arXiv preprint arXiv:1906.03255},
  year = {2019}
}

@inproceedings{steenkiste2019DisForAbstractReasoning,
 author = {van Steenkiste, S. and Locatello, F. and Schmidhuber, J. and Bachem, O.},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Are Disentangled Representations Helpful for Abstract Visual Reasoning?},
 year = {2019}
}

@inproceedings{
dittadi2021sim2real_dis,
title={On the Transfer of Disentangled Representations in Realistic Settings},
author={A. Dittadi and F. Tr{\"a}uble and F. Locatello and M. Wuthrich and V. Agrawal and O. Winther and S. Bauer and B. Sch{\"o}lkopf},
booktitle={International Conference on Learning Representations},
year={2021}
}

@inproceedings{
montero2021disGen,
title={The role of Disentanglement in Generalisation},
author={M. L. Montero and C. JH Ludwig and R. P. Costa and G. Malhotra and J. Bowers},
booktitle={International Conference on Learning Representations},
year={2021}
}

@conference{Zhangetal22b,
  title = {Towards Principled Disentanglement for Domain Generalization},
  author = {Zhang, H. and Zhang, Y.-F. and Liu, W. and Weller, A. and Sch{\"o}lkopf, B. and Xing, E.},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2022}
}

% Generalization bounds + statistical machine learning
@book{MohriRostamizadehTalwalkar18,
  author = {Mohri, M. and Rostamizadeh, A. and Talwalkar, A.},
  publisher = {MIT Press},
  year = 2018
}


% dataset

@misc{3dshapes18,
  title={3D Shapes Dataset},
  author={Burgess, Chris and Kim, Hyunjik},
  howpublished={https://github.com/deepmind/3dshapes-dataset/},
  year={2018}
}

@inproceedings{mikolov2010recurrent,
  added-at = {2015-04-09T00:00:00.000+0200},
  author = {Mikolov, T. and Karafiát, M. and Burget, L. and Cernocký, J. and Khudanpur, S.},
  publisher = {ISCA},
  title = {Recurrent neural network based language model},
  year = 2010
}


@book{CaseBerg2001,
  author = {Casella, G. and Berger, R.},
  publisher = {{Duxbury Resource Center}},
  title = {Statistical Inference},
  year = 2001
}

@book{Boyd_Vandenberghe2004,
  title={Convex optimization},
  author={S. P. Boyd and and L. Vandenberghe},
  year={2004},
  publisher={Cambridge university press}
}

@article{Chang_Lin2011,
  title={LIBSVM: a library for support vector machines},
  author={C.-C. Chang and C.-L. Lin},
  journal={ACM transactions on intelligent systems and technology (TIST)},
  volume={2},
  number={3},
  pages={1--27},
  year={2011},
  publisher={Acm New York, NY, USA}
}

@article{Chen_Lin_Scholkopf2005,
  title={A tutorial on $\nu$-support vector machines},
  author={P.-H. Chen and C.-J. Lin and B. Sch{\"o}lkopf},
  journal={Applied Stochastic Models in Business and Industry},
  volume={21},
  number={2},
  pages={111--136},
  year={2005},
  publisher={Wiley Online Library}
}
@article{Bottou_Lin2007,
  title={Support vector machine solvers},
  author={L. Bottou and C.-J. Lin},
  journal={Large scale kernel machines},
  volume={3},
  number={1},
  pages={301--320},
  year={2007},
  publisher={MIT press Cambridge, MA}
}

@inproceedings{Hsieh2008,
  title={A dual coordinate descent method for large-scale linear SVM},
  author={C.-J. Hsieh and K.-W. Chang and C.-J. Lin and S. S. Keerthi and S. Sundararajan},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={408--415},
  year={2008}
}

@article{Shalev_Zhang2012,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization},
  author={S. Shalev-Shwartz and T. Zhang},
  journal={The Journal of Machine Learning Research},
  year={2012}
}

@inproceedings{Finn_Abbel_Levine2017,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={C. Finn and P. Abbeel and S. Levine},
  booktitle={International conference on machine learning},
  pages={1126--1135},
  year={2017},
  organization={PMLR}
}


@article{scholkopf2021toward,
  title={Toward causal representation learning},
  author={Sch{\"o}lkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  journal={Proceedings of the IEEE},
  volume={109},
  number={5},
  pages={612--634},
  year={2021},
  publisher={IEEE}
}

@article{bengio2013representation,
  title={Representation learning: A review and new perspectives},
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={8},
  pages={1798--1828},
  year={2013},
  publisher={IEEE}
}

@inproceedings{locatello2019challenging,
  title={Challenging common assumptions in the unsupervised learning of disentangled representations},
  author={Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Raetsch, Gunnar and Gelly, Sylvain and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
  booktitle={international conference on machine learning},
  pages={4114--4124},
  year={2019},
  organization={PMLR}
}

@article{tschannen2018recent,
  title={Recent advances in autoencoder-based representation learning},
  author={Tschannen, Michael and Bachem, Olivier and Lucic, Mario},
  journal={arXiv preprint arXiv:1812.05069},
  year={2018}
}



@inproceedings{hyvarinen2017nonlinear,
  title={Nonlinear ICA of temporally dependent stationary sources},
  author={Hyvarinen, Aapo and Morioka, Hiroshi},
  booktitle={Artificial Intelligence and Statistics},
  pages={460--469},
  year={2017},
  organization={PMLR}
}



@inproceedings{khemakhem2020variational,
  title={Variational autoencoders and nonlinear ica: A unifying framework},
  author={Khemakhem, Ilyes and Kingma, Diederik and Monti, Ricardo and Hyvarinen, Aapo},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2207--2217},
  year={2020},
  organization={PMLR}
}

@inproceedings{lippe2022citris,
  title={Citris: Causal identifiability from temporal intervened sequences},
  author={Lippe, Phillip and Magliacane, Sara and L{\"o}we, Sindy and Asano, Yuki M and Cohen, Taco and Gavves, Stratis},
  booktitle={International Conference on Machine Learning},
  pages={13557--13603},
  year={2022},
  organization={PMLR}
}

@article{scholkopf2012causal,
  title={On causal and anticausal learning},
  author={Sch{\"o}lkopf, Bernhard and Janzing, Dominik and Peters, Jonas and Sgouritsa, Eleni and Zhang, Kun and Mooij, Joris},
  journal={arXiv preprint arXiv:1206.6471},
  year={2012}
}

@article{lippe2023biscuit,
  title={BISCUIT: Causal Representation Learning from Binary Interactions},
  author={Lippe, Phillip and Magliacane, Sara and L{\"o}we, Sindy and Asano, Yuki M and Cohen, Taco and Gavves, Efstratios},
  journal={Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
  year={2023}
}


@inproceedings{von2023nonparametric,
  title={Nonparametric Identifiability of Causal Representations from Unknown Interventions},
  author={von K{\"u}gelgen, Julius and Besserve, Michel and Liang, Wendong and Gresele, Luigi and Keki{\'c}, Armin and Bareinboim, Elias and Blei, David M and Sch{\"o}lkopf, Bernhard},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}

@inproceedings{Liang2023cca,
title={Causal Component Analysis},
author={Liang Wendong and Armin Keki\'c and Julius von K\"ugelgen and Simon Buchholz and Michel Besserve and Luigi Gresele and Bernhard Sch\"olkopf},
year={2023},
booktitle={Advances in Neural Information Processing Systems},
}


@article{perry2022causal,
  title={Causal discovery in heterogeneous environments under the sparse mechanism shift hypothesis},
  author={Perry, Ronan and Von K{\"u}gelgen, Julius and Sch{\"o}lkopf, Bernhard},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={10904--10917},
  year={2022}
}

@article{fumero2023leveraging,
  title={Leveraging sparse and shared feature activations for disentangled representation learning},
  author={Fumero, Marco and Wenzel, Florian and Zancato, Luca and Achille, Alessandro and Rodol{\`a}, Emanuele and Soatto, Stefano and Sch{\"o}lkopf, Bernhard and Locatello, Francesco},
  journal={arXiv preprint arXiv:2304.07939},
  year={2023}
}

@inproceedings{zhang2023identifiability,
title={Identifiability Guarantees for Causal Disentanglement from Soft Interventions},
author={Jiaqi Zhang and Kristjan Greenewald and Chandler Squires and Akash Srivastava and Karthikeyan Shanmugam and Caroline Uhler},
booktitle={Advances in Neural Information Processing Systems},
year={2023},
}

@inproceedings{ahuja2023multi,
  title={Multi-Domain Causal Representation Learning via Weak Distributional Invariances},
  author={Ahuja, Kartik and Mansouri, Amin and Wang, Yixin},
  booktitle={Causal Representation Learning Workshop at NeurIPS 2023},
  year={2023}
}

@inproceedings{squires2023linear,
  title={Linear Causal Disentanglement via Interventions}, 
  author={Chandler Squires and Anna Seigal and Salil Bhate and Caroline Uhler},
  year={2023},
  booktitle={40th International Conference on Machine Learning},
}

@inproceedings{buchholz2023learning,
  title={Learning Linear Causal Representations from Interventions under General Nonlinear Mixing},
  author={Buchholz, Simon and Rajendran, Goutham and Rosenfeld, Elan and Aragam, Bryon},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}

@inproceedings{ke2023learning,
  title={Learning Disentangled Causal Representations via Principal-Mechanism Analysis},
  author={Ke, Nan Rosemary and Bengio, Yoshua and Goyal, Anirudh},
  booktitle={Conference on Causal Learning and Reasoning},
  pages={368--396},
  year={2023},
  organization={PMLR}
}






%%%% CORRECTED VERSION SEE ABOVE
@inproceedings{kim_unsupervised_2011,
	title = {Unsupervised {Disaggregation} of {Low} {Frequency} {Power} {Measurements}},
	isbn = {978-0-89871-992-5 978-1-61197-281-8},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611972818.64},
	doi = {10.1137/1.9781611972818.64},
	language = {en},
	urldate = {2023-11-15},
	booktitle = {Proceedings of the 2011 {SIAM} {International} {Conference} on {Data} {Mining}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Kim, Hyungsul and Marwah, Manish and Arlitt, Martin and Lyon, Geoff and Han, Jiawei},
	month = apr,
	year = {2011},
	pages = {747--758},
}

@misc{noauthor_unsupervised_nodate,
	title = {Unsupervised {Disaggregation} of {Low} {Frequency} {Power} {Measurements}},
	url = {https://epubs.siam.org/doi/epdf/10.1137/1.9781611972818.64},
	language = {en},
	urldate = {2023-11-14},
	note = {ISBN: 9781611972818},
}

@inproceedings{trauble_disentangled_2021,
	title = {On {Disentangled} {Representations} {Learned} from {Correlated} {Data}},
	url = {https://proceedings.mlr.press/v139/trauble21a.html},
	abstract = {The focus of disentanglement approaches has been on identifying independent factors of variation in data. However, the causal variables underlying real-world observations are often not statistically independent. In this work, we bridge the gap to real-world scenarios by analyzing the behavior of the most prominent disentanglement approaches on correlated data in a large-scale empirical study (including 4260 models). We show and quantify that systematically induced correlations in the dataset are being learned and reflected in the latent representations, which has implications for downstream applications of disentanglement such as fairness. We also demonstrate how to resolve these latent correlations, either using weak supervision during training or by post-hoc correcting a pre-trained model with a small number of labels.},
	language = {en},
	urldate = {2023-11-13},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Träuble, Frederik and Creager, Elliot and Kilbertus, Niki and Locatello, Francesco and Dittadi, Andrea and Goyal, Anirudh and Schölkopf, Bernhard and Bauer, Stefan},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {10401--10412},
}

@misc{xia_causal-neural_2022,
	title = {The {Causal}-{Neural} {Connection}: {Expressiveness}, {Learnability}, and {Inference}},
	shorttitle = {The {Causal}-{Neural} {Connection}},
	url = {http://arxiv.org/abs/2107.00793},
	abstract = {One of the central elements of any causal inference is an object called structural causal model (SCM), which represents a collection of mechanisms and exogenous sources of random variation of the system under investigation (Pearl, 2000). An important property of many kinds of neural networks is universal approximability: the ability to approximate any function to arbitrary precision. Given this property, one may be tempted to surmise that a collection of neural nets is capable of learning any SCM by training on data generated by that SCM. In this paper, we show this is not the case by disentangling the notions of expressivity and learnability. Specifically, we show that the causal hierarchy theorem (Thm. 1, Bareinboim et al., 2020), which describes the limits of what can be learned from data, still holds for neural models. For instance, an arbitrarily complex and expressive neural net is unable to predict the effects of interventions given observational data alone. Given this result, we introduce a special type of SCM called a neural causal model (NCM), and formalize a new type of inductive bias to encode structural constraints necessary for performing causal inferences. Building on this new class of models, we focus on solving two canonical tasks found in the literature known as causal identification and estimation. Leveraging the neural toolbox, we develop an algorithm that is both sufficient and necessary to determine whether a causal effect can be learned from data (i.e., causal identifiability); it then estimates the effect whenever identifiability holds (causal estimation). Simulations corroborate the proposed approach.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Xia, Kevin and Lee, Kai-Zhan and Bengio, Yoshua and Bareinboim, Elias},
	month = oct,
	year = {2022},
	note = {arXiv:2107.00793 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{wang_desiderata_2022,
	title = {Desiderata for {Representation} {Learning}: {A} {Causal} {Perspective}},
	shorttitle = {Desiderata for {Representation} {Learning}},
	url = {http://arxiv.org/abs/2109.03795},
	abstract = {Representation learning constructs low-dimensional representations to summarize essential features of high-dimensional data. This learning problem is often approached by describing various desiderata associated with learned representations; e.g., that they be non-spurious, efficient, or disentangled. It can be challenging, however, to turn these intuitive desiderata into formal criteria that can be measured and enhanced based on observed data. In this paper, we take a causal perspective on representation learning, formalizing non-spuriousness and efficiency (in supervised representation learning) and disentanglement (in unsupervised representation learning) using counterfactual quantities and observable consequences of causal assertions. This yields computable metrics that can be used to assess the degree to which representations satisfy the desiderata of interest and learn non-spurious and disentangled representations from single observational datasets.},
	urldate = {2023-11-10},
	publisher = {arXiv},
	author = {Wang, Yixin and Jordan, Michael I.},
	month = feb,
	year = {2022},
	note = {arXiv:2109.03795 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@inproceedings{ahuja_interventional_2023,
	title = {Interventional {Causal} {Representation} {Learning}},
	url = {https://proceedings.mlr.press/v202/ahuja23a.html},
	abstract = {Causal representation learning seeks to extract high-level latent factors from low-level sensory data. Most existing methods rely on observational data and structural assumptions (e.g., conditional independence) to identify the latent factors. However, interventional data is prevalent across applications. Can interventional data facilitate causal representation learning? We explore this question in this paper. The key observation is that interventional data often carries geometric signatures of the latent factors’ support (i.e. what values each latent can possibly take). For example, when the latent factors are causally connected, interventions can break the dependency between the intervened latents’ support and their ancestors’. Leveraging this fact, we prove that the latent causal factors can be identified up to permutation and scaling given data from perfect do interventions. Moreover, we can achieve block affine identification, namely the estimated latent factors are only entangled with a few other latents if we have access to data from imperfect interventions. These results highlight the unique power of interventional data in causal representation learning; they can enable provable identification of latent factors without any assumptions about their distributions or dependency structure.},
	language = {en},
	urldate = {2023-11-10},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ahuja, Kartik and Mahajan, Divyat and Wang, Yixin and Bengio, Yoshua},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {372--407},
}

@misc{chen_isolating_2019,
	title = {Isolating {Sources} of {Disentanglement} in {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/1802.04942},
	abstract = {We decompose the evidence lower bound to show the existence of a term measuring the total correlation between latent variables. We use this to motivate our \${\textbackslash}beta\$-TCVAE (Total Correlation Variational Autoencoder), a refinement of the state-of-the-art \${\textbackslash}beta\$-VAE objective for learning disentangled representations, requiring no additional hyperparameters during training. We further propose a principled classifier-free measure of disentanglement called the mutual information gap (MIG). We perform extensive quantitative and qualitative experiments, in both restricted and non-restricted settings, and show a strong relation between total correlation and disentanglement, when the latent variables model is trained using our framework.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Chen, Ricky T. Q. and Li, Xuechen and Grosse, Roger and Duvenaud, David},
	month = apr,
	year = {2019},
	note = {arXiv:1802.04942 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{zimmermann_contrastive_2022,
	title = {Contrastive {Learning} {Inverts} the {Data} {Generating} {Process}},
	url = {http://arxiv.org/abs/2102.08850},
	abstract = {Contrastive learning has recently seen tremendous success in self-supervised learning. So far, however, it is largely unclear why the learned representations generalize so effectively to a large variety of downstream tasks. We here prove that feedforward models trained with objectives belonging to the commonly used InfoNCE family learn to implicitly invert the underlying generative model of the observed data. While the proofs make certain statistical assumptions about the generative model, we observe empirically that our findings hold even if these assumptions are severely violated. Our theory highlights a fundamental connection between contrastive learning, generative modeling, and nonlinear independent component analysis, thereby furthering our understanding of the learned representations as well as providing a theoretical foundation to derive more effective contrastive losses.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Zimmermann, Roland S. and Sharma, Yash and Schneider, Steffen and Bethge, Matthias and Brendel, Wieland},
	month = apr,
	year = {2022},
	note = {arXiv:2102.08850 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{bai_contrastively_nodate,
	title = {Contrastively {Disentangled} {Sequential} {Variational} {Autoencoder}},
	abstract = {Self-supervised disentangled representation learning is a critical task in sequence modeling. The learnt representations contribute to better model interpretability as well as the data generation, and improve the sample efﬁciency for downstream tasks. We propose a novel sequence representation learning method, named Contrastively Disentangled Sequential Variational Autoencoder (C-DSVAE), to extract and separate the static (time-invariant) and dynamic (time-variant) factors in the latent space. Different from previous sequential variational autoencoder methods, we use a novel evidence lower bound which maximizes the mutual information between the input and the latent factors, while penalizes the mutual information between the static and dynamic factors. We leverage contrastive estimations of the mutual information terms in training, together with simple yet effective augmentation techniques, to introduce additional inductive biases. Our experiments show that C-DSVAE signiﬁcantly outperforms the previous state-of-the-art methods on multiple metrics.},
	language = {en},
	author = {Bai, Junwen and Wang, Weiran and Gomes, Carla},
}

@misc{kopiczko_vera_2023,
	title = {{VeRA}: {Vector}-based {Random} {Matrix} {Adaptation}},
	shorttitle = {{VeRA}},
	url = {http://arxiv.org/abs/2310.11454},
	abstract = {Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous per-user or per-task adapted models. In this work, we present Vector-based Random Matrix Adaptation (VeRA), which reduces the number of trainable parameters by 10x compared to LoRA, yet maintains the same performance. It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead. We demonstrate its effectiveness on the GLUE and E2E benchmarks, and show its application in instruction-following with just 1.4M parameters using the Llama2 7B model.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Kopiczko, Dawid Jan and Blankevoort, Tijmen and Asano, Yuki Markus},
	month = oct,
	year = {2023},
	note = {arXiv:2310.11454 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{camps-valls_discovering_2023,
	title = {Discovering {Causal} {Relations} and {Equations} from {Data}},
	url = {http://arxiv.org/abs/2305.13341},
	abstract = {Physics is a field of science that has traditionally used the scientific method to answer questions about why natural phenomena occur and to make testable models that explain the phenomena. Discovering equations, laws and principles that are invariant, robust and causal explanations of the world has been fundamental in physical sciences throughout the centuries. Discoveries emerge from observing the world and, when possible, performing interventional studies in the system under study. With the advent of big data and the use of data-driven methods, causal and equation discovery fields have grown and made progress in computer science, physics, statistics, philosophy, and many applied fields. All these domains are intertwined and can be used to discover causal relations, physical laws, and equations from observational data. This paper reviews the concepts, methods, and relevant works on causal and equation discovery in the broad field of Physics and outlines the most important challenges and promising future lines of research. We also provide a taxonomy for observational causal and equation discovery, point out connections, and showcase a complete set of case studies in Earth and climate sciences, fluid dynamics and mechanics, and the neurosciences. This review demonstrates that discovering fundamental laws and causal relations by observing natural phenomena is being revolutionised with the efficient exploitation of observational data, modern machine learning algorithms and the interaction with domain knowledge. Exciting times are ahead with many challenges and opportunities to improve our understanding of complex systems.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Camps-Valls, Gustau and Gerhardus, Andreas and Ninad, Urmi and Varando, Gherardo and Martius, Georg and Balaguer-Ballester, Emili and Vinuesa, Ricardo and Diaz, Emiliano and Zanna, Laure and Runge, Jakob},
	month = may,
	year = {2023},
	note = {arXiv:2305.13341 [physics, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability, Statistics - Methodology},
}

@misc{camps-valls_discovering_2023-1,
	title = {Discovering {Causal} {Relations} and {Equations} from {Data}},
	url = {http://arxiv.org/abs/2305.13341},
	abstract = {Physics is a field of science that has traditionally used the scientific method to answer questions about why natural phenomena occur and to make testable models that explain the phenomena. Discovering equations, laws and principles that are invariant, robust and causal explanations of the world has been fundamental in physical sciences throughout the centuries. Discoveries emerge from observing the world and, when possible, performing interventional studies in the system under study. With the advent of big data and the use of data-driven methods, causal and equation discovery fields have grown and made progress in computer science, physics, statistics, philosophy, and many applied fields. All these domains are intertwined and can be used to discover causal relations, physical laws, and equations from observational data. This paper reviews the concepts, methods, and relevant works on causal and equation discovery in the broad field of Physics and outlines the most important challenges and promising future lines of research. We also provide a taxonomy for observational causal and equation discovery, point out connections, and showcase a complete set of case studies in Earth and climate sciences, fluid dynamics and mechanics, and the neurosciences. This review demonstrates that discovering fundamental laws and causal relations by observing natural phenomena is being revolutionised with the efficient exploitation of observational data, modern machine learning algorithms and the interaction with domain knowledge. Exciting times are ahead with many challenges and opportunities to improve our understanding of complex systems.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Camps-Valls, Gustau and Gerhardus, Andreas and Ninad, Urmi and Varando, Gherardo and Martius, Georg and Balaguer-Ballester, Emili and Vinuesa, Ricardo and Diaz, Emiliano and Zanna, Laure and Runge, Jakob},
	month = may,
	year = {2023},
	note = {arXiv:2305.13341 [physics, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability, Statistics - Methodology},
}

@misc{davidson_hyperspherical_2022,
	title = {Hyperspherical {Variational} {Auto}-{Encoders}},
	url = {http://arxiv.org/abs/1804.00891},
	abstract = {The Variational Auto-Encoder (VAE) is one of the most used unsupervised machine learning models. But although the default choice of a Gaussian distribution for both the prior and posterior represents a mathematically convenient distribution often leading to competitive results, we show that this parameterization fails to model data with a latent hyperspherical structure. To address this issue we propose using a von Mises-Fisher (vMF) distribution instead, leading to a hyperspherical latent space. Through a series of experiments we show how such a hyperspherical VAE, or \${\textbackslash}mathcal\{S\}\$-VAE, is more suitable for capturing data with a hyperspherical latent structure, while outperforming a normal, \${\textbackslash}mathcal\{N\}\$-VAE, in low dimensions on other data types. Code at http://github.com/nicola-decao/s-vae-tf and https://github.com/nicola-decao/s-vae-pytorch},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Davidson, Tim R. and Falorsi, Luca and De Cao, Nicola and Kipf, Thomas and Tomczak, Jakub M.},
	month = sep,
	year = {2022},
	note = {arXiv:1804.00891 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{mattei_miwae_2019,
	title = {{MIWAE}: {Deep} {Generative} {Modelling} and {Imputation} of {Incomplete} {Data}},
	shorttitle = {{MIWAE}},
	url = {http://arxiv.org/abs/1812.02633},
	abstract = {We consider the problem of handling missing data with deep latent variable models (DLVMs). First, we present a simple technique to train DLVMs when the training set contains missing-at-random data. Our approach, called MIWAE, is based on the importance-weighted autoencoder (IWAE), and maximises a potentially tight lower bound of the log-likelihood of the observed data. Compared to the original IWAE, our algorithm does not induce any additional computational overhead due to the missing data. We also develop Monte Carlo techniques for single and multiple imputation using a DLVM trained on an incomplete data set. We illustrate our approach by training a convolutional DLVM on a static binarisation of MNIST that contains 50\% of missing pixels. Leveraging multiple imputation, a convolutional network trained on these incomplete digits has a test performance similar to one trained on complete data. On various continuous and binary data sets, we also show that MIWAE provides accurate single imputations, and is highly competitive with state-of-the-art methods.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Mattei, Pierre-Alexandre and Frellsen, Jes},
	month = feb,
	year = {2019},
	note = {arXiv:1812.02633 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@article{rahimpour_non-intrusive_2017,
	title = {Non-{Intrusive} {Energy} {Disaggregation} {Using} {Non}-negative {Matrix} {Factorization} with {Sum}-to-k {Constraint}},
	volume = {32},
	issn = {0885-8950, 1558-0679},
	url = {http://arxiv.org/abs/1704.07308},
	doi = {10.1109/TPWRS.2017.2660246},
	abstract = {Energy disaggregation or Non-Intrusive Load Monitoring (NILM) addresses the issue of extracting device-level energy consumption information by monitoring the aggregated signal at one single measurement point without installing meters on each individual device. Energy disaggregation can be formulated as a source separation problem where the aggregated signal is expressed as linear combination of basis vectors in a matrix factorization framework. In this paper, an approach based on Sum-to-k constrained Non-negative Matrix Factorization (S2K-NMF) is proposed. By imposing the sum-to-k constraint and the non-negative constraint, S2K-NMF is able to effectively extract perceptually meaningful sources from complex mixtures. The strength of the proposed algorithm is demonstrated through two sets of experiments: Energy disaggregation in a residential smart home, and HVAC components energy monitoring in an industrial building testbed maintained at the Oak Ridge National Laboratory (ORNL). Extensive experimental results demonstrate the superior performance of S2K-NMF as compared to state-of-the-art decomposition-based disaggregation algorithms. The source code and our collected data (HVORUT) for studying NILM for HVAC units can be found at https://bitbucket.org/aicip/nonintrusive-load-monitoring.},
	number = {6},
	urldate = {2023-11-08},
	journal = {IEEE Transactions on Power Systems},
	author = {Rahimpour, Alireza and Qi, Hairong and Fugate, David and Kuruganti, Teja},
	month = nov,
	year = {2017},
	note = {arXiv:1704.07308 [cs]},
	keywords = {Computer Science - Computational Engineering, Finance, and Science},
	pages = {4430--4441},
}

@misc{kingma_auto-encoding_2022,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2022},
	note = {arXiv:1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{yeche_neighborhood_2021,
	title = {Neighborhood {Contrastive} {Learning} {Applied} to {Online} {Patient} {Monitoring}},
	url = {https://proceedings.mlr.press/v139/yeche21a.html},
	abstract = {Intensive care units (ICU) are increasingly looking towards machine learning for methods to provide online monitoring of critically ill patients. In machine learning, online monitoring is often formulated as a supervised learning problem. Recently, contrastive learning approaches have demonstrated promising improvements over competitive supervised benchmarks. These methods rely on well-understood data augmentation techniques developed for image data which do not apply to online monitoring. In this work, we overcome this limitation by supplementing time-series data augmentation techniques with a novel contrastive learning objective which we call neighborhood contrastive learning (NCL). Our objective explicitly groups together contiguous time segments from each patient while maintaining state-specific information. Our experiments demonstrate a marked improvement over existing work applying contrastive methods to medical time-series.},
	language = {en},
	urldate = {2023-11-08},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yèche, Hugo and Dresdner, Gideon and Locatello, Francesco and Hüser, Matthias and Rätsch, Gunnar},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {11964--11974},
}

@article{cha_orthogonality-enforced_nodate,
	title = {Orthogonality-{Enforced} {Latent} {Space} in {Autoencoders}: {An} {Approach} to {Learning} {Disentangled} {Representations}},
	abstract = {Noting the importance of factorizing (or disentangling) the latent space, we propose a novel, non-probabilistic disentangling framework for autoencoders, based on the principles of symmetry transformations that are independent of one another. To the best of our knowledge, this is the first deterministic model that is aiming to achieve disentanglement based on autoencoders using only a reconstruction loss without pairs of images or labels, by explicitly introducing inductive biases into a model architecture through Euler encoding. The proposed model is then compared with a number of state-of-the-art models, relevant to disentanglement, including symmetry-based models and generative models. Our evaluation using six different disentanglement metrics, including the unsupervised disentanglement metric we propose here in this paper, shows that the proposed model can offer better disentanglement, especially when variances of the features are different, where other methods may struggle. We believe that this model opens several opportunities for linear disentangled representation learning based on deterministic autoencoders.},
	language = {en},
	author = {Cha, Jaehoon and Thiyagalingam, Jeyan},
}


@article{yao2022temporally,
  title={Temporally disentangled representation learning},
  author={Yao, Weiran and Chen, Guangyi and Zhang, Kun},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={26492--26503},
  year={2022}
}

@inproceedings{yue2022ts2vec,
  title={Ts2vec: Towards universal representation of time series},
  author={Yue, Zhihan and Wang, Yujing and Duan, Juanyong and Yang, Tianmeng and Huang, Congrui and Tong, Yunhai and Xu, Bixiong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={8},
  pages={8980--8987},
  year={2022}
}


@article{ruff2021unifying,
  title={A unifying review of deep and shallow anomaly detection},
  author={Ruff, Lukas and Kauffmann, Jacob R and Vandermeulen, Robert A and Montavon, Gr{\'e}goire and Samek, Wojciech and Kloft, Marius and Dietterich, Thomas G and M{\"u}ller, Klaus-Robert},
  journal={Proceedings of the IEEE},
  volume={109},
  number={5},
  pages={756--795},
  year={2021},
  publisher={IEEE}
}

@misc{davidson_hyperspherical_2022-1,
	title = {Hyperspherical {Variational} {Auto}-{Encoders}},
	url = {http://arxiv.org/abs/1804.00891},
	abstract = {The Variational Auto-Encoder (VAE) is one of the most used unsupervised machine learning models. But although the default choice of a Gaussian distribution for both the prior and posterior represents a mathematically convenient distribution often leading to competitive results, we show that this parameterization fails to model data with a latent hyperspherical structure. To address this issue we propose using a von Mises-Fisher (vMF) distribution instead, leading to a hyperspherical latent space. Through a series of experiments we show how such a hyperspherical VAE, or \${\textbackslash}mathcal\{S\}\$-VAE, is more suitable for capturing data with a hyperspherical latent structure, while outperforming a normal, \${\textbackslash}mathcal\{N\}\$-VAE, in low dimensions on other data types. Code at http://github.com/nicola-decao/s-vae-tf and https://github.com/nicola-decao/s-vae-pytorch},
	urldate = {2023-11-07},
	publisher = {arXiv},
	author = {Davidson, Tim R. and Falorsi, Luca and De Cao, Nicola and Kipf, Thomas and Tomczak, Jakub M.},
	month = sep,
	year = {2022},
	note = {arXiv:1804.00891 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{domke_importance_2018,
	title = {Importance {Weighting} and {Variational} {Inference}},
	url = {http://arxiv.org/abs/1808.09034},
	abstract = {Recent work used importance sampling ideas for better variational bounds on likelihoods. We clarify the applicability of these ideas to pure probabilistic inference, by showing the resulting Importance Weighted Variational Inference (IWVI) technique is an instance of augmented variational inference, thus identifying the looseness in previous work. Experiments confirm IWVI's practicality for probabilistic inference. As a second contribution, we investigate inference with elliptical distributions, which improves accuracy in low dimensions, and convergence in high dimensions.},
	urldate = {2023-11-07},
	publisher = {arXiv},
	author = {Domke, Justin and Sheldon, Daniel},
	month = oct,
	year = {2018},
	note = {arXiv:1808.09034 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{alvarez_melis_towards_2018,
	title = {Towards {Robust} {Interpretability} with {Self}-{Explaining} {Neural} {Networks}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html},
	abstract = {Most recent work on interpretability of complex machine learning models has focused on estimating a-posteriori explanations for previously trained models around specific predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general -- explicitness, faithfulness, and stability -- and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.},
	urldate = {2023-11-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Alvarez Melis, David and Jaakkola, Tommi},
	year = {2018},
}

@inproceedings{alvarez_melis_towards_2018-1,
	title = {Towards {Robust} {Interpretability} with {Self}-{Explaining} {Neural} {Networks}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html},
	abstract = {Most recent work on interpretability of complex machine learning models has focused on estimating a-posteriori explanations for previously trained models around specific predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general -- explicitness, faithfulness, and stability -- and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.},
	urldate = {2023-11-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Alvarez Melis, David and Jaakkola, Tommi},
	year = {2018},
}

@misc{noauthor_carbon_nodate,
	title = {The carbon footprint of household energy use in the {United} {States}},
	url = {https://www.pnas.org/doi/10.1073/pnas.1922205117},
	language = {en},
	urldate = {2023-10-21},
	note = {ISBN: 9781922205117},
}

@article{booth_us_2010,
	title = {U.{S}. smart grid value at stake: {The} \$130 billion question},
	language = {en},
	author = {Booth, Adrian and Greene, Mike and Tai, Humayun},
	year = {2010},
}

@article{noauthor_chapter_nodate,
	title = {Chapter 5: {Increasing} {Efficiency} of {Building} {Systems} and {Technologies}},
	language = {en},
}

@misc{noauthor_neurips_nodate,
	title = {{NeurIPS} 2023 {Workshop} {UniReps} {Reviewers}},
	url = {https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/UniReps/Reviewers},
	abstract = {Welcome to the OpenReview homepage for NeurIPS 2023 Workshop UniReps Reviewers},
	language = {en},
	urldate = {2023-10-19},
	journal = {OpenReview},
}

@misc{wang_desiderata_2022-1,
	title = {Desiderata for {Representation} {Learning}: {A} {Causal} {Perspective}},
	shorttitle = {Desiderata for {Representation} {Learning}},
	url = {http://arxiv.org/abs/2109.03795},
	abstract = {Representation learning constructs low-dimensional representations to summarize essential features of high-dimensional data. This learning problem is often approached by describing various desiderata associated with learned representations; e.g., that they be non-spurious, efficient, or disentangled. It can be challenging, however, to turn these intuitive desiderata into formal criteria that can be measured and enhanced based on observed data. In this paper, we take a causal perspective on representation learning, formalizing non-spuriousness and efficiency (in supervised representation learning) and disentanglement (in unsupervised representation learning) using counterfactual quantities and observable consequences of causal assertions. This yields computable metrics that can be used to assess the degree to which representations satisfy the desiderata of interest and learn non-spurious and disentangled representations from single observational datasets.},
	urldate = {2023-10-19},
	publisher = {arXiv},
	author = {Wang, Yixin and Jordan, Michael I.},
	month = feb,
	year = {2022},
	note = {arXiv:2109.03795 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@misc{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://arxiv.org/abs/2002.05709},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	urldate = {2023-10-17},
	publisher = {arXiv},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {arXiv:2002.05709 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{oord_representation_2019,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	urldate = {2023-10-17},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	month = jan,
	year = {2019},
	note = {arXiv:1807.03748 [cs, stat]
version: 2},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{davidson_hyperspherical_2022-2,
	title = {Hyperspherical {Variational} {Auto}-{Encoders}},
	url = {http://arxiv.org/abs/1804.00891},
	doi = {10.48550/arXiv.1804.00891},
	abstract = {The Variational Auto-Encoder (VAE) is one of the most used unsupervised machine learning models. But although the default choice of a Gaussian distribution for both the prior and posterior represents a mathematically convenient distribution often leading to competitive results, we show that this parameterization fails to model data with a latent hyperspherical structure. To address this issue we propose using a von Mises-Fisher (vMF) distribution instead, leading to a hyperspherical latent space. Through a series of experiments we show how such a hyperspherical VAE, or \${\textbackslash}mathcal\{S\}\$-VAE, is more suitable for capturing data with a hyperspherical latent structure, while outperforming a normal, \${\textbackslash}mathcal\{N\}\$-VAE, in low dimensions on other data types. Code at http://github.com/nicola-decao/s-vae-tf and https://github.com/nicola-decao/s-vae-pytorch},
	urldate = {2023-10-13},
	publisher = {arXiv},
	author = {Davidson, Tim R. and Falorsi, Luca and De Cao, Nicola and Kipf, Thomas and Tomczak, Jakub M.},
	month = sep,
	year = {2022},
	note = {arXiv:1804.00891 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{jeon_gt-gan_2022,
	title = {{GT}-{GAN}: {General} {Purpose} {Time} {Series} {Synthesis} with {Generative} {Adversarial} {Networks}},
	shorttitle = {{GT}-{GAN}},
	url = {http://arxiv.org/abs/2210.02040},
	doi = {10.48550/arXiv.2210.02040},
	abstract = {Time series synthesis is an important research topic in the field of deep learning, which can be used for data augmentation. Time series data types can be broadly classified into regular or irregular. However, there are no existing generative models that show good performance for both types without any model changes. Therefore, we present a general purpose model capable of synthesizing regular and irregular time series data. To our knowledge, we are the first designing a general purpose time series synthesis model, which is one of the most challenging settings for time series synthesis. To this end, we design a generative adversarial network-based method, where many related techniques are carefully integrated into a single framework, ranging from neural ordinary/controlled differential equations to continuous time-flow processes. Our method outperforms all existing methods.},
	urldate = {2023-10-13},
	publisher = {arXiv},
	author = {Jeon, Jinsung and Kim, Jeonghak and Song, Haryong and Cho, Seunghyeon and Park, Noseong},
	month = oct,
	year = {2022},
	note = {arXiv:2210.02040 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{noauthor_monthly_2023,
	title = {Monthly {Energy} {Review} - {September} 2023},
	language = {en},
	year = {2023},
}

@article{makonin_rae_2018,
	title = {{RAE}: {The} {Rainforest} {Automation} {Energy} {Dataset} for {Smart} {Grid} {Meter} {Data} {Analysis}},
	volume = {3},
	issn = {2306-5729},
	shorttitle = {{RAE}},
	url = {http://arxiv.org/abs/1705.05767},
	doi = {10.3390/data3010008},
	abstract = {Datasets are important for researchers to build models and test how well their machine learning algorithms perform. This paper presents the Rainforest Automation Energy (RAE) dataset to help smart grid researchers test their algorithms which make use of smart meter data. This initial release of RAE contains 1Hz data (mains and sub-meters) from two a residential house. In addition to power data, environmental and sensor data from the house's thermostat is included. Sub-meter data from one of the houses includes heat pump and rental suite captures which is of interest to power utilities. We also show and energy breakdown of each house and show (by example) how RAE can be used to test non-intrusive load monitoring (NILM) algorithms.},
	number = {1},
	urldate = {2023-10-12},
	journal = {Data},
	author = {Makonin, Stephen and Wang, Z. Jane and Tumpach, Chris},
	month = feb,
	year = {2018},
	note = {arXiv:1705.05767 [cs]},
	keywords = {Computer Science - Other Computer Science},
	pages = {8},
}

@misc{wang_desiderata_2022-2,
	title = {Desiderata for {Representation} {Learning}: {A} {Causal} {Perspective}},
	shorttitle = {Desiderata for {Representation} {Learning}},
	url = {http://arxiv.org/abs/2109.03795},
	abstract = {Representation learning constructs low-dimensional representations to summarize essential features of high-dimensional data. This learning problem is often approached by describing various desiderata associated with learned representations; e.g., that they be non-spurious, efficient, or disentangled. It can be challenging, however, to turn these intuitive desiderata into formal criteria that can be measured and enhanced based on observed data. In this paper, we take a causal perspective on representation learning, formalizing non-spuriousness and efficiency (in supervised representation learning) and disentanglement (in unsupervised representation learning) using counterfactual quantities and observable consequences of causal assertions. This yields computable metrics that can be used to assess the degree to which representations satisfy the desiderata of interest and learn non-spurious and disentangled representations from single observational datasets.},
	urldate = {2023-10-12},
	publisher = {arXiv},
	author = {Wang, Yixin and Jordan, Michael I.},
	month = feb,
	year = {2022},
	note = {arXiv:2109.03795 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@misc{trauble_disentangled_2021-1,
	title = {On {Disentangled} {Representations} {Learned} {From} {Correlated} {Data}},
	url = {http://arxiv.org/abs/2006.07886},
	abstract = {The focus of disentanglement approaches has been on identifying independent factors of variation in data. However, the causal variables underlying real-world observations are often not statistically independent. In this work, we bridge the gap to real-world scenarios by analyzing the behavior of the most prominent disentanglement approaches on correlated data in a large-scale empirical study (including 4260 models). We show and quantify that systematically induced correlations in the dataset are being learned and reflected in the latent representations, which has implications for downstream applications of disentanglement such as fairness. We also demonstrate how to resolve these latent correlations, either using weak supervision during training or by post-hoc correcting a pre-trained model with a small number of labels.},
	urldate = {2023-10-12},
	publisher = {arXiv},
	author = {Träuble, Frederik and Creager, Elliot and Kilbertus, Niki and Locatello, Francesco and Dittadi, Andrea and Goyal, Anirudh and Schölkopf, Bernhard and Bauer, Stefan},
	month = jul,
	year = {2021},
	note = {arXiv:2006.07886 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{trauble_disentangled_nodate,
  title={On disentangled representations learned from correlated data},
  author={Tr{\"a}uble, Frederik and Creager, Elliot and Kilbertus, Niki and Locatello, Francesco and Dittadi, Andrea and Goyal, Anirudh and Sch{\"o}lkopf, Bernhard and Bauer, Stefan},
  booktitle={International Conference on Machine Learning},
  pages={10401--10412},
  year={2021},
  organization={PMLR}
}

@article{oublal_temporal_2023,
	series = {Workshop},
	title = {Temporal {Attention} {Bottleneck} is informative?  {Interpretability} through {Disentangled} {Generative} {Representations} for {Time} {Series} {Disaggregation}},
	abstract = {Generative models have garnered significant attention for their ability to address the challenge of source separation in disaggregation tasks. Energy Disaggregation holds promise for promoting energy conservation by allowing homeowners to gain comprehensive insights into their energy consumption solely through the interpretation of aggregated load curves. Nevertheless, the model’s ability to generalize and its interpretability remain two major challenges. To tackle these challenges, we deploy a generative model called TAB-VAE (Temporal Attention Bottleneck for Variational Auto-encoder), based on hierarchical architecture, addresses signature variability, and provides a robust, interpretable separation through the design of its informative representation of latent space. Our implementation and evaluation guidelines are available at https://github.com/ oublalkhalid/TAB-VAE.},
	language = {en},
	journal = {ICML 2023},
	author = {Oublal, Khalid and Ladjal, Saïd and Roueff, François and Benhaiem, David},
	year = {2023},
}

@article{higgins_learning_2017,
	title = {{LEARNING} {BASIC} {VISUAL} {CONCEPTS} {WITH} {A} {CONSTRAINED} {VARIATIONAL} {FRAMEWORK}},
	abstract = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artiﬁcial intelligence that is able to learn and reason in the same way that humans do. We introduce β-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modiﬁcation of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter β that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that β-VAE with appropriately tuned β {\textgreater} 1 qualitatively outperforms VAE (β = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also signiﬁcantly outperforms all baselines quantitatively. Unlike InfoGAN, β-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter β, which can be directly optimised through a hyperparameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.},
	language = {en},
	author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
	year = {2017},
}

@article{noauthor_notitle_nodate,
}

@misc{le_parisien_italie_2023,
	title = {Italie : un conducteur de {Ferrari} provoque un accident mortel lors d’un rallye de voitures de luxe},
	shorttitle = {Italie},
	url = {https://www.youtube.com/watch?v=c_R7uZCMCYU},
	abstract = {Deux touristes suisses sont morts carbonisés au volant d’une Ferrari louée à l’occasion d’un rallye de voitures de luxe, en Sardaigne. L’accident s’est produit sur une route à double sens, en Sardaigne,  
alors que le couple tentait un dépassement,  il a heurté cette Lamborghini, qui a ensuite percuté un camping-car. L'accident s'est produit sur une route qui comporte plusieurs tronçons où les dépassements sont interdits.},
	urldate = {2023-10-04},
	author = {{Le Parisien}},
	month = oct,
	year = {2023},
}

@inproceedings{chung_recurrent_2015,
	title = {A {Recurrent} {Latent} {Variable} {Model} for {Sequential} {Data}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper_files/paper/2015/hash/b618c3210e934362ac261db280128c22-Abstract.html},
	abstract = {In this paper, we explore the inclusion of latent random variables into the hidden state of a recurrent neural network (RNN) by combining the elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN) can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against other related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamics.},
	urldate = {2023-09-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron C and Bengio, Yoshua},
	year = {2015},
}

@misc{elfwing_sigmoid-weighted_2017,
	title = {Sigmoid-{Weighted} {Linear} {Units} for {Neural} {Network} {Function} {Approximation} in {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1702.03118},
	doi = {10.48550/arXiv.1702.03118},
	abstract = {In recent years, neural networks have enjoyed a renaissance as function approximators in reinforcement learning. Two decades after Tesauro's TD-Gammon achieved near top-level human performance in backgammon, the deep reinforcement learning algorithm DQN achieved human-level performance in many Atari 2600 games. The purpose of this study is twofold. First, we propose two activation functions for neural network function approximation in reinforcement learning: the sigmoid-weighted linear unit (SiLU) and its derivative function (dSiLU). The activation of the SiLU is computed by the sigmoid function multiplied by its input. Second, we suggest that the more traditional approach of using on-policy learning with eligibility traces, instead of experience replay, and softmax action selection with simple annealing can be competitive with DQN, without the need for a separate target network. We validate our proposed approach by, first, achieving new state-of-the-art results in both stochastic SZ-Tetris and Tetris with a small 10\${\textbackslash}times\$10 board, using TD(\${\textbackslash}lambda\$) learning and shallow dSiLU network agents, and, then, by outperforming DQN in the Atari 2600 domain by using a deep Sarsa(\${\textbackslash}lambda\$) agent with SiLU and dSiLU hidden units.},
	urldate = {2023-09-29},
	publisher = {arXiv},
	author = {Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
	month = nov,
	year = {2017},
	note = {arXiv:1702.03118 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{lea_temporal_2016,
	title = {Temporal {Convolutional} {Networks}: {A} {Unified} {Approach} to {Action} {Segmentation}},
	shorttitle = {Temporal {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1608.08242},
	abstract = {The dominant paradigm for video-based action segmentation is composed of two steps: first, for each frame, compute low-level features using Dense Trajectories or a Convolutional Neural Network that encode spatiotemporal information locally, and second, input these features into a classifier that captures high-level temporal relationships, such as a Recurrent Neural Network (RNN). While often effective, this decoupling requires specifying two separate models, each with their own complexities, and prevents capturing more nuanced long-range spatiotemporal relationships. We propose a unified approach, as demonstrated by our Temporal Convolutional Network (TCN), that hierarchically captures relationships at low-, intermediate-, and high-level time-scales. Our model achieves superior or competitive performance using video or sensor data on three public action segmentation datasets and can be trained in a fraction of the time it takes to train an RNN.},
	urldate = {2023-09-29},
	publisher = {arXiv},
	author = {Lea, Colin and Vidal, Rene and Reiter, Austin and Hager, Gregory D.},
	month = aug,
	year = {2016},
	note = {arXiv:1608.08242 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{yue_bert4nilm_2020,
	address = {New York, NY, USA},
	series = {{NILM}'20},
	title = {{BERT4NILM}: {A} {Bidirectional} {Transformer} {Model} for {Non}-{Intrusive} {Load} {Monitoring}},
	isbn = {978-1-4503-8191-8},
	shorttitle = {{BERT4NILM}},
	url = {https://dl.acm.org/doi/10.1145/3427771.3429390},
	doi = {10.1145/3427771.3429390},
	abstract = {Non-intrusive load monitoring (NILM) based energy disaggregation is the decomposition of a system's energy into the consumption of its individual appliances. Previous work on deep learning NILM algorithms has shown great potential in the field of energy management and smart grids. In this paper, we propose BERT4NILM, an architecture based on bidirectional encoder representations from transformers (BERT) and an improved objective function designed specifically for NILM learning. We adapt the bidirectional transformer architecture to the field of energy disaggregation and follow the pattern of sequence-to-sequence learning. With the improved loss function and masked training, BERT4NILM outperforms state-of-the-art models across various metrics on the two publicly available datasets UK-DALE and REDD.},
	urldate = {2023-09-29},
	booktitle = {Proceedings of the 5th {International} {Workshop} on {Non}-{Intrusive} {Load} {Monitoring}},
	publisher = {Association for Computing Machinery},
	author = {Yue, Zhenrui and Witzig, Camilo Requena and Jorde, Daniel and Jacobsen, Hans-Arno},
	month = nov,
	year = {2020},
	keywords = {Deep Learning, Energy Disaggregation, NILM, Neural Network, Non-Intrusive Load Monitoring, Transformer},
	pages = {89--93},
}

@article{kingma_adam_2014,
	title = {Adam: {A} method for stochastic optimization},
	journal = {arXiv preprint arXiv:1412.6980},
	author = {Kingma, Diederik P and Ba, Jimmy},
	year = {2014},
}

@article{yang_sequence_2021,
	title = {Sequence to {Point} {Learning} {Based} on an {Attention} {Neural} {Network} for {Nonintrusive} {Load} {Decomposition}},
	journal = {Electronics},
	author = {Yang, Mingzhi and Li, Xinchun and Liu, Yue},
	year = {2021},
}

@article{chen_convolutional_2018,
	title = {Convolutional sequence to sequence non-intrusive load monitoring},
	volume = {2018},
	number = {17},
	journal = {the Journal of Engineering},
	author = {Chen, Kunjin and Wang, Qin and He, Ziyu and Chen, Kunlong and Hu, Jun and He, Jinliang},
	year = {2018},
	note = {Publisher: Wiley Online Library},
	pages = {1860--1864},
}

@article{ciancetta_new_2021,
	title = {A {New} {Convolutional} {Neural} {Network}-{Based} {System} for {NILM} {Applications}},
	doi = {10.1109/TIM.2020.3035193},
	journal = {IEEE Transactions on Instrumentation and Measurement},
	author = {Ciancetta, Fabrizio and Bucci, Giovanni and Fiorucci, Edoardo and Mari, Simone and Fioravanti, Andrea},
	year = {2021},
}

@inproceedings{vahdat_nvae_2020,
	title = {{NVAE}: {A} {Deep} {Hierarchical} {Variational} {Autoencoder}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Vahdat, Arash and Kautz, Jan},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
}

@inproceedings{chen_isolating_2018,
	title = {Isolating {Sources} of {Disentanglement} in {Variational} {Autoencoders}},
	volume = {31},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Ricky T. Q. and Li, Xuechen and Grosse, Roger B and Duvenaud, David K},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
}

@article{kelly_uk-dale_2015,
	title = {The {UK}-{DALE} dataset, domestic appliance-level electricity demand and whole-house demand from five {UK} homes},
	volume = {2},
	journal = {Scientific data},
	author = {Kelly, Jack and Knottenbelt, William},
	year = {2015},
	note = {Publisher: Nature Publishing Group},
}


@inproceedings{valenti_exploiting_2018,
	title = {Exploiting the reactive power in deep neural models for non-intrusive load monitoring},
	booktitle = {2018 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Valenti, Michele and Bonfigli, Roberto and Principi, Emanuele and Squartini, Stefano},
	year = {2018},
}

@misc{zimmermann_contrastive_2022-1,
	title = {Contrastive {Learning} {Inverts} the {Data} {Generating} {Process}},
	url = {http://arxiv.org/abs/2102.08850},
	doi = {10.48550/arXiv.2102.08850},
	abstract = {Contrastive learning has recently seen tremendous success in self-supervised learning. So far, however, it is largely unclear why the learned representations generalize so effectively to a large variety of downstream tasks. We here prove that feedforward models trained with objectives belonging to the commonly used InfoNCE family learn to implicitly invert the underlying generative model of the observed data. While the proofs make certain statistical assumptions about the generative model, we observe empirically that our findings hold even if these assumptions are severely violated. Our theory highlights a fundamental connection between contrastive learning, generative modeling, and nonlinear independent component analysis, thereby furthering our understanding of the learned representations as well as providing a theoretical foundation to derive more effective contrastive losses.},
	urldate = {2023-09-29},
	publisher = {arXiv},
	author = {Zimmermann, Roland S. and Sharma, Yash and Schneider, Steffen and Bethge, Matthias and Brendel, Wieland},
	month = apr,
	year = {2022},
	note = {arXiv:2102.08850 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{zimmermann_contrastive_nodate,
	title = {Contrastive {Learning} {Inverts} the {Data} {Generating} {Process}},
	language = {en},
	author = {Zimmermann, Roland S and Sharma, Yash and Schneider, Steffen and Bethge, Matthias and Brendel, Wieland},
}

@misc{bengio_representation_2014,
	title = {Representation {Learning}: {A} {Review} and {New} {Perspectives}},
	shorttitle = {Representation {Learning}},
	url = {http://arxiv.org/abs/1206.5538},
	doi = {10.48550/arXiv.1206.5538},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	month = apr,
	year = {2014},
	note = {arXiv:1206.5538 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{wang_correlated_2023,
	title = {Correlated {Time} {Series} {Self}-{Supervised} {Representation} {Learning} via {Spatiotemporal} {Bootstrapping}},
	url = {http://arxiv.org/abs/2306.06994},
	abstract = {Correlated time series analysis plays an important role in many real-world industries. Learning an efficient representation of this large-scale data for further downstream tasks is necessary but challenging. In this paper, we propose a time-step-level representation learning framework for individual instances via bootstrapped spatiotemporal representation prediction. We evaluated the effectiveness and flexibility of our representation learning framework on correlated time series forecasting and cold-start transferring the forecasting model to new instances with limited data. A linear regression model trained on top of the learned representations demonstrates our model performs best in most cases. Especially compared to representation learning models, we reduce the RMSE, MAE, and MAPE by 37\%, 49\%, and 48\% on the PeMS-BAY dataset, respectively. Furthermore, in real-world metro passenger flow data, our framework demonstrates the ability to transfer to infer future information of new cold-start instances, with gains of 15\%, 19\%, and 18\%. The source code will be released under the GitHub https://github.com/bonaldli/Spatiotemporal-TS-Representation-Learning},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Wang, Luxuan and Bai, Lei and Li, Ziyue and Zhao, Rui and Tsung, Fugee},
	month = jun,
	year = {2023},
	note = {arXiv:2306.06994 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{yang_causalvae_2021,
	address = {Nashville, TN, USA},
	title = {{CausalVAE}: {Disentangled} {Representation} {Learning} via {Neural} {Structural} {Causal} {Models}},
	isbn = {978-1-66544-509-2},
	shorttitle = {{CausalVAE}},
	url = {https://ieeexplore.ieee.org/document/9578520/},
	doi = {10.1109/CVPR46437.2021.00947},
	abstract = {Learning disentanglement aims at ﬁnding a low dimensional representation which consists of multiple explanatory and generative factors of the observational data. The framework of variational autoencoder (VAE) is commonly used to disentangle independent factors from observations. However, in real scenarios, factors with semantics are not necessarily independent. Instead, there might be an underlying causal structure which renders these factors dependent. We thus propose a new VAE based framework named CausalVAE, which includes a Causal Layer to transform independent exogenous factors into causal endogenous ones that correspond to causally related concepts in data. We further analyze the model identiﬁabitily, showing that the proposed model learned from observations recovers the true one up to a certain degree. Experiments are conducted on various datasets, including synthetic and real word benchmark CelebA. Results show that the causal representations learned by CausalVAE are semantically interpretable, and their causal relationship as a Directed Acyclic Graph (DAG) is identiﬁed with good accuracy. Furthermore, we demonstrate that the proposed CausalVAE model is able to generate counterfactual data through “do-operation” to the causal factors.},
	language = {en},
	urldate = {2023-09-28},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yang, Mengyue and Liu, Furui and Chen, Zhitang and Shen, Xinwei and Hao, Jianye and Wang, Jun},
	month = jun,
	year = {2021},
	pages = {9588--9597},
}

@misc{scholkopf_towards_2021,
	title = {Towards {Causal} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2102.11107},
	abstract = {The two fields of machine learning and graphical causality arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Schölkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
	month = feb,
	year = {2021},
	note = {arXiv:2102.11107 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{shanmugam_elements_2018,
	title = {Elements of causal inference: foundations and learning algorithms},
	volume = {88},
	issn = {0094-9655, 1563-5163},
	shorttitle = {Elements of causal inference},
	url = {https://www.tandfonline.com/doi/full/10.1080/00949655.2018.1505197},
	doi = {10.1080/00949655.2018.1505197},
	language = {en},
	number = {16},
	urldate = {2023-09-27},
	journal = {Journal of Statistical Computation and Simulation},
	author = {Shanmugam, Ramalingam},
	month = nov,
	year = {2018},
	pages = {3248--3248},
}

@article{nalmpantis_time_2020,
	title = {On time series representations for multi-label {NILM}},
	volume = {32},
	issn = {0941-0643},
	url = {https://link.springer.com/epdf/10.1007/s00521-020-04916-5},
	doi = {10.1007/s00521-020-04916-5},
	abstract = {Given only the main power consumption of a household, a non-intrusive load monitoring (NILM) system identifies which appliances are operating. With the rise of Internet of things, running energy disaggregation models on the edge is more and more essential for privacy concerns and economic reasons. However, current NILM solutions use data-hungry deep learning models that can recognize only one device and are impossible to run on a device with limited resources. This research investigates in-depth multi-label NILM systems and suggests a novel framework which enables a cost-effective solution. It can be deployed on an embedded device, and thus, privacy can be preserved. The proposed system leverages dimensionality reduction using Signal2Vec, is evaluated on two popular public datasets and outperforms another state-of-the-art multi-label NILM system.},
	language = {en},
	number = {23},
	urldate = {2023-09-27},
	journal = {Neural Computing and Applications},
	author = {Nalmpantis, Christoforos and Vrakas, Dimitris},
	year = {2020},
}

@article{yang_semisupervised_2020,
	title = {Semisupervised {Multilabel} {Deep} {Learning} {Based} {Nonintrusive} {Load} {Monitoring} in {Smart} {Grids}},
	volume = {16},
	issn = {1551-3203, 1941-0050},
	url = {https://ieeexplore.ieee.org/document/8911216/},
	doi = {10.1109/TII.2019.2955470},
	abstract = {Nonintrusive load monitoring (NILM) is a technique that infers appliance-level energy consumption patterns and operation state changes based on feeder power signals. With the availability of ﬁne-grained electric load proﬁles, there has been increasing interest in using this approach for demand-side energy management in smart grids. NILM is a multilabel classiﬁcation problem due to the simultaneous operation of multiple appliances. Recently, deep learning based techniques have been shown to be a promising approach to solving this problem, but annotating the huge volume of load proﬁle data with multiple active appliances for learning is very challenging and impractical. In this article, a new semisupervised multilabel deep learning based framework is proposed to address this problem with the goal of mitigating the reliance on large labeled datasets. Speciﬁcally, a temporal convolutional neural network is used to automatically extract high-level load signatures for individual appliances. These signatures can be efﬁciently used to improve the feature representation capability of the framework. Case studies conducted on two open-access NILM datasets demonstrate the effectiveness and superiority of the proposed approach.},
	language = {en},
	number = {11},
	urldate = {2023-09-27},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Yang, Yandong and Zhong, Jing and Li, Wei and Gulliver, T. Aaron and Li, Shufang},
	month = nov,
	year = {2020},
	pages = {6892--6902},
}

@misc{noauthor_zimbra_nodate,
	title = {Zimbra: {Réception}},
	url = {https://z.imt.fr/zimbra/mail#1},
	urldate = {2023-09-27},
}

@misc{noauthor_zimbra_nodate-1,
	title = {Zimbra: {Réception}},
	url = {https://z.imt.fr/zimbra/mail#1},
	urldate = {2023-09-27},
}

@inproceedings{bai_contrastively_2021,
	title = {Contrastively {Disentangled} {Sequential} {Variational} {Autoencoder}},
	url = {https://openreview.net/forum?id=rWPxhfz2_S},
	abstract = {Self-supervised disentangled representation learning is a critical task in sequence modeling. The learnt representations contribute to better model interpretability as well as the data generation, and improve the sample efficiency for downstream tasks. We propose a novel sequence representation learning method, named Contrastively Disentangled Sequential Variational Autoencoder (C-DSVAE), to extract and separate the static (time-invariant) and dynamic (time-variant) factors in the latent space. Different from previous sequential variational autoencoder methods, we use a novel evidence lower bound which maximizes the mutual information between the input and the latent factors, while penalizes the mutual information between the static and dynamic factors. We leverage contrastive estimations of the mutual information terms in training, together with simple yet effective augmentation techniques, to introduce additional inductive biases. Our experiments show that C-DSVAE significantly outperforms the previous state-of-the-art methods on multiple metrics.},
	language = {en},
	urldate = {2023-09-27},
	author = {Bai, Junwen and Wang, Weiran and Gomes, Carla P.},
	month = nov,
	year = {2021},
}

@misc{carbonneau_measuring_2022,
	title = {Measuring {Disentanglement}: {A} {Review} of {Metrics}},
	shorttitle = {Measuring {Disentanglement}},
	url = {http://arxiv.org/abs/2012.09276},
	abstract = {Learning to disentangle and represent factors of variation in data is an important problem in AI. While many advances have been made to learn these representations, it is still unclear how to quantify disentanglement. While several metrics exist, little is known on their implicit assumptions, what they truly measure, and their limits. In consequence, it is difficult to interpret results when comparing different representations. In this work, we survey supervised disentanglement metrics and thoroughly analyze them. We propose a new taxonomy in which all metrics fall into one of three families: intervention-based, predictor-based and information-based. We conduct extensive experiments in which we isolate properties of disentangled representations, allowing stratified comparison along several axes. From our experiment results and analysis, we provide insights on relations between disentangled representation properties. Finally, we share guidelines on how to measure disentanglement.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Carbonneau, Marc-André and Zaidi, Julian and Boilard, Jonathan and Gagnon, Ghyslain},
	month = may,
	year = {2022},
	note = {arXiv:2012.09276 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2023-09-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
}

@inproceedings{apostolopoulou_deep_2022,
  title={Deep Attentive Variational Inference},
  author={Apostolopoulou, Ifigeneia and Char, Ian and Rosenfeld, Elan and Dubrawski, Artur},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@misc{franceschi_unsupervised_2020,
	title = {Unsupervised {Scalable} {Representation} {Learning} for {Multivariate} {Time} {Series}},
	url = {http://arxiv.org/abs/1901.10738},
	abstract = {Time series constitute a challenging data type for machine learning algorithms, due to their highly variable lengths and sparse labeling in practice. In this paper, we tackle this challenge by proposing an unsupervised method to learn universal embeddings of time series. Unlike previous works, it is scalable with respect to their length and we demonstrate the quality, transferability and practicability of the learned representations with thorough experiments and comparisons. To this end, we combine an encoder based on causal dilated convolutions with a novel triplet loss employing time-based negative sampling, obtaining general-purpose representations for variable length and multivariate time series.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Franceschi, Jean-Yves and Dieuleveut, Aymeric and Jaggi, Martin},
	month = jan,
	year = {2020},
	note = {arXiv:1901.10738 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{paul_psa-gan_2022,
	title = {{PSA}-{GAN}: {Progressive} {Self} {Attention} {GANs} for {Synthetic} {Time} {Series}},
	shorttitle = {{PSA}-{GAN}},
	url = {http://arxiv.org/abs/2108.00981},
	abstract = {Realistic synthetic time series data of sufficient length enables practical applications in time series modeling tasks, such as forecasting, but remains a challenge. In this paper we present PSA-GAN, a generative adversarial network (GAN) that generates long time series samples of high quality using progressive growing of GANs and self-attention. We show that PSA-GAN can be used to reduce the error in two downstream forecasting tasks over baselines that only use real data. We also introduce a Frechet-Inception Distance-like score, Context-FID, assessing the quality of synthetic time series samples. In our downstream tasks, we find that the lowest scoring models correspond to the best-performing ones. Therefore, Context-FID could be a useful tool to develop time series GAN models.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Paul, Jeha and Michael, Bohlke-Schneider and Pedro, Mercado and Shubham, Kapoor and Rajbir, Singh Nirwan and Valentin, Flunkert and Jan, Gasthaus and Tim, Januschowski},
	month = mar,
	year = {2022},
	note = {arXiv:2108.00981 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{zimmermann_contrastive_2022-2,
	title = {Contrastive {Learning} {Inverts} the {Data} {Generating} {Process}},
	url = {http://arxiv.org/abs/2102.08850},
	abstract = {Contrastive learning has recently seen tremendous success in self-supervised learning. So far, however, it is largely unclear why the learned representations generalize so effectively to a large variety of downstream tasks. We here prove that feedforward models trained with objectives belonging to the commonly used InfoNCE family learn to implicitly invert the underlying generative model of the observed data. While the proofs make certain statistical assumptions about the generative model, we observe empirically that our findings hold even if these assumptions are severely violated. Our theory highlights a fundamental connection between contrastive learning, generative modeling, and nonlinear independent component analysis, thereby furthering our understanding of the learned representations as well as providing a theoretical foundation to derive more effective contrastive losses.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Zimmermann, Roland S. and Sharma, Yash and Schneider, Steffen and Bethge, Matthias and Brendel, Wieland},
	month = apr,
	year = {2022},
	note = {arXiv:2102.08850 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{li_generative_2023,
	title = {Generative {Time} {Series} {Forecasting} with {Diffusion}, {Denoise}, and {Disentanglement}},
	url = {http://arxiv.org/abs/2301.03028},
	abstract = {Time series forecasting has been a widely explored task of great importance in many applications. However, it is common that real-world time series data are recorded in a short time period, which results in a big gap between the deep model and the limited and noisy time series. In this work, we propose to address the time series forecasting problem with generative modeling and propose a bidirectional variational auto-encoder (BVAE) equipped with diffusion, denoise, and disentanglement, namely D3VAE. Specifically, a coupled diffusion probabilistic model is proposed to augment the time series data without increasing the aleatoric uncertainty and implement a more tractable inference process with BVAE. To ensure the generated series move toward the true target, we further propose to adapt and integrate the multiscale denoising score matching into the diffusion process for time series forecasting. In addition, to enhance the interpretability and stability of the prediction, we treat the latent variable in a multivariate manner and disentangle them on top of minimizing total correlation. Extensive experiments on synthetic and real-world data show that D3VAE outperforms competitive algorithms with remarkable margins. Our implementation is available at https://github.com/PaddlePaddle/PaddleSpatial/tree/main/research/D3VAE.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Li, Yan and Lu, Xinjiang and Wang, Yaqing and Dou, Dejing},
	month = jan,
	year = {2023},
	note = {arXiv:2301.03028 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{li_generative_2023-1,
	title = {Generative {Time} {Series} {Forecasting} with {Diffusion}, {Denoise}, and {Disentanglement}},
	url = {http://arxiv.org/abs/2301.03028},
	abstract = {Time series forecasting has been a widely explored task of great importance in many applications. However, it is common that real-world time series data are recorded in a short time period, which results in a big gap between the deep model and the limited and noisy time series. In this work, we propose to address the time series forecasting problem with generative modeling and propose a bidirectional variational auto-encoder (BVAE) equipped with diffusion, denoise, and disentanglement, namely D3VAE. Specifically, a coupled diffusion probabilistic model is proposed to augment the time series data without increasing the aleatoric uncertainty and implement a more tractable inference process with BVAE. To ensure the generated series move toward the true target, we further propose to adapt and integrate the multiscale denoising score matching into the diffusion process for time series forecasting. In addition, to enhance the interpretability and stability of the prediction, we treat the latent variable in a multivariate manner and disentangle them on top of minimizing total correlation. Extensive experiments on synthetic and real-world data show that D3VAE outperforms competitive algorithms with remarkable margins. Our implementation is available at https://github.com/PaddlePaddle/PaddleSpatial/tree/main/research/D3VAE.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Li, Yan and Lu, Xinjiang and Wang, Yaqing and Dou, Dejing},
	month = jan,
	year = {2023},
	note = {arXiv:2301.03028 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{noauthor_zimbra_nodate-2,
	title = {Zimbra: {Réception}},
	url = {https://z.imt.fr/zimbra/mail#1},
	urldate = {2023-09-24},
}


@misc{maaloe_biva_2019,
	title = {{BIVA}: {A} {Very} {Deep} {Hierarchy} of {Latent} {Variables} for {Generative} {Modeling}},
	shorttitle = {{BIVA}},
	url = {http://arxiv.org/abs/1902.02102},
	abstract = {With the introduction of the variational autoencoder (VAE), probabilistic latent variable models have received renewed attention as powerful generative models. However, their performance in terms of test likelihood and quality of generated samples has been surpassed by autoregressive models without stochastic units. Furthermore, flow-based models have recently been shown to be an attractive alternative that scales well to high-dimensional data. In this paper we close the performance gap by constructing VAE models that can effectively utilize a deep hierarchy of stochastic variables and model complex covariance structures. We introduce the Bidirectional-Inference Variational Autoencoder (BIVA), characterized by a skip-connected generative model and an inference network formed by a bidirectional stochastic inference path. We show that BIVA reaches state-of-the-art test likelihoods, generates sharp and coherent natural images, and uses the hierarchy of latent variables to capture different aspects of the data distribution. We observe that BIVA, in contrast to recent results, can be used for anomaly detection. We attribute this to the hierarchy of latent variables which is able to extract high-level semantic features. Finally, we extend BIVA to semi-supervised classification tasks and show that it performs comparably to state-of-the-art results by generative adversarial networks.},
	urldate = {2023-09-24},
	publisher = {arXiv},
	author = {Maaløe, Lars and Fraccaro, Marco and Liévin, Valentin and Winther, Ole},
	month = nov,
	year = {2019},
	note = {arXiv:1902.02102 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{do_theory_2021,
	title = {Theory and {Evaluation} {Metrics} for {Learning} {Disentangled} {Representations}},
	url = {http://arxiv.org/abs/1908.09961},
	abstract = {We make two theoretical contributions to disentanglement learning by (a) defining precise semantics of disentangled representations, and (b) establishing robust metrics for evaluation. First, we characterize the concept "disentangled representations" used in supervised and unsupervised methods along three dimensions-informativeness, separability and interpretability - which can be expressed and quantified explicitly using information-theoretic constructs. This helps explain the behaviors of several well-known disentanglement learning models. We then propose robust metrics for measuring informativeness, separability and interpretability. Through a comprehensive suite of experiments, we show that our metrics correctly characterize the representations learned by different methods and are consistent with qualitative (visual) results. Thus, the metrics allow disentanglement learning methods to be compared on a fair ground. We also empirically uncovered new interesting properties of VAE-based methods and interpreted them with our formulation. These findings are promising and hopefully will encourage the design of more theoretically driven models for learning disentangled representations.},
	urldate = {2023-09-23},
	publisher = {arXiv},
	author = {Do, Kien and Tran, Truyen},
	month = mar,
	year = {2021},
	note = {arXiv:1908.09961 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}


@inproceedings{higgins_beta-vae_2016,
  title={beta-vae: Learning basic visual concepts with a constrained variational framework},
  author={Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  booktitle={International conference on learning representations},
  year={2016}
}

@misc{kim_disentangling_2019,
	title = {Disentangling by {Factorising}},
	url = {http://arxiv.org/abs/1802.05983},
	abstract = {We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon \${\textbackslash}beta\$-VAE by providing a better trade-off between disentanglement and reconstruction quality. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.},
	urldate = {2023-09-22},
	publisher = {arXiv},
	author = {Kim, Hyunjik and Mnih, Andriy},
	month = jul,
	year = {2019},
	note = {arXiv:1802.05983 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{kingma_auto-encoding_2022-1,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2023-09-22},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2022},
	note = {arXiv:1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bucci_new_2021,
	title = {A {New} {Convolutional} {Neural} {Network}-{Based} {System} for {NILM} {Applications}},
	doi = {10.1109/TIM.2020.3035193},
	journal = {IEEE Transactions on Instrumentation and Measurement},
	author = {Bucci, Giovanni and Fiorucci, Edoardo and Mari, Simone and Fioravanti, Andrea},
	year = {2021},
}

@article{koublal_xgen_2023,
	title = {{XGen}: {A} {Comprehensive} {Archive} and an {eXplainable} {Time} {Series} {Generation} {Framework} for {Energy}},
	url = {https://xgentimeseries.github.io},
	author = {Koublal, Ladjal S, Benhaiem D, le-borgne E and Roueff, F.},
	year = {2023},
}

@article{chen_isolating_2018-1,
	title = {Isolating sources of disentanglement in variational autoencoders},
	volume = {31},
	journal = {Advances in neural information processing systems},
	author = {Chen, Ricky TQ and Li, Xuechen and Grosse, Roger B and Duvenaud, David K},
	year = {2018},
}

@article{davidson_hyperspherical_2018,
	title = {Hyperspherical variational auto-encoders},
	journal = {arXiv preprint arXiv:1804.00891},
	author = {Davidson, Tim R and Falorsi, Luca and De Cao, Nicola and Kipf, Thomas and Tomczak, Jakub M},
	year = {2018},
}

@article{yang_sequence_2021-1,
	title = {Sequence to {Point} {Learning} {Based} on an {Attention} {Neural} {Network} for {Nonintrusive} {Load} {Decomposition}},
	journal = {Electronics},
	author = {Yang, Mingzhi and Li, Xinchun and Liu, Yue},
	year = {2021},
}

@article{kingma_adam_2014-1,
	title = {Adam: {A} method for stochastic optimization},
	journal = {arXiv preprint arXiv:1412.6980},
	author = {Kingma, Diederik P and Ba, Jimmy},
	year = {2014},
}

@article{chen_convolutional_2018-1,
	title = {Convolutional sequence to sequence non-intrusive load monitoring},
	volume = {2018},
	number = {17},
	journal = {the Journal of Engineering},
	author = {Chen, Kunjin and Wang, Qin and He, Ziyu and Chen, Kunlong and Hu, Jun and He, Jinliang},
	year = {2018},
	note = {Publisher: Wiley Online Library},
	pages = {1860--1864},
}

@inproceedings{vahdat_nvae_2020-1,
	title = {{NVAE}: {A} {Deep} {Hierarchical} {Variational} {Autoencoder}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Vahdat, Arash and Kautz, Jan},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
}

@inproceedings{chen_isolating_2018-2,
	title = {Isolating {Sources} of {Disentanglement} in {Variational} {Autoencoders}},
	volume = {31},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Ricky T. Q. and Li, Xuechen and Grosse, Roger B and Duvenaud, David K},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
}

@article{kelly_uk-dale_2015-1,
	title = {The {UK}-{DALE} dataset, domestic appliance-level electricity demand and whole-house demand from five {UK} homes},
	volume = {2},
	journal = {Scientific data},
	author = {Kelly, Jack and Knottenbelt, William},
	year = {2015},
	note = {Publisher: Nature Publishing Group},
}

@inproceedings{valenti_exploiting_2018-1,
	title = {Exploiting the reactive power in deep neural models for non-intrusive load monitoring},
	booktitle = {2018 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Valenti, Michele and Bonfigli, Roberto and Principi, Emanuele and Squartini, Stefano},
	year = {2018},
}

@inproceedings{kolter_redd_2011,
	title = {{REDD}: {A} public data set for energy disaggregation research},
	volume = {25},
	booktitle = {Workshop on data mining applications in sustainability ({SIGKDD}), {San} {Diego}, {CA}},
	author = {Kolter, J Zico and Johnson, Matthew J},
	year = {2011},
	note = {Issue: Citeseer},
}

@article{bardes_vicreg_2022,
	title = {{VICREG}: {VARIANCE}-{INVARIANCE}-{COVARIANCE} {RE}- {GULARIZATION} {FOR} {SELF}-{SUPERVISED} {LEARNING}},
	abstract = {Recent self-supervised methods for image representation learning maximize the agreement between embedding vectors produced by encoders fed with different views of the same image. The main challenge is to prevent a collapse in which the encoders produce constant or non-informative vectors. We introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with two regularizations terms applied to both embeddings separately: (1) a term that maintains the variance of each embedding dimension above a threshold, (2) a term that decorrelates each pair of variables. Unlike most other approaches to the same problem, VICReg does not require techniques such as: weight sharing between the branches, batch normalization, feature-wise normalization, output quantization, stop gradient, memory banks, etc., and achieves results on par with the state of the art on several downstream tasks. In addition, we show that our variance regularization term stabilizes the training of other methods and leads to performance improvements.},
	language = {en},
	author = {Bardes, Adrien and Ponce, Jean and LeCun, Yann},
	year = {2022},
}

@inproceedings{bardes_vicreg_2021,
	title = {{VICReg}: {Variance}-{Invariance}-{Covariance} {Regularization} for {Self}-{Supervised} {Learning}},
	shorttitle = {{VICReg}},
	url = {https://openreview.net/forum?id=xm6YD62D1Ub},
	abstract = {Recent self-supervised methods for image representation learning maximize the agreement between embedding vectors produced by encoders fed with different views of the same image. The main challenge is to prevent a collapse in which the encoders produce constant or non-informative vectors. We introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with two regularizations terms applied to both embeddings separately: (1) a term that maintains the variance of each embedding dimension above a threshold, (2) a term that decorrelates each pair of variables. Unlike most other approaches to the same problem, VICReg does not require techniques such as: weight sharing between the branches, batch normalization, feature-wise normalization, output quantization, stop gradient, memory banks, etc., and achieves results on par with the state of the art on several downstream tasks. In addition, we show that our variance regularization term stabilizes the training of other methods and leads to performance improvements.},
	language = {en},
	urldate = {2023-09-21},
	author = {Bardes, Adrien and Ponce, Jean and LeCun, Yann},
	month = oct,
	year = {2021},
}

@misc{roth_disentanglement_2023,
	title = {Disentanglement of {Correlated} {Factors} via {Hausdorff} {Factorized} {Support}},
	url = {http://arxiv.org/abs/2210.07347},
	abstract = {A grand goal in deep learning research is to learn representations capable of generalizing across distribution shifts. Disentanglement is one promising direction aimed at aligning a model's representation with the underlying factors generating the data (e.g. color or background). Existing disentanglement methods, however, rely on an often unrealistic assumption: that factors are statistically independent. In reality, factors (like object color and shape) are correlated. To address this limitation, we consider the use of a relaxed disentanglement criterion -- the Hausdorff Factorized Support (HFS) criterion -- that encourages only pairwise factorized {\textbackslash}emph\{support\}, rather than a factorial distribution, by minimizing a Hausdorff distance. This allows for arbitrary distributions of the factors over their support, including correlations between them. We show that the use of HFS consistently facilitates disentanglement and recovery of ground-truth factors across a variety of correlation settings and benchmarks, even under severe training correlations and correlation shifts, with in parts over \$+60{\textbackslash}\%\$ in relative improvement over existing disentanglement methods. In addition, we find that leveraging HFS for representation learning can even facilitate transfer to downstream tasks such as classification under distribution shifts. We hope our original approach and positive empirical results inspire further progress on the open problem of robust generalization. Code available at https://github.com/facebookresearch/disentangling-correlated-factors.},
	urldate = {2023-09-21},
	publisher = {arXiv},
	author = {Roth, Karsten and Ibrahim, Mark and Akata, Zeynep and Vincent, Pascal and Bouchacourt, Diane},
	month = feb,
	year = {2023},
	note = {arXiv:2210.07347 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}


@article{ren2021learning,
  title={Learning disentangled representation by exploiting pretrained generative models: A contrastive learning view},
  author={Ren, Xuanchi and Yang, Tao and Wang, Yuwang and Zeng, Wenjun},
  journal={arXiv preprint arXiv:2102.10543},
  year={2021}
}

@article{klindt_towards_2021,
  title={Towards nonlinear disentanglement in natural data with temporal sparse coding},
  author={Klindt, David and Schott, Lukas and Sharma, Yash and Ustyuzhaninov, Ivan and Brendel, Wieland and Bethge, Matthias and Paiton, Dylan},
  journal={arXiv preprint arXiv:2007.10930},
  year={2020}
}

@article{zhao2019deep,
  title={Deep temporal convolutional networks for short-term traffic flow forecasting},
  author={Zhao, Wentian and Gao, Yanyun and Ji, Tingxiang and Wan, Xili and Ye, Feng and Bai, Guangwei},
  journal={Ieee Access},
  volume={7},
  pages={114496--114507},
  year={2019},
  publisher={IEEE}
}

@article{murray2017electrical,
  title={An electrical load measurements dataset of United Kingdom households from a two-year longitudinal study},
  author={Murray, David and Stankovic, Lina and Stankovic, Vladimir},
  journal={Scientific data},
  volume={4},
  number={1},
  pages={1--12},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@book{ash2012information,
  title={Information theory},
  author={Ash, Robert B},
  year={2012},
  publisher={Courier Corporation}
}

@inproceedings{zhang_use_2022,
	address = {New Orleans, LA, USA},
	title = {Use {All} {The} {Labels}: {A} {Hierarchical} {Multi}-{Label} {Contrastive} {Learning} {Framework}},
	isbn = {978-1-66546-946-3},
	shorttitle = {Use {All} {The} {Labels}},
	url = {https://ieeexplore.ieee.org/document/9880213/},
	doi = {10.1109/CVPR52688.2022.01616},
	abstract = {Current contrastive learning frameworks focus on leveraging a single supervisory signal to learn representations, which limits the efficacy on unseen data and downstream tasks. In this paper, we present a hierarchical multi-label representation learning framework that can leverage all available labels and preserve the hierarchical relationship between classes. We introduce novel hierarchy preserving losses, which jointly apply a hierarchical penalty to the contrastive loss, and enforce the hierarchy constraint. The loss function is data driven and automatically adapts to arbitrary multi-label structures. Experiments on several datasets show that our relationship-preserving embedding performs well on a variety of tasks and outperform the baseline supervised and self-supervised approaches. Code is available at https://github.com/salesforce/ hierarchicalContrastiveLearning.},
	language = {en},
	urldate = {2023-09-20},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhang, Shu and Xu, Ran and Xiong, Caiming and Ramaiah, Chetan},
	month = jun,
	year = {2022},
	pages = {16639--16648},
}

@article{do_theory_2020,
	title = {{THEORY} {AND} {EVALUATION} {METRICS} {FOR} {LEARNING} {DISENTANGLED} {REPRESENTATIONS}},
	abstract = {We make two theoretical contributions to disentanglement learning by (a) deﬁning precise semantics of disentangled representations, and (b) establishing robust metrics for evaluation. First, we characterize the concept “disentangled representations” used in supervised and unsupervised methods along three dimensions–informativeness, separability and interpretability–which can be expressed and quantiﬁed explicitly using information-theoretic constructs. This helps explain the behaviors of several well-known disentanglement learning models. We then propose robust metrics for measuring informativeness, separability, and interpretability. Through a comprehensive suite of experiments, we show that our metrics correctly characterize the representations learned by different methods and are consistent with qualitative (visual) results. Thus, the metrics allow disentanglement learning methods to be compared on a fair ground. We also empirically uncovered new interesting properties of VAE-based methods and interpreted them with our formulation. These ﬁndings are promising and hopefully will encourage the design of more theoretically driven models for learning disentangled representations.},
	language = {en},
	author = {Do, Kien and Tran, Truyen},
	year = {2020},
}

@article{daudel_alpha-divergence_nodate,
	title = {Alpha-divergence {Variational} {Inference} {Meets} {Importance} {Weighted} {Auto}-{Encoders}: {Methodology} and {Asymptotics}},
	abstract = {Several algorithms involving the Variational R´enyi (VR) bound have been proposed to minimize an alpha-divergence between a target posterior distribution and a variational distribution. Despite promising empirical results, those algorithms resort to biased stochastic gradient descent procedures and thus lack theoretical guarantees. In this paper, we formalize and study the VR-IWAE bound, a generalization of the importance weighted auto-encoder (IWAE) bound. We show that the VR-IWAE bound enjoys several desirable properties and notably leads to the same stochastic gradient descent procedure as the VR bound in the reparameterized case, but this time by relying on unbiased gradient estimators. We then provide two complementary theoretical analyses of the VR-IWAE bound and thus of the standard IWAE bound. Those analyses shed light on the beneﬁts or lack thereof of these bounds. Lastly, we illustrate our theoretical claims over toy and real-data examples.},
	language = {en},
	author = {Daudel, Kamelia and Benton, Joe and Shi, Yuyang and Doucet, Arnaud},
}

@misc{pham_pcaae_2020,
	title = {{PCAAE}: {Principal} {Component} {Analysis} {Autoencoder} for organising the latent space of generative networks},
	shorttitle = {{PCAAE}},
	url = {http://arxiv.org/abs/2006.07827},
	abstract = {Autoencoders and generative models produce some of the most spectacular deep learning results to date. However, understanding and controlling the latent space of these models presents a considerable challenge. Drawing inspiration from principal component analysis and autoencoder, we propose the Principal Component Analysis Autoencoder (PCAAE). This is a novel autoencoder whose latent space verifies two properties. Firstly, the dimensions are organised in decreasing importance with respect to the data at hand. Secondly, the components of the latent space are statistically independent. We achieve this by progressively increasing the latent space during training, and with a covariance loss applied to the latent codes. The resulting autoencoder produces a latent space which separates the intrinsic attributes of the data into different components of the latent space, in a completely unsupervised manner. We also describe an extension of our approach to the case of powerful, pre-trained GANs. We show results on both synthetic examples of shapes and on a state-of-the-art GAN. For example, we are able to separate the color shade scale of hair and skin, pose of faces and the gender in the CelebA, without accessing any labels. We compare the PCAAE with other state-of-the-art approaches, in particular with respect to the ability to disentangle attributes in the latent space. We hope that this approach will contribute to better understanding of the intrinsic latent spaces of powerful deep generative models.},
	urldate = {2023-09-20},
	publisher = {arXiv},
	author = {Pham, Chi-Hieu and Ladjal, Saïd and Newson, Alasdair},
	month = jun,
	year = {2020},
	note = {arXiv:2006.07827 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}


@inproceedings{eastwood_framework_2018,
  title={A framework for the quantitative evaluation of disentangled representations},
  author={Eastwood, Cian and Williams, Christopher KI},
  booktitle={International conference on learning representations},
  year={2018}
}

@misc{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://arxiv.org/abs/2002.05709},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	urldate = {2023-09-20},
	publisher = {arXiv},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {arXiv:2002.05709 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_iclr-2023-self-supervised-attention-based-variational-autoencoder-for-appliance-usage_nodate,
	title = {{ICLR}-2023-{SELF}-{SUPERVISED}-{ATTENTION}-{BASED}-{VARIATIONAL}-{AUTOENCODER}-{FOR}-{APPLIANCE}-{USAGE}},
	url = {https://www.overleaf.com/project/64d55386b646ac8e003d85ac},
	abstract = {An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2023-09-19},
}

@misc{khosla_supervised_2021,
	title = {Supervised {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2004.11362},
	abstract = {Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches subsume or significantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4\% on the ImageNet dataset, which is 0.8\% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows benefits for robustness to natural corruptions and is more stable to hyperparameter settings such as optimizers and data augmentations. Our loss function is simple to implement, and reference TensorFlow code is released at https://t.ly/supcon.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
	month = mar,
	year = {2021},
	note = {arXiv:2004.11362 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{luo_time_2023,
	title = {Time {Series} {Contrastive} {Learning} with {Information}-{Aware} {Augmentations}},
	url = {http://arxiv.org/abs/2303.11911},
	abstract = {Various contrastive learning approaches have been proposed in recent years and achieve significant empirical success. While effective and prevalent, contrastive learning has been less explored for time series data. A key component of contrastive learning is to select appropriate augmentations imposing some priors to construct feasible positive samples, such that an encoder can be trained to learn robust and discriminative representations. Unlike image and language domains where ``desired'' augmented samples can be generated with the rule of thumb guided by prefabricated human priors, the ad-hoc manual selection of time series augmentations is hindered by their diverse and human-unrecognizable temporal structures. How to find the desired augmentations of time series data that are meaningful for given contrastive learning tasks and datasets remains an open question. In this work, we address the problem by encouraging both high {\textbackslash}textit\{fidelity\} and {\textbackslash}textit\{variety\} based upon information theory. A theoretical analysis leads to the criteria for selecting feasible data augmentations. On top of that, we propose a new contrastive learning approach with information-aware augmentations, InfoTS, that adaptively selects optimal augmentations for time series representation learning. Experiments on various datasets show highly competitive performance with up to 12.0{\textbackslash}\% reduction in MSE on forecasting tasks and up to 3.7{\textbackslash}\% relative improvement in accuracy on classification tasks over the leading baselines.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Luo, Dongsheng and Cheng, Wei and Wang, Yingheng and Xu, Dongkuan and Ni, Jingchao and Yu, Wenchao and Zhang, Xuchao and Liu, Yanchi and Chen, Yuncong and Chen, Haifeng and Zhang, Xiang},
	month = mar,
	year = {2023},
	note = {arXiv:2303.11911 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv:1406.2661 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{li_dual-stream_2021,
	title = {Dual-stream {Multiple} {Instance} {Learning} {Network} for {Whole} {Slide} {Image} {Classification} with {Self}-supervised {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2011.08939},
	abstract = {We address the challenging problem of whole slide image (WSI) classification. WSIs have very high resolutions and usually lack localized annotations. WSI classification can be cast as a multiple instance learning (MIL) problem when only slide-level labels are available. We propose a MIL-based method for WSI classification and tumor detection that does not require localized annotations. Our method has three major components. First, we introduce a novel MIL aggregator that models the relations of the instances in a dual-stream architecture with trainable distance measurement. Second, since WSIs can produce large or unbalanced bags that hinder the training of MIL models, we propose to use self-supervised contrastive learning to extract good representations for MIL and alleviate the issue of prohibitive memory cost for large bags. Third, we adopt a pyramidal fusion mechanism for multiscale WSI features, and further improve the accuracy of classification and localization. Our model is evaluated on two representative WSI datasets. The classification accuracy of our model compares favorably to fully-supervised methods, with less than 2\% accuracy gap across datasets. Our results also outperform all previous MIL-based methods. Additional benchmark results on standard MIL datasets further demonstrate the superior performance of our MIL aggregator on general MIL problems. GitHub repository: https://github.com/binli123/dsmil-wsi},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Li, Bin and Li, Yin and Eliceiri, Kevin W.},
	month = apr,
	year = {2021},
	note = {arXiv:2011.08939 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{zbontar_barlow_2021,
	title = {Barlow {Twins}: {Self}-{Supervised} {Learning} via {Redundancy} {Reduction}},
	shorttitle = {Barlow {Twins}},
	url = {http://arxiv.org/abs/2103.03230},
	abstract = {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stéphane},
	month = jun,
	year = {2021},
	note = {arXiv:2103.03230 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
}

@article{woo_cost_2022,
  title={CoST: Contrastive learning of disentangled seasonal-trend representations for time series forecasting},
  author={Woo, Gerald and Liu, Chenghao and Sahoo, Doyen and Kumar, Akshat and Hoi, Steven},
  journal={arXiv preprint arXiv:2202.01575},
  year={2022}
}

@inproceedings{liu2022multivariate,
  title={Multivariate Time-series Imputation with Disentangled Temporal Representations},
  author={LIU, SHUAI and Li, Xiucheng and Cong, Gao and Chen, Yile and JIANG, YUE},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@misc{vahdat_nvae_2021,
	title = {{NVAE}: {A} {Deep} {Hierarchical} {Variational} {Autoencoder}},
	shorttitle = {{NVAE}},
	url = {http://arxiv.org/abs/2007.03898},
	abstract = {Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256\${\textbackslash}times\$256 pixels. The source code is available at https://github.com/NVlabs/NVAE .},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Vahdat, Arash and Kautz, Jan},
	month = jan,
	year = {2021},
	note = {arXiv:2007.03898 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kumar_variational_2018,
  title={Variational inference of disentangled latent concepts from unlabeled observations},
  author={Kumar, Abhishek and Sattigeri, Prasanna and Balakrishnan, Avinash},
  journal={arXiv preprint arXiv:1711.00848},
  year={2017}
}

@article{liu_multivariate_2023-1,
	title = {{MULTIVARIATE} {TIME}-{SERIES} {IMPUTATION} {WITH} {DIS}- {ENTANGLED} {TEMPORAL} {REPRESENTATIONS}},
	abstract = {Multivariate time series often faces the problem of missing value. Many time series imputation methods have been developed in literature. However, they all rely on an entangled representation to model dynamics of time series, which may fail to fully exploit the multiple factors (e.g., periodic patterns) presented in the data. Moreover, the entangled representations usually have no semantic meaning, and thus they often lack interpretability. In addition, many recent models are proposed to deal with the whole time series to identify temporal dynamics, but they are not scalable to long time series. Different from existing approaches, we propose TIDER, a novel matrix factorization-based method with disentangled temporal representations that account for multiple factors, namely trend, seasonality, and local bias, to model complex dynamics. The learned disentanglement makes the imputation process more reliable and offers explainability for imputation results. Moreover, TIDER is scalable to long time series. Empirical results show that our method outperforms existing approaches on three typical real-world datasets, especially on long time series, reducing mean absolute error by up to 50\%. It also scales well to long datasets on which existing deep learning based methods struggle. Disentanglement validation experiments further highlight the robustness and accuracy of our model.},
	language = {en},
	author = {Liu, Shuai and Li, Xiucheng and Cong, Gao and Chen, Yile and Jiang, Yue},
	year = {2023},
}

@misc{bouchacourt_multi-level_2017,
	title = {Multi-{Level} {Variational} {Autoencoder}: {Learning} {Disentangled} {Representations} from {Grouped} {Observations}},
	shorttitle = {Multi-{Level} {Variational} {Autoencoder}},
	url = {http://arxiv.org/abs/1705.08841},
	abstract = {We would like to learn a representation of the data which decomposes an observation into factors of variation which we can independently control. Specifically, we want to use minimal supervision to learn a latent representation that reflects the semantics behind a specific grouping of the data, where within a group the samples share a common factor of variation. For example, consider a collection of face images grouped by identity. We wish to anchor the semantics of the grouping into a relevant and disentangled representation that we can easily exploit. However, existing deep probabilistic models often assume that the observations are independent and identically distributed. We present the Multi-Level Variational Autoencoder (ML-VAE), a new deep probabilistic model for learning a disentangled representation of a set of grouped observations. The ML-VAE separates the latent representation into semantically meaningful parts by working both at the group level and the observation level, while retaining efficient test-time inference. Quantitative and qualitative evaluations show that the ML-VAE model (i) learns a semantically meaningful disentanglement of grouped data, (ii) enables manipulation of the latent representation, and (iii) generalises to unseen groups.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Bouchacourt, Diane and Tomioka, Ryota and Nowozin, Sebastian},
	month = may,
	year = {2017},
	note = {arXiv:1705.08841 [cs, stat]},
	keywords = {Computer Science - Machine Learning, ML-VAE, Statistics - Machine Learning},
}


@article{jutten2004advances,
  title={Advances in blind source separation (BSS) and independent component analysis (ICA) for nonlinear mixtures},
  author={Jutten, Christian and Karhunen, Juha},
  journal={International journal of neural systems},
  volume={14},
  number={05},
  pages={267--292},
  year={2004},
  publisher={World Scientific}
}

@book{peters2017elements,
  title={Elements of causal inference: foundations and learning algorithms},
  author={Peters, Jonas and Janzing, Dominik and Sch{\"o}lkopf, Bernhard},
  year={2017},
  publisher={The MIT Press}
}


@inproceedings{haber2018learning,
  title={Learning across scales---multiscale methods for convolution neural networks},
  author={Haber, Eldad and Ruthotto, Lars and Holtham, Elliot and Jun, Seong-Hwan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}


@article{autoformer,
  title={Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting},
  author={Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={22419--22430},
  year={2021}
}


@inproceedings{chen2023learning,
  title={Learning a sparse transformer network for effective image deraining},
  author={Chen, Xiang and Li, Hao and Li, Mingqiang and Pan, Jinshan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5896--5905},
  year={2023}
}

@inproceedings{fedformer,
  title={Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting},
  author={Zhou, Tian and Ma, Ziqing and Wen, Qingsong and Wang, Xue and Sun, Liang and Jin, Rong},
  booktitle={International conference on machine learning},
  pages={27268--27286},
  year={2022},
  organization={PMLR}
}

@inproceedings{timesnet,
  title={Timesnet: Temporal 2d-variation modeling for general time series analysis},
  author={Wu, Haixu and Hu, Tengge and Liu, Yong and Zhou, Hang and Wang, Jianmin and Long, Mingsheng},
  booktitle={The eleventh international conference on learning representations},
  year={2022}
}

@inproceedings{Dlinear,
  title={Are transformers effective for time series forecasting?},
  author={Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={37},
  number={9},
  pages={11121--11128},
  year={2023}
}

@article{bai2021contrastively,
  title={Contrastively disentangled sequential variational autoencoder},
  author={Bai, Junwen and Wang, Weiran and Gomes, Carla P},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={10105--10118},
  year={2021}
}

@incollection{jutten2010nonlinear,
  title={Nonlinear mixtures},
  author={Jutten, Christian and Babaie-Zadeh, Massoud and Karhunen, Juha},
  booktitle={Handbook of Blind Source Separation},
  pages={549--592},
  year={2010},
  publisher={Elsevier}
}

@article{hyvarinen2013independent,
  title={Independent component analysis: recent advances},
  author={Hyv{\"a}rinen, Aapo},
  journal={Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume={371},
  number={1984},
  pages={20110534},
  year={2013},
  publisher={The Royal Society Publishing}
}

@article{comon1994independent,
  title={Independent component analysis, a new concept?},
  author={Comon, Pierre},
  journal={Signal processing},
  volume={36},
  number={3},
  pages={287--314},
  year={1994},
  publisher={Elsevier}
}


@article{nguyen2019approximations,
  title={On approximations via convolution-defined mixture models},
  author={Nguyen, Hien D and McLachlan, Geoffrey},
  journal={Communications in Statistics-Theory and Methods},
  volume={48},
  number={16},
  pages={3945--3955},
  year={2019},
  publisher={Taylor \& Francis}
}

@article{yao2021learning,
  title={Learning temporally causal latent processes from general temporal data},
  author={Yao, Weiran and Sun, Yuewen and Ho, Alex and Sun, Changyin and Zhang, Kun},
  journal={arXiv preprint arXiv:2110.05428},
  year={2021}
}

@article{ng2023identifiability,
  title={On the identifiability of sparse ICA without assuming non-Gaussianity},
  author={Ng, Ignavier and Zheng, Yujia and Dong, Xinshuai and Zhang, Kun},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={47960--47990},
  year={2023}
}


@inproceedings{brady2023provably,
  author = {
    Brady, Jack and
    Zimmermann, Roland S. and
    Sharma, Yash and
    Sch{\"o}lkopf, Bernhard and
    von K{\"u}gelgen, Julius
    Brendel, Wieland and
  },
  title = {
    Provably Learning
    Object-Centric Representations
  },
  year = {2023},
  booktitle = {
    Proceedings of the 40th International
    Conference on Machine Learning
  },
  articleno = {126},
  numpages = {25},
  location = {Honolulu, Hawaii, USA},
  series = {ICML'23}
}



%%%%% COST

@article{scholkopf2021toward,
  title={Toward causal representation learning},
  author={Sch{\"o}lkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  journal={Proceedings of the IEEE},
  volume={109},
  number={5},
  pages={612--634},
  year={2021},
  publisher={IEEE}
}

% ------ Start: Time Series Decomopsition ------
@book{hyndman2018forecasting,
  title={Forecasting: principles and practice},
  author={Hyndman, Rob J and Athanasopoulos, George},
  year={2018},
  publisher={OTexts}
}

@misc{wen2018robuststl,
      title={RobustSTL: A Robust Seasonal-Trend Decomposition Algorithm for Long Time Series}, 
      author={Qingsong Wen and Jingkun Gao and Xiaomin Song and Liang Sun and Huan Xu and Shenghuo Zhu},
      year={2018},
      eprint={1812.01767},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wen2019robusttrend,
    title={RobustTrend: A Huber Loss with a Combined First and Second Order Difference Regularization for Time Series Trend Filtering}, 
    author={Qingsong Wen and Jingkun Gao and Xiaomin Song and Liang Sun and Jian Tan},
    year={2019},
    eprint={1906.03751},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{wen2020fast,
    author = {Wen, Qingsong and Zhang, Zhe and Li, Yan and Sun, Liang},
    title = {Fast RobustSTL: Efficient and Robust Seasonal-Trend Decomposition for Time Series with Complex Patterns},
    year = {2020},
    isbn = {9781450379984},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3394486.3403271},
    doi = {10.1145/3394486.3403271},
    booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
    pages = {2203–2213},
    numpages = {11},
    keywords = {time series, generalized ADMM, seasonal-trend decomposition, multiple seasonality},
    location = {Virtual Event, CA, USA},
    series = {KDD '20}
}

@INPROCEEDINGS{yang2021multiscale,
  author={Yang, Linxiao and Wen, Qingsong and Yang, Bo and Sun, Liang},
  booktitle={ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={A Robust and Efficient Multi-Scale Seasonal-Trend Decomposition}, 
  year={2021},
  volume={},
  number={},
  pages={5085-5089},
  doi={10.1109/ICASSP39728.2021.9413939}}

@article{godfrey2017decomposition,
   title={Neural Decomposition of Time-Series Data for Effective Generalization},
   ISSN={2162-2388},
   url={http://dx.doi.org/10.1109/TNNLS.2017.2709324},
   DOI={10.1109/tnnls.2017.2709324},
   journal={IEEE Transactions on Neural Networks and Learning Systems},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Godfrey, Luke B. and Gashler, Michael S.},
   year={2017},
   pages={1–13}
}


% ------ End: Time Series Decomopsition ------

% ------ Start: Time series representation learning ------
@misc{yue2021ts2vec,
  title={TS2Vec: Towards Universal Representation of Time Series}, 
  author={Zhihan Yue and Yujing Wang and Juanyong Duan and Tianmeng Yang and Congrui Huang and Yunhai Tong and Bixiong Xu},
  year={2021},
  eprint={2106.10466},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}
@inproceedings{
tonekaboni2021unsupervised,
title={Unsupervised Representation Learning for Time Series with Temporal Neighborhood Coding},
author={Sana Tonekaboni and Danny Eytan and Anna Goldenberg},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=8qDwejCuCN}
}
@inproceedings{eldele2021time,
  title     = {Time-Series Representation Learning via Temporal and Contextual Contrasting},
  author    = {Eldele, Emadeldeen and Ragab, Mohamed and Chen, Zhenghua and Wu, Min and Kwoh, Chee Keong and Li, Xiaoli and Guan, Cuntai},
  booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, {IJCAI-21}},
  pages     = {2352--2359},
  year      = {2021},
}
@misc{franceschi2020unsupervised,
      title={Unsupervised Scalable Representation Learning for Multivariate Time Series}, 
      author={Jean-Yves Franceschi and Aymeric Dieuleveut and Martin Jaggi},
      year={2020},
      eprint={1901.10738},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{zerveas2021transformer,
author = {Zerveas, George and Jayaraman, Srideepika and Patel, Dhaval and Bhamidipaty, Anuradha and Eickhoff, Carsten},
title = {A Transformer-Based Framework for Multivariate Time Series Representation Learning},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467401},
doi = {10.1145/3447548.3467401},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
pages = {2114–2124},
numpages = {11},
series = {KDD '21}
}
% ------ End: Time series representation learning ------



@misc{oord2019representation,
      title={Representation Learning with Contrastive Predictive Coding}, 
      author={Aaron van den Oord and Yazhe Li and Oriol Vinyals},
      year={2019},
      eprint={1807.03748},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{he2020momentum,
      title={Momentum Contrast for Unsupervised Visual Representation Learning}, 
      author={Kaiming He and Haoqi Fan and Yuxin Wu and Saining Xie and Ross Girshick},
      year={2020},
      eprint={1911.05722},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
% ------ End: General representation learning ------

% ------ Start: Deep forecasting ------
@article{wen2017multi,
  title={A multi-horizon quantile recurrent forecaster},
  author={Wen, Ruofeng and Torkkola, Kari and Narayanaswamy, Balakrishnan and Madeka, Dhruv},
  journal={arXiv preprint arXiv:1711.11053},
  year={2017}
}

@inproceedings{lai2018modeling,
  title={Modeling long-and short-term temporal patterns with deep neural networks},
  author={Lai, Guokun and Chang, Wei-Cheng and Yang, Yiming and Liu, Hanxiao},
  booktitle={The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval},
  pages={95--104},
  year={2018}
}

@misc{salinas2019deepar,
      title={DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks}, 
      author={David Salinas and Valentin Flunkert and Jan Gasthaus},
      year={2019},
      eprint={1704.04110},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{rangapuram2018deep,
  title={Deep state space models for time series forecasting},
  author={Rangapuram, Syama Sundar and Seeger, Matthias W and Gasthaus, Jan and Stella, Lorenzo and Wang, Yuyang and Januschowski, Tim},
  journal={Advances in neural information processing systems},
  volume={31},
  pages={7785--7794},
  year={2018}
}

@inproceedings{wang2019deep,
  title={Deep factors for forecasting},
  author={Wang, Yuyang and Smola, Alex and Maddix, Danielle and Gasthaus, Jan and Foster, Dean and Januschowski, Tim},
  booktitle={International conference on machine learning},
  pages={6607--6617},
  year={2019},
  organization={PMLR}
}

@misc{li2020enhancing,
      title={Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting}, 
      author={Shiyang Li and Xiaoyong Jin and Yao Xuan and Xiyou Zhou and Wenhu Chen and Yu-Xiang Wang and Xifeng Yan},
      year={2020},
      eprint={1907.00235},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{zhou2021informer,
  title={Informer: Beyond efficient transformer for long sequence time-series forecasting},
  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  booktitle={Proceedings of AAAI},
  year={2021}
}

@misc{oreshkin2020nbeats,
      title={N-BEATS: Neural basis expansion analysis for interpretable time series forecasting}, 
      author={Boris N. Oreshkin and Dmitri Carpov and Nicolas Chapados and Yoshua Bengio},
      year={2020},
      eprint={1905.10437},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
% ------ End: Deep forecasting ------

@article{cuaresma2004forecasting,
  title={Forecasting electricity spot-prices using linear univariate time-series models},
  author={Cuaresma, Jes{\'u}s Crespo and Hlouskova, Jaroslava and Kossmeier, Stephan and Obersteiner, Michael},
  journal={Applied Energy},
  volume={77},
  number={1},
  pages={87--106},
  year={2004},
  publisher={Elsevier}
}

@article{carbonneau2008application,
  title={Application of machine learning techniques for supply chain demand forecasting},
  author={Carbonneau, Real and Laframboise, Kevin and Vahidov, Rustam},
  journal={European Journal of Operational Research},
  volume={184},
  number={3},
  pages={1140--1154},
  year={2008},
  publisher={Elsevier}
}

@article{kim2003financial,
  title={Financial time series forecasting using support vector machines},
  author={Kim, Kyoung-jae},
  journal={Neurocomputing},
  volume={55},
  number={1-2},
  pages={307--319},
  year={2003},
  publisher={Elsevier}
}

@inproceedings{laptev2017time,
  title={Time-series extreme event forecasting with neural networks at uber},
  author={Laptev, Nikolay and Yosinski, Jason and Li, Li Erran and Smyl, Slawek},
  booktitle={International conference on machine learning},
  volume={34},
  pages={1--5},
  year={2017}
}

@article{bai2018empirical,
  title={An empirical evaluation of generic convolutional and recurrent networks for sequence modeling},
  author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  journal={arXiv preprint arXiv:1803.01271},
  year={2018}
}

@article{makridakis2020m5,
  title={The M5 accuracy competition: Results, findings and conclusions},
  author={Makridakis, S and Spiliotis, E and Assimakopoulos, V},
  journal={Int J Forecast},
  year={2020}
}

@inproceedings{khurana2016cognito,
  title={Cognito: Automated feature engineering for supervised learning},
  author={Khurana, Udayan and Turaga, Deepak and Samulowitz, Horst and Parthasrathy, Srinivasan},
  booktitle={2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)},
  pages={1304--1307},
  year={2016},
  organization={IEEE}
}

@article{qiu2018multivariate,
  title={Multivariate Bayesian Structural Time Series Model.},
  author={Qiu, Jinwen and Jammalamadaka, S Rao and Ning, Ning},
  journal={J. Mach. Learn. Res.},
  volume={19},
  number={1},
  pages={2744--2776},
  year={2018}
}

@book{scott20154,
  title={4. Bayesian Variable Selection for Nowcasting Economic Time Series},
  author={Scott, Steven L and Varian, Hal R},
  year={2015},
  publisher={University of Chicago Press}
}

@inproceedings{parascandolo2018learning,
  title={Learning independent causal mechanisms},
  author={Parascandolo, Giambattista and Kilbertus, Niki and Rojas-Carulla, Mateo and Sch{\"o}lkopf, Bernhard},
  booktitle={International Conference on Machine Learning},
  pages={4036--4044},
  year={2018},
  organization={PMLR}
}

@book{peters2017elements,
  title={Elements of causal inference: foundations and learning algorithms},
  author={Peters, Jonas and Janzing, Dominik and Sch{\"o}lkopf, Bernhard},
  year={2017},
  publisher={The MIT Press}
}

@article{mitrovic2020representation,
  title={Representation learning via invariant causal mechanisms},
  author={Mitrovic, Jovana and McWilliams, Brian and Walker, Jacob and Buesing, Lars and Blundell, Charles},
  journal={arXiv preprint arXiv:2010.07922},
  year={2020}
}

@article{von2021self,
  title={Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style},
  author={von K{\"u}gelgen, Julius and Sharma, Yash and Gresele, Luigi and Brendel, Wieland and Sch{\"o}lkopf, Bernhard and Besserve, Michel and Locatello, Francesco},
  journal={arXiv preprint arXiv:2106.04619},
  year={2021}
}

@article{hyndman2008automatic,
  title={Automatic time series forecasting: the forecast package for R},
  author={Hyndman, Rob J and Khandakar, Yeasmin},
  journal={Journal of statistical software},
  volume={27},
  number={1},
  pages={1--22},
  year={2008}
}

@book{shumway2000time,
  title={Time series analysis and its applications},
  author={Shumway, Robert H and Stoffer, David S and Stoffer, David S},
  volume={3},
  year={2000},
  publisher={Springer}
}

@article{van2008visualizing,
  title={Visualizing data using t-SNE.},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={11},
  year={2008}
}

@article{de2011forecasting,
  title={Forecasting time series with complex seasonal patterns using exponential smoothing},
  author={De Livera, Alysha M and Hyndman, Rob J and Snyder, Ralph D},
  journal={Journal of the American statistical association},
  volume={106},
  number={496},
  pages={1513--1527},
  year={2011},
  publisher={Taylor \& Francis}
}

@article{cordeiro2009forecasting,
  title={Forecasting time series with BOOT. EXPOS procedure},
  author={Cordeiro, Clara and Neves, M},
  journal={REVSTAT-Statistical Journal},
  volume={7},
  number={2},
  pages={135--149},
  year={2009}
}

@inproceedings{Pearl2012TheDR,
  title={The Do-Calculus Revisited},
  author={Judea Pearl},
  booktitle={UAI},
  year={2012}
}

@misc{he2021fastmoe,
    title={FastMoE: A Fast Mixture-of-Expert Training System}, 
    author={Jiaao He and Jiezhong Qiu and Aohan Zeng and Zhilin Yang and Jidong Zhai and Jie Tang},
    year={2021},
    eprint={2103.13262},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


%%%%%%
Khalid add
%%%%%
@article{Hoerl1970,
  title={Ridge regression: Biased estimation for nonorthogonal problems},
  author={A. E. Hoerl and R. W. Kennard},
  journal={Technometrics},
  volume={12},
  number={1},
  pages={55--67},
  year={1970},
  publisher={Taylor \& Francis}
}

@article{bach2012optimization,
  title={Optimization with sparsity-inducing penalties},
  author={Bach, Francis and Jenatton, Rodolphe and Mairal, Julien and Obozinski, Guillaume and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={4},
  number={1},
  pages={1--106},
  year={2012},
  publisher={Now Publishers, Inc.}
}


@article{kuhnHungarianMethodAssignment1955,
  title = {The {{Hungarian}} Method for the Assignment Problem},
  author = {Kuhn, H. W.},
  year = {1955},
  month = mar,
  journal = {Naval Research Logistics Quarterly},
  volume = {2},
  number = {1-2},
  pages = {83--97},
  issn = {00281441, 19319193},
  doi = {10.1002/nav.3800020109},
  urldate = {2023-08-21},
  langid = {english}
}

@inproceedings{eastwood2018framework,
  title={A framework for the quantitative evaluation of disentangled representations},
  author={Eastwood, Cian and Williams, Christopher KI},
  booktitle={International Conference on Learning Representations},
  year={2018}
}


@article{Tibshirani1996,
  title={Regression shrinkage and selection via the lasso},
  author={R. Tibshirani},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={58},
  number={1},
  pages={267--288},
  year={1996},
  publisher={Wiley Online Library}
}


@article{Lounici_Pontil_Tsybakov2010,
  title={Oracle inequalities and optimal inference under group sparsity},
  author={K. Lounici and M. Pontil and A. B. Tsybakov},
  journal={The Annals of statistics},
  year={2011}
}

@article{Bickel_Ritov_Tsybakov2009,
  title={Simultaneous analysis of Lasso and {Dantzig} selector},
  author={P. J. Bickel and Y. Ritov and A. B. Tsybakov},
  journal={The Annals of statistics},
  volume={37},
  number={4},
  pages={1705--1732},
  year={2009},
  publisher={Institute of Mathematical Statistics}
}

@article{Mairal_Ponce_Sapiro_Zisserman_Bach2008,
  title={Supervised dictionary learning},
  author={J. Mairal and J. Ponce and G. Sapiro and A. Zisserman and F. Bach},
  journal={Advances in neural information processing systems},
  volume={21},
  year={2008}
}

@article{Chen1998atomic,
author = {Chen, S. S. and Donoho, D. L. and Saunders, M. A.},
title = {Atomic Decomposition by Basis Pursuit},
journal = {SIAM Journal on Scientific Computing},
year = {1998}
}



@article{Mairal2011,
  title={Task-driven dictionary learning},
  author={J. Mairal and F. Bach and J. Ponce},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={34},
  number={4},
  pages={791--804},
  year={2011},
  publisher={IEEE}
}

@inproceedings{sparseComponentAnalysisSurvey,
  TITLE = {A survey of Sparse Component Analysis for blind source separation: principles, perspectives, and new challenges},
  AUTHOR = {Gribonval, R. and Lesage, S.},
  BOOKTITLE = {{ESANN'06 proceedings - 14th European Symposium on Artificial Neural Networks}},
  YEAR = {2006}
}

@article{Richtarik2014,
  title={Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function},
  author={P. Richt{\'a}rik and M. Tak{\'a}{\v{c}}},
  journal={Mathematical Programming},
  volume={144},
  number={1},
  pages={1--38},
  year={2014},
  publisher={Springer}
}


@article{Lecun2015,
  title={Deep learning},
  author={Y. LeCun and Y. Bengio and G. Hinton},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}


@inproceedings{He2016,
  title={Deep residual learning for image recognition},
  author={K. He and X. Zhang and S. Ren and J. Sun},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}


@article{Wright_Nocedal1999,
  title={Numerical optimization},
  author={S. Wright and J. Nocedal},
  journal={Springer Science},
  volume={35},
  number={67-68},
  pages={7},
  year={1999}
}


@article{kreutz2003dictionary,
  title={Dictionary learning algorithms for sparse representation},
  author={K. Kreutz-Delgado and J. F. Murray and B. D. Rao and K. Engan and T.-W. Lee and T. J. Sejnowski},
  journal={Neural computation},
  volume={15},
  number={2},
  pages={349--396},
  year={2003},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}


@inproceedings{Mairal_Bach_Ponce2009,
  title={Online dictionary learning for sparse coding},
  author={J. Mairal and F. Bach and J. Ponce and G. Sapiro},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={689--696},
  year={2009}
}


@article{layernorm,
  title={Layer normalization},
  author={J. L. Ba and J. R. Kiros and G. E. Hinton},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}


@article{Malezieux2022,
  title={Dictionary and prior learning with unrolled algorithms for unsupervised inverse problems},
  author={B. Mal{\'e}zieux and T. Moreau and M. Kowalski},
  journal={ICLR},
  year={2022}
}


@article{Kingma_Ba2014,
  title={Adam: A method for stochastic optimization},
  author={D. P. Kingma and J. Ba},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}


@article{Obozinski2006,
  title={Multi-task feature selection},
  author={G. Obozinski and B. Taskar and M. Jordan},
  journal={Statistics Department, UC Berkeley, Tech. Rep},
  volume={2},
  number={2.2},
  pages={2},
  year={2006},
  publisher={Citeseer}
}


@article{Lounici2009,
  title={Taking advantage of sparsity in multi-task learning},
  author={K. Lounici and M. Pontil and A. B. Tsybakov and S. Van De Geer},
  journal={arXiv preprint arXiv:0903.1468},
  year={2009}
}


@article{Argyriou2008,
  title={Convex multi-task feature learning},
  author={A. Argyriou and T. Evgeniou and M. Pontil},
  journal={Machine learning},
  volume={73},
  number={3},
  pages={243--272},
  year={2008},
  publisher={Springer}
}

@article{benefitOfMTRL2016,
author = {Maurer, A. and Pontil, M. and Romera-Paredes, B.},
title = {The Benefit of Multitask Representation Learning},
year = {2016},
journal = {J. Mach. Learn. Res.}
}


@article{lounici2011oracle,
  title={Oracle inequalities and optimal inference under group sparsity},
  author={Lounici, K. and Pontil, M. and Van De Geer, S. and Tsybakov, A. B},
  journal={The annals of statistics},
  year={2011}
}


@book{Goodfellow2016,
  title={Deep learning},
  author={I. Goodfellow and Y. Bengio and A. Courville},
  year={2016},
  publisher={MIT press}
}

@inproceedings{Bertrand2020,
  title={Implicit differentiation of Lasso-type models for hyperparameter optimization},
  author={Q. Bertrand and Q. Klopfenstein and M. Blondel and S. Vaiter and A. Gramfort and J. Salmon},
  booktitle={International Conference on Machine Learning},
  pages={810--821},
  year={2020},
  organization={PMLR}
}


@article{Bolte2022,
  title={Automatic differentiation of nonsmooth iterative algorithms},
  author={J. Bolte and E. Pauwels and S. Vaiter},
  journal={NeurIPS},
  year={2022}
}


@article{Bolte2021,
  title={Nonsmooth implicit differentiation for machine-learning and optimization},
  author={J. Bolte and T. Le and E. and Pauwels and T. Silveti-Falls},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={13537--13549},
  year={2021}
}


@article{Bertrand2022,
  title={Implicit differentiation for fast hyperparameter selection in non-smooth convex learning},
  author={Q. Bertrand and Q. Klopfenstein and M. Massias and M. Blondel and S. Vaiter and A. Gramfort and J. Salmon},
  journal={JMLR},
  year={2022}
}

@inproceedings{Franceschi2018,
  title={Bilevel programming for hyperparameter optimization and meta-learning},
  author={L. Franceschi and P. Frasconi and S. Salzo and R. Grazzi and M. Pontil},
  booktitle={International Conference on Machine Learning},
  pages={1568--1577},
  year={2018},
  organization={PMLR}
}

@inproceedings{Pedregosa2016,
  title={Hyperparameter optimization with approximate gradient},
  author={F. Pedregosa},
  booktitle={International conference on machine learning},
  pages={737--746},
  year={2016},
  organization={PMLR}
}


@article{bengio2000,
  title={Gradient-based optimization of hyperparameters},
  author={Y. Bengio},
  journal={Neural computation},
  volume={12},
  number={8},
  pages={1889--1900},
  year={2000},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@incollection{Bengio+chapter2007,
author = {Y. Bengio and Y. LeCun},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {G. E.Hinton and S. Osindero and Y. W. Teh},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={I. Goodfellow and A. Courville and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

% linear & nonlinear ICA
@article{Darmois1953AnalyseGD,
  title={Analyse g{\'e}n{\'e}rale des liaisons stochastiques: etude particuli{\`e}re de l'analyse factorielle lin{\'e}aire},
  journal={Revue de l’Institut International
de Statistique},
  author={G. Darmois},
  year={1953}
}

 (1953). "" Dokl. Akad. Nauk SSSR (N.S.) (89): 217—219 (in Russian).

@article{Skitovich1953,
  title={On a property of the normal distribution.},
  author={Skitivic, V. P.},
  journal={Izvestiya Akademii Nauk SSSR. Seriya
Matematicheskaya},
  year={1953}
}

@article{comon1992,
  title={Independent component analysis.},
  author={Comon, P.},
  journal={Higher-Order Statistics},
  year={1992}
}

@book{ICAbook,
author = {Hyvärinen, A. and Karhunen, J. and Oja, E.},
year = {2001},
title = {Independent Component Analysis},
publisher = {Wiley}
}

@INPROCEEDINGS{AmuseICA90,
  author={Tong, L. and Soon, V.C. and Huang, Y.F. and Liu, R.},
  booktitle={IEEE International Symposium on Circuits and Systems},
  title={AMUSE: a new blind identification algorithm},
  year={1990}}

@article{HYVARINEN1999429,
title = {Nonlinear independent component analysis: Existence and uniqueness results},
journal = {Neural Networks},
year = {1999},
author = {Hyvärinen, A. and Pajunen, P.}
}

@article{Pavan2018OnTD,
  title={On the Darmois-Skitovich Theorem and Spatial Independence in Blind Source Separation},
  author={F. R. M. Pavan and M. D. Miranda},
  journal={Journal of Communication and Information Systems},
  year={2018}
}

@inproceedings{TCL2016,
 author = {Hyvärinen, A. and Morioka, H.},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA},
 year = {2016}
}

@InProceedings{PCL17,
  title = 	 {{Nonlinear ICA of Temporally Dependent Stationary Sources}},
  author = 	 {Hyvärinen, A. and Morioka, H.},
  booktitle = 	 {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  year = 	 {2017}
}

@inproceedings{HyvarinenST19,
  author = {Hyvärinen, A. and Sasaki, H. and Turner, R. E.},
  booktitle = {AISTATS},
  publisher = {PMLR},
  title = {Nonlinear ICA Using Auxiliary Variables and Generalized Contrastive Learning},
  year = 2019
}

@article{Gutmann12JMLR,
 author = {Gutmann, M. U. and Hyvärinen, A.},
 journal = {The Journal of Machine Learning Research},
 title = {Noise-contrastive estimation of unnormalized statistical models, with
applications to natural image statistics.},
 year = {2012}
}

@inproceedings{iVAEkhemakhem20a,
  title = 	 {Variational Autoencoders and Nonlinear ICA: A Unifying Framework},
  author =       {Khemakhem, I. and Kingma, D. and Monti, R. and Hyvärinen, A.},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  year = 	 {2020}
}

@inproceedings{Sorrenson2020Disentanglement,
title={Disentanglement by Nonlinear ICA with General Incompressible-flow Networks (GIN)},
author={Sorrenson, P. and Rother, C. and Köthe, U.},
booktitle={International Conference on Learning Representations},
year={2020}
}

@inproceedings{ice-beem20,
 author = {Khemakhem, I. and Monti, R. and Kingma, D. and Hyvärinen, A.},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {ICE-BeeM: Identifiable Conditional Energy-Based Deep Models Based on Nonlinear ICA},
 year = {2020}
}

% Disentanglement in DL
@article{bengio2013representation,
  author = {Bengio, Y. and Courville, A. and Vincent, P.},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  title = {Representation Learning: A Review and New Perspectives},
  year = 2013
}

@article{oord2018representation,
 author = {Oord, A. and Li, Y. and Vinyals, O.},
 journal = {Advances in Neural Information Processing Systems},
 title = {Representation learning with contrastive predictive coding},
 year = {2018}
}


@inproceedings{chen2021exploring,
  title={Exploring simple siamese representation learning},
  author={Chen, Xinlei and He, Kaiming},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={15750--15758},
  year={2021}
}

@article{grill2020bootstrap,
  title={Bootstrap your own latent-a new approach to self-supervised learning},
  author={Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21271--21284},
  year={2020}
}

@inproceedings{chen2020simple,
 author = {Chen, T. and
Kornblith, S. and
Norouzi, M. and
Hinton, G. E.},
 booktitle = {Proceedings of the 37th International Conference on Machine Learning},
 title = {A Simple Framework for Contrastive Learning of Visual Representations},
 year = {2020}
}


@InProceedings{pmlr-v80-kim18b,
  title = 	 {Disentangling by Factorising},
  author =       {Kim, H. and Mnih, A.},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  year = 	 {2018}
}

@inproceedings{
kumar2018variational,
title={VARIATIONAL INFERENCE OF DISENTANGLED LATENT CONCEPTS FROM UNLABELED OBSERVATIONS},
author={A. Kumar and P. Sattigeri and A. Balakrishnan},
booktitle={International Conference on Learning Representations},
year={2018}
}

@InProceedings{roeder2020linear,
  title = 	 {On Linear Identifiability of Learned Representations},
  author =       {Roeder, G. and Metz, L. and Kingma, D. P.},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  year = 	 {2021}
}

@conference{GreRubMehLocSch19,
  title = {The Incomplete Rosetta Stone problem: Identifiability results for Multi-view Nonlinear {ICA}},
  author = {Gresele, L. and Rubenstein, P. K. and Mehrjou, A. and Locatello, F. and Sch{\"{o}}lkopf, B.},
  booktitle = {Proceedings of the 35th Conference on Uncertainty in Artificial Intelligence (UAI)},
  year = {2019},
}

@inproceedings{ahuja2023interventional,
  title={Interventional causal representation learning},
  author={Ahuja, Kartik and Mahajan, Divyat and Wang, Yixin and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={372--407},
  year={2023},
  organization={PMLR}
}

@inproceedings{vonkugelgen2021selfsupervised,
title={Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style},
author={Von K{\"u}gelgen, J. and Sharma, Y. and Gresele, L. and Brendel, W. and Sch{\"o}lkopf, B. and Besserve, M. and Locatello, F.},
booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
year={2021}
}

@inproceedings{tcvae,
author = {Chen, R. T. Q. and Li, X. and G., R. and Duvenaud, D.},
title = {Isolating Sources of Disentanglement in VAEs},
year = {2018},
booktitle = {Advances in Neural Information Processing Systems}
}

@InProceedings{pmlr-v97-locatello19a,
title = {Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations},
author = {Locatello, F. and Bauer, S. and Lucic, M. and Raetsch, G. and Gelly, S. and Sch{\"o}lkopf, B. and Bachem, O.},
booktitle = {Proceedings of the 36th International Conference on Machine Learning},
year = {2019}
}

@inproceedings{
Locatello2020Disentangling,
title={Disentangling Factors of Variations Using Few Labels},
author={F. Locatello and M. Tschannen and S. Bauer and G. Rätsch and B. Schölkopf and O. Bachem},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SygagpEKwB}
}

@article{Bouchacourt_Tomioka_Nowozin_2018, title={Multi-Level Variational Autoencoder: Learning Disentangled Representations From Grouped Observations},
journal={Proceedings of the AAAI Conference on Artificial Intelligence},
author={Bouchacourt, D. and Tomioka, R. and Nowozin, S.},
year={2018}
}

@ARTICLE{TalebJutten1999,
  author={Taleb, A. and Jutten, C.},
  journal={IEEE Transactions on Signal Processing},
  title={Source separation in post-nonlinear mixtures},
  year={1999}
  }

@InProceedings{pmlr-v119-locatello20a,
  title = 	 {Weakly-Supervised Disentanglement Without Compromises},
  author =       {Locatello, F. and Poole, B. and Raetsch, G. and Sch{\"o}lkopf, B. and Bachem, O. and Tschannen, M.},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  year = {2020}
}

@InProceedings{zimmermann2021cl,
  title = 	 {Contrastive Learning Inverts the Data Generating Process},
  author =       {Zimmermann, R. S. and Sharma, Y. and Schneider, S. and Bethge, M. and Brendel, W.},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  year = 	 {2021}
}

@InProceedings{Yang_2021_CVPR,
    author    = {Yang, M. and Liu, F. and Chen, Z. and Shen, X. and Hao, J. and Wang, J.},
    title     = {{CausalVAE}: Disentangled Representation Learning via Neural Structural Causal Models},
    booktitle = {Proceedings of the {IEEE/CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
    year      = {2021}
}

@misc{shen2021disentangled,
      title={Disentangled Generative Causal Representation Learning},
      author={Shen, X. and Liu, F. and Dong, H. and Lian, Q. and Chen, Z. and Zhang, T.},
      year={2021},
      journal={arXiv preprint arXiv:2010.02637}
}

@inproceedings{
kocaoglu2018causalgan,
title={Causal{GAN}: Learning Causal Implicit Generative Models with Adversarial Training},
author={Kocaoglu, M. and Snyder, C. and Dimakis, A. G. and Vishwanath, S.},
booktitle={International Conference on Learning Representations},
year={2018}
}

@misc{nair2019causal,
      title={Causal Induction from Visual Observations for Goal Directed Tasks},
      author={Nair, S. and Zhu, Y. and Savarese, S. and Fei-Fei, L.},
      year={2019},
      journal={arXiv preprint arXiv:1910.01751}
}

@article{peters2015causal,
author = {Peters, J. and Bühlmann, P. and Meinshausen, N.},
title = {Causal inference by using invariant prediction: identification and confidence intervals},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
year = {2016}
}

@misc{arjovsky2020invariant,
      title={Invariant Risk Minimization},
      author={Arjovsky, M. and Bottou, L. and Gulrajani, I. and Lopez-Paz, D.},
      year={2020},
      journal={arXiv preprint arXiv:1907.02893}
}

@inproceedings{IRMgames,
author = {Ahuja, K. and Shanmugam, K. and Varshney, K. R. and Dhurandhar, A.},
title = {Invariant Risk Minimization Games},
year = {2020},
booktitle = {Proceedings of the 37th International Conference on Machine Learning}
}

@misc{
krueger2021outofdistribution,
title={Out-of-Distribution Generalization via Risk Extrapolation ({\{}RE{\}}x)},
author={D. Krueger and E. Caballero and J.-H. Jacobsen and A. Zhang and J. Binas and R. Le Priol and D. Zhang and A. Courville},
year={2021}
}


@misc{lu2021nonlinear,
      title={Nonlinear Invariant Risk Minimization: A Causal Approach},
      author={Chaochao Lu and Yuhuai Wu and Jośe Miguel Hernández-Lobato and Bernhard Schölkopf},
      year={2021},
      journal={arXiv preprint arXiv:2102.12353}
}

@inproceedings{
ahuja2022towards,
title={Towards efficient representation identification in supervised learning},
author={K. Ahuja and D. Mahajan and V. Syrgkanis and I. Mitliagkas},
booktitle={First Conference on Causal Learning and Reasoning},
year={2022}
}

@inproceedings{gulrajani2020search,
title={In Search of Lost Domain Generalization},
author={Gulrajani, I. and Lopez-Paz, D.},
booktitle={International Conference on Learning Representations},
year={2021}
}

@inproceedings{slowVAE,
  author    = {Klindt, D. A. and
               Schott, L. and
               Sharma, Y  and
               Ustyuzhaninov, I and
               Brendel, W. and
               Bethge, M. and
               Paiton, D. M.},
  title     = {Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse
               Coding},
  booktitle = {9th International Conference on Learning Representations},
  year      = {2021}
}

@inproceedings{
Duan2020UDR,
title={Unsupervised Model Selection for Variational Disentangled Representation Learning},
author={Duan, S. and Matthey, L. and Saraiva, A. and Watters, N. and Burgess, C. and Lerchner, A. and Higgins, I.},
booktitle={International Conference on Learning Representations},
year={2020}
}

@article{consciousBengio,
  author    = {Bengio, Y.},
  title     = {The Consciousness Prior},
  year      = {2017},
  journal={arXiv preprint arXiv:1709.08568}
}

% Causal Feature Learning
@article{Chalupka2017CausalFL,
  title={Causal feature learning: an overview},
  author={Chalupka, K. and Eberhardt, F. and Perona, P.},
  journal={Behaviormetrika},
  year={2017}
}


% Causal discovery
@article{ng2019masked,
  title={Masked Gradient-Based Causal Structure Learning},
  author={Ng, I. and Fang, Z. and Zhu, S. and Chen, Z. and Wang, J.},
  journal={arXiv preprint arXiv:1910.08527},
  year={2019}
}


@inproceedings{
Bengio2020A,
title={A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms},
author={Bengio, Y. and Deleu, T. and Rahaman, N. and Ke, N. R. and Lachapelle, S. and Bilaniuk, O. and Goyal, A. and Pal, C.},
booktitle={International Conference on Learning Representations},
year={2020}
}

@inproceedings{dcdi,
 author = {Brouillard, P. and Lachapelle, S. and Lacoste, A. and Lacoste-Julien, S. and Drouin, A.},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Differentiable Causal Discovery from Interventional Data},
 year = {2020}
}

@article{ut_igsp,
  title={Permutation-Based Causal Structure Learning with Unknown Intervention Targets},
  author={Squires, C. and Wang, Y. and Uhler, C.},
  journal={Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence},
  year={2020}
}

@article{JCI_jmlr,
  author  = {Mooij, J. M. and Magliacane, S. and Claassen, T.},
  title   = {Joint Causal Inference from Multiple Contexts},
  journal = {Journal of Machine Learning Research},
  year    = {2020}
}

@inproceedings{eaton2007exact,
  title={Exact Bayesian structure learning from uncertain interventions},
  author={Eaton, D. and Murphy, K.},
  booktitle={Artificial Intelligence and Statistics},
  year={2007}
}

@incollection{NIPS2019_9581,
title = {Characterization and Learning of Causal Graphs with Latent Variables from Soft Interventions},
author = {Kocaoglu, M. and Jaber, A. and Shanmugam, K. and Bareinboim, E.},
booktitle = {Advances in Neural Information Processing Systems 32},
year = {2019}
}

@inproceedings{NEURIPS2020_6cd9313e,
 author = {Jaber, A. and Kocaoglu, M. and Shanmugam, K. and Bareinboim, E.},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Causal Discovery from Soft Interventions with Unknown Targets: Characterization and Learning},
 year = {2020}
}

% causality
@book{pearl2009causality,
  title={Causality},
  author={Pearl, J.},
  year={2009},
  publisher={Cambridge university press}
}

@book{peters2017elements,
  title = {Elements of Causal Inference - Foundations and Learning Algorithms},
  author = {Peters, J. and Janzing, D. and Sch{\"o}lkopf, B.},
  publisher = {MIT Press},
  year = {2017}
}

@article{Pearl2018TheSP,
 title = {The Seven Tools of Causal Inference, with Reflections on Machine Learning},
 author = {Pearl, J.},
 journal = {Commun. ACM},
 year = {2019}
}

% Causality + disentanglement
@misc{scholkopf2019causality,
      title={Causality for Machine Learning},
      author={Schölkopf, B.},
      year={2019},
      journal={arXiv preprint arXiv:1911.10500}
}

@article{scholkopf2021causal,
  title = {Toward Causal Representation Learning},
  author = {Sch{\"o}lkopf, B. and Locatello, F. and Bauer, S. and Ke, N. R. and Kalchbrenner, N. and Goyal, A. and Bengio, Y.},
  journal = {Proceedings of the IEEE - Advances in Machine Learning and Deep Neural Networks},
  year = {2021}
}

@article{InducGoyal2021,
  author    = {Goyal, A. and
               Bengio, Y.},
  title     = {Inductive Biases for Deep Learning of Higher-Level Cognition},
  journal    = {Proc. R. Soc. A 478: 20210068},
  year      = {2022}
}

@misc{CITRIS,
  author = {Lippe, P. and Magliacane, S. and Löwe, S. and Asano, Y. M. and Cohen, T. and Gavves, E.},
  title = {{CITRIS}: Causal Identifiability from Temporal Intervened Sequences},
  year = {2022},
  journal={arXiv preprint arXiv:2202.03169}
}

@inproceedings{
lippe2022icitris,
title={i{CITRIS}: Causal Representation Learning for Instantaneous Temporal Effects},
author={P. Lippe and S. Magliacane and S. L{\"o}we and Y. M Asano and T. Cohen and E. Gavves},
booktitle={UAI 2022 Workshop on Causal Representation Learning},
year={2022}
}


% measure theory
@book{pollard_2001,
title={A User's Guide to Measure Theoretic Probability},
publisher={Cambridge University Press},
author={Pollard, D.},
year={2001}
}

% Variational autoencoders
@inproceedings{Kingma2014,
  booktitle = {2nd International Conference on Learning Representations},
  author = {Kingma, D. P. and Welling, M.},
  title = {Auto-Encoding Variational Bayes},
  year = 2014
}

@inproceedings{Higgins2017betaVAELB,
  title={beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework},
  author={Higgins, I. and Matthey, L. and Pal, A. and Burgess, C. P. and Glorot, X. and Botvinick, M. and Mohamed, S. and Lerchner, A.},
  booktitle={ICLR},
  year={2017}
}

% Optimization
@inproceedings{Adam,
  author    = {Kingma, D. P. and
               Ba, J.},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations},
  year      = {2015}
}

% Gumbel softmax
@article{jang2016categorical,
  title={Categorical Reparameterization with Gumbel-Softmax},
  author={Jang, E. and Gu, S. and Poole, B.},
  journal={Proceedings of the 34th International Conference on Machine Learning},
  year={2017}
}

@article{maddison2016concrete,
  title={The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables},
  author={Maddison, C. J. and Mnih, A. and Teh, Y. W.},
  journal={Proceedings of the 34th International Conference on Machine Learning},
  year={2017}
}

% NN stuff
@InProceedings{pmlr-v9-glorot10a,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, X. and Bengio, Y.},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  year = 	 {2010}
}

% Graphical models
@article{WainwrightJordan08,
author = {Wainwright, M. J. and Jordan, M. I.},
title = {Graphical Models, Exponential Families, and Variational Inference},
year = {2008},
journal = {Found. Trends Mach. Learn.}
}

@article{ke2021systematic,
  title={Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning},
  author={Ke, Nan Rosemary and Didolkar, Aniket Rajiv and Mittal, Sarthak and Goyal, Anirudh and Lajoie, Guillaume and Bauer, Stefan and Rezende, Danilo Jimenez and Mozer, Michael Curtis and Bengio, Yoshua and Pal, Christopher},
  year={2021}
}

@article{goyal2019recurrent,
  title={Recurrent independent mechanisms},
  author={A. Goyal and A. Lamb and J. Hoffmann and S. Sodhani and S. Levine and Y. Bengio and B. Sch{\"o}lkopf},
  journal={arXiv preprint arXiv:1909.10893},
  year={2019}
}

% Deep Learning
@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Goodfellow, I and Bengio, Y. and Courville, A.},
    publisher={MIT Press},
    year={2016}
}




@InProceedings{batchnorm2015,
  title = 	 {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author = 	 {Ioffe, S. and Szegedy, C.},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  year = 	 {2015}
}



@article{Bohning1992,
  title={Multinomial logistic regression algorithm},
  author={B{\"o}hning, D},
  journal={Annals of the institute of Statistical Mathematics},
  volume={44},
  number={1},
  pages={197--200},
  year={1992},
  publisher={Springer}
}

@inproceedings{Lee_Maji_Ravichandran_Soatto2019meta,
  title={Meta-learning with differentiable convex optimization},
  author={K. Lee and S.Maji and A. Ravichandran and S. Soatto},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10657--10665},
  year={2019}
}

@article{Crammer_Singer2001,
  title={On the algorithmic implementation of multiclass kernel-based vector machines},
  author={K. Crammer and Y. Singer},
  journal={Journal of machine learning research},
  volume={2},
  number={Dec},
  pages={265--292},
  year={2001}
}

@article{Tseng2001,
  title={Convergence of a block coordinate descent method for nondifferentiable minimization},
  author={P. Tseng},
  journal={Journal of optimization theory and applications},
  volume={109},
  number={3},
  pages={475--494},
  year={2001},
  publisher={Springer}
}

@inproceedings{Bertrand_Massias2021,
  title={Anderson acceleration of coordinate descent},
  author={Q. Bertrand and M. Massias},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1288--1296},
  year={2021},
  organization={PMLR}
}

@article{jaxopt,
  title={Efficient and modular implicit differentiation},
  author={M. Blondel and Q. Berthet and M. Cuturi and R. Frostig and S. Hoyer and F. Llinares-L{\'o}pez and F. Pedregosa and J.-P. Vert},
  journal={NeurIPS},
  year={2022}
}

@software{jax2018github,
  author = {J. Bradbury and R. Frostig and P. Hawkins and M. James Johnson and C. Leary and D. Maclaurin and G. Necula and A. Paszke and J. Vander{P}las and S. Wanderman-{M}ilne and Q. Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  year = {2018},
}

@inproceedings{Tiomoko_Couillet2019,
  title={M. Tiomoko and R. Couillet and F. Bouchard and G. Ginolhac},
  booktitle={ICML},
  pages={6254--6263},
  year={2019},
  organization={PMLR}
}

@inproceedings{
lachapelle2022disentanglement,
title={Disentanglement via Mechanism Sparsity Regularization: A New Principle for Nonlinear {ICA}},
author={Lachapelle, S. and Rodriguez Lopez, P. and Sharma, Y. and Everett, K. E. and {Le Priol}, R. and Lacoste, A. and Lacoste-Julien, S.},
booktitle={First Conference on Causal Learning and Reasoning},
year={2022}
}

@inproceedings{
lachapelle2022partial,
title={Partial Disentanglement via Mechanism Sparsity},
author={S. Lachapelle and S. Lacoste-Julien},
booktitle={UAI 2022 Workshop on Causal Representation Learning},
year={2022}
}

@inproceedings{
zheng2022on,
title={On the Identifiability of Nonlinear {ICA}: Sparsity and Beyond},
author={Y. Zheng and I. Ng and K. Zhang},
booktitle={Advances in Neural Information Processing Systems},
year={2022}
}

@article{
moran2022identifiable,
title={Identifiable Deep Generative Models via Sparse Decoding},
author={G. E. Moran and D. Sridhar and Y. Wang and D. Blei},
journal={Transactions on Machine Learning Research},
year={2022}
}



@misc{ahuja2022sparse,
  author = {Ahuja, K. and Hartford, J. and Bengio, Y.},
  title = {Weakly Supervised Representation Learning with Sparse Perturbations},
  journal={arXiv preprint arXiv:2206.01101},
  year = {2022}
}

@inproceedings{ahuja2022properties,
  title = {Properties from mechanisms: an equivariance perspective on identifiable representation learning},
  author = {Ahuja, K. and Hartford, J. and Bengio, Y.},
  booktitle = {International Conference on Learning Representations},
  year = {2022}
}


@inproceedings{bertinetto2018r2d2,
  title={Meta-learning with differentiable closed-form solvers},
  author={L. Bertinetto and J. F. Henriques and P. HS Torr and Vedaldi, Andrea},
  journal={International Conference on Learning Representations},
  year={2019}
}

@article{vinyals2016matchingnet,
  title={Matching networks for one shot learning},
  author={Vinyals, O. and Blundell, C. and Lillicrap, T. and Wierstra, D. and others},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{bengio2000implicitdiff,
  title={Gradient-based optimization of hyperparameters},
  author={Y. Bengio},
  journal={Neural computation},
  year={2000},
  publisher={MIT Press}
}

@inproceedings{lorraine2020million,
  title={Optimizing millions of hyperparameters by implicit differentiation},
  author={J. Lorraine and P. Vicol and D. Duvenaud},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1540--1552},
  year={2020},
  organization={PMLR}
}

@article{rajeswaran2019imaml,
  title={Meta-learning with implicit gradients},
  author={A. Rajeswaran and C. Finn and S. M. Kakade and S. Levine},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{snell2017protonet,
  title={Prototypical networks for few-shot learning},
  author={J. Snell and K. Swersky and R. Zemel},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{hospedales2021metalearningsurvey,
  title={Meta-learning in neural networks: A survey},
  author={Hospedales, T. and Antoniou, A. and Micaelli, P. and Storkey, A.},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={44},
  number={9},
  pages={5149--5169},
  year={2021},
  publisher={IEEE}
}

@article{brown2020gpt3,
  title={Language models are few-shot learners},
  author={Brown, T. and Mann, B. and Ryder, N. and Subbiah, M. and Kaplan, J. D and Dhariwal, P. and Neelakantan, A. and Shyam, P. and Sastry, G. and Askell, A. and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{dosovitskiy2020vit,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, A. and Beyer, L. and Kolesnikov, A. and Weissenborn, D. and Zhai, X. and Unterthiner, T. and Dehghani, M. and Minderer, M. and Heigold, G. and Gelly, S. and others},
  journal={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{chen2020simclr,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, T. and Kornblith, S. and Norouzi, M. and Hinton, G.},
  booktitle={International Conference on Machine Learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@inproceedings{radford2021clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, A. and Kim, J. W. and Hallacy, C. and Ramesh, A. and Goh, G. and Agarwal, S. and Sastry, G. and Askell, A. and Mishkin, P. and Clark, J. and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{ramesh2022dalle2,
  author = {Ramesh, A. and Dhariwal, P. and Nichol, A. and Chu, C. and Chen, M.},
  title = {Hierarchical Text-Conditional Image Generation with CLIP Latents},
  journal={arXiv preprint arXiv:2204.06125},
  year = {2022}
}

@inproceedings{lachapelle2023synergies,
  title={Synergies between Disentanglement and Sparsity: Generalization and Identifiability in Multi-Task Learning},
  author={Lachapelle, S{\'e}bastien and Deleu, Tristan and Mahajan, Divyat and Mitliagkas, Ioannis and Bengio, Yoshua and Lacoste-Julien, Simon and Bertrand, Quentin},
  booktitle={International Conference on Machine Learning},
  pages={18171--18206},
  year={2023},
  organization={PMLR}
}
@book{pearl1988probabilistic,
	title        = {{Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference}},
	author       = {Pearl, Judea},
	year         = 1988,
	publisher    = {Morgan Kaufmann Publishers Inc.},
	address      = {San Francisco, CA, USA},
	isbn         = {0934613737}
}
@book{pearl2009causality,
	title        = {{Causality: Models, Reasoning and Inference}},
	author       = {Pearl, Judea},
	year         = 2009,
	publisher    = {Cambridge University Press},
	address      = {USA},
	isbn         = {052189560X},
	edition      = {2nd}
}
@article{peters2015structural,
	title        = {{Structural Intervention Distance (SID) for Evaluating Causal Graphs}},
	author       = {Peters, Jonas and B{\"u}hlmann, Peters},
	year         = 2015,
	journal      = {Neural Computation},
	volume       = 27,
	number       = 3
}
@article{peters2016causal,
	title        = {{Causal inference by using invariant prediction: identification and confidence intervals}},
	author       = {Peters, Jonas and B{\"u}hlmann, Peter and Meinshausen, Nicolai},
	year         = 2016,
	journal      = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
	publisher    = {JSTOR}
}
@book{peters2017elements,
	title        = {{Elements of Causal Inference: Foundations and Learning Algorithms}},
	author       = {Peters, Jonas and Janzing, Dominik and Schlkopf, Bernhard},
	year         = 2017,
	publisher    = {The MIT Press},
	isbn         = {0262037319},
	
}


@InProceedings{pmlr-v115-tonolini20a,
  title = 	 {Variational Sparse Coding},
  author =       {Tonolini, Francesco and Jensen, Bj{\o}rn Sand and Murray-Smith, Roderick},
  booktitle = 	 {Proceedings of The 35th Uncertainty in Artificial Intelligence Conference},
  pages = 	 {690--700},
  year = 	 {2020},
  editor = 	 {Adams, Ryan P. and Gogate, Vibhav},
  volume = 	 {115},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {22--25 Jul},
  publisher =    {PMLR},
 
}

@book{Spirtes_2000,
	title        = {{Causation, Prediction, and Search}},
	author       = {Spirtes, Peter and Glymour, Clark and Scheines, Richard},
	year         = 2000,
	publisher    = {MIT Press},
	address      = {Cambridge MA},
	added-at     = {2016-11-26T13:19:29.000+0100},
	date-added   = {2008-05-16 16:46:46 -0700},
	date-modified = {2008-05-16 16:48:00 -0700},
	edition      = {2nd},
	interhash    = {559e17fcd12a76214629ba6c4efe3f9a},
	intrahash    = {e2b107e8fd3469c8b0e944ca37a559f3},
	keywords     = {imported ml}
}


@article{kivva2022identifiability,
  title={Identifiability of deep generative models without auxiliary information},
  author={Kivva, Bohdan and Rajendran, Goutham and Ravikumar, Pradeep and Aragam, Bryon},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15687--15701},
  year={2022}
}


@article{teicher1963identifiability,
  title={Identifiability of finite mixtures},
  author={Teicher, Henry},
  journal={The annals of Mathematical statistics},
  pages={1265--1269},
  year={1963},
  publisher={JSTOR}
}

@article{yakowitz1968identifiability,
  title={On the identifiability of finite mixtures},
  author={Yakowitz, Sidney J and Spragins, John D},
  journal={The Annals of Mathematical Statistics},
  volume={39},
  number={1},
  pages={209--214},
  year={1968},
  publisher={Institute of Mathematical Statistics}
}

@book{devlin2012joy,
  title={The joy of sets: fundamentals of contemporary set theory},
  author={Devlin, Keith},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{tarieladze2007disintegration,
  title={Disintegration of Gaussian measures and average-case optimal algorithms},
  author={Tarieladze, Vaja and Vakhania, Nicholas},
  journal={Journal of Complexity},
  volume={23},
  number={4-6},
  pages={851--866},
  year={2007},
  publisher={Elsevier}
}



@article{scholkopf2021toward,
  title={Toward causal representation learning},
  author={Sch{\"o}lkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  journal={Proceedings of the IEEE},
  volume={109},
  number={5},
  pages={612--634},
  year={2021},
  publisher={IEEE}
}

@article{bengio2013representation,
  title={Representation learning: A review and new perspectives},
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={8},
  pages={1798--1828},
  year={2013},
  publisher={IEEE}
}


@inproceedings{lachapelle2023synergies,
  title={Synergies between disentanglement and sparsity: Generalization and identifiability in multi-task learning},
  author={Lachapelle, S{\'e}bastien and Deleu, Tristan and Mahajan, Divyat and Mitliagkas, Ioannis and Bengio, Yoshua and Lacoste-Julien, Simon and Bertrand, Quentin},
  booktitle={International Conference on Machine Learning},
  pages={18171--18206},
  year={2023},
  organization={PMLR}
}

@misc{zbontar_barlow_2021,
	title = {Barlow {Twins}: {Self}-{Supervised} {Learning} via {Redundancy} {Reduction}},
	shorttitle = {Barlow {Twins}},
	url = {http://arxiv.org/abs/2103.03230},
	abstract = {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stéphane},
	month = jun,
	year = {2021},
	note = {arXiv:2103.03230 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
}


@inproceedings{
oublal2024disentangling,
title={Disentangling Time Series Representations via Contrastive Independence-of-Support on l-Variational Inference},
author={Khalid Oublal and Said Ladjal and David Benhaiem and Emmanuel LE BORGNE and Fran{\c{c}}ois Roueff},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=iI7hZSczxE}
}

@article{tschannen2018recent,
  title={Recent advances in autoencoder-based representation learning},
  author={Tschannen, Michael and Bachem, Olivier and Lucic, Mario},
  journal={arXiv preprint arXiv:1812.05069},
  year={2018}
}

@inproceedings{Hyvrinen2017NonlinearIO,
 author = {Aapo Hyv{\"{a}}rinen and
Hiroshi Morioka},
 booktitle = {{AISTATS}},
 pages = {460--469},
 series = {Proceedings of Machine Learning Research},
 title = {Nonlinear {ICA} of Temporally Dependent Stationary Sources},
 volume = {54},
 year = {2017}
}

@inproceedings{klindt2020towards,
 author = {David A. Klindt and
Lukas Schott and
Yash Sharma and
Ivan Ustyuzhaninov and
Wieland Brendel and
Matthias Bethge and
Dylan M. Paiton},
 booktitle = {{ICLR}},
 title = {Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse
Coding},
 year = {2021}
}


@inproceedings{zimmermann2021contrastive,
 author = {Roland S. Zimmermann and
Yash Sharma and
Steffen Schneider and
Matthias Bethge and
Wieland Brendel},
 booktitle = {{ICML}},
 pages = {12979--12990},
 series = {Proceedings of Machine Learning Research},
 title = {Contrastive Learning Inverts the Data Generating Process},
 volume = {139},
 year = {2021}
}


@inproceedings{halva2020hidden,
 author = {Hermanni H{\"{a}}lv{\"{a}} and
Aapo Hyv{\"{a}}rinen},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/uai/HalvaH20.bib},
 booktitle = {Proceedings of the Thirty-Sixth Conference on Uncertainty in Artificial
Intelligence, {UAI} 2020, virtual online, August 3-6, 2020},
 pages = {939--948},
 series = {Proceedings of Machine Learning Research},
 title = {Hidden Markov Nonlinear {ICA:} Unsupervised Learning from Nonstationary
Time Series},
 volume = {124},
 year = {2020}
}


@inproceedings{hyvarinen2019nonlinear,
  title={Nonlinear ICA using auxiliary variables and generalized contrastive learning},
  author={Hyvarinen, Aapo and Sasaki, Hiroaki and Turner, Richard},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={859--868},
  year={2019},
  organization={PMLR}
}



@article{hyvarinen2016unsupervised,
  title={Unsupervised feature extraction by time-contrastive learning and nonlinear ica},
  author={Hyvarinen, Aapo and Morioka, Hiroshi},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{locatello2020weakly,
  title={Weakly-supervised disentanglement without compromises},
  author={Locatello, Francesco and Poole, Ben and R{\"a}tsch, Gunnar and Sch{\"o}lkopf, Bernhard and Bachem, Olivier and Tschannen, Michael},
  booktitle={International Conference on Machine Learning},
  pages={6348--6359},
  year={2020},
  organization={PMLR}
}

@article{gresele2021independent,
  title={Independent mechanism analysis, a new concept?},
  author={Gresele, Luigi and von K{\"u}gelgen, Julius and Stimper, Vincent and Sch{\"o}lkopf, Bernhard and Besserve, Michel},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={28233--28248},
  year={2021}
}








@inproceedings{sparseCodingForMTL2013,
author = {Maurer, A. and Pontil, M. and Romera-Paredes, B.},
title = {Sparse Coding for Multitask and Transfer Learning},
year = {2013},
series = {ICML'13}
}



@article{marcus2022preliminary,
  author = {Marcus, G. and Davis, E. and Aaronson, S.},
  title = {A very preliminary analysis of DALL-E 2},
  journal={arXiv preprint arXiv:2204.13807},
  year = {2022}
}


@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, J. and Chang, M.-W. and Lee, K. and Toutanova, K.},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{andreassen2021robustness,
  title={The evolution of out-of-distribution robustness throughout fine-tuning},
  author={Andreassen, A. and Bahri, Y. and Neyshabur, B. and Roelofs, R.},
  journal={arXiv preprint arXiv:2106.15831},
  year={2021}
}

@InProceedings{wortsman2022wiseft,
    author    = {Wortsman, M. and Ilharco, G. and Kim, J. W. and Li, M. and Kornblith, S. and Roelofs, R. and Lopes, R. G. and Hajishirzi, H. and Farhadi, A. and Namkoong, H. and Schmidt, L.},
    title     = {Robust Fine-Tuning of Zero-Shot Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {7959-7971}
}

% Usefulness of disentanglement
@article{miladinovic2019disentangledODE,
  author = {Miladinović, D. and Gondal, M. W. and Schölkopf, B. and Buhmann, J. M. and Bauer, S.},
  title = {Disentangled State Space Representations},
  journal = {arXiv preprint arXiv:1906.03255},
  year = {2019}
}

@inproceedings{steenkiste2019DisForAbstractReasoning,
 author = {van Steenkiste, S. and Locatello, F. and Schmidhuber, J. and Bachem, O.},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Are Disentangled Representations Helpful for Abstract Visual Reasoning?},
 year = {2019}
}

@inproceedings{
dittadi2021sim2real_dis,
title={On the Transfer of Disentangled Representations in Realistic Settings},
author={A. Dittadi and F. Tr{\"a}uble and F. Locatello and M. Wuthrich and V. Agrawal and O. Winther and S. Bauer and B. Sch{\"o}lkopf},
booktitle={International Conference on Learning Representations},
year={2021}
}

@inproceedings{
montero2021disGen,
title={The role of Disentanglement in Generalisation},
author={M. L. Montero and C. JH Ludwig and R. P. Costa and G. Malhotra and J. Bowers},
booktitle={International Conference on Learning Representations},
year={2021}
}

@conference{Zhangetal22b,
  title = {Towards Principled Disentanglement for Domain Generalization},
  author = {Zhang, H. and Zhang, Y.-F. and Liu, W. and Weller, A. and Sch{\"o}lkopf, B. and Xing, E.},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2022}
}

% Generalization bounds + statistical machine learning
@book{MohriRostamizadehTalwalkar18,
  author = {Mohri, M. and Rostamizadeh, A. and Talwalkar, A.},
  publisher = {MIT Press},
  year = 2018
}


% dataset

@misc{3dshapes18,
  title={3D Shapes Dataset},
  author={Burgess, Chris and Kim, Hyunjik},
  howpublished={https://github.com/deepmind/3dshapes-dataset/},
  year={2018}
}

@inproceedings{mikolov2010recurrent,
  added-at = {2015-04-09T00:00:00.000+0200},
  author = {Mikolov, T. and Karafiát, M. and Burget, L. and Cernocký, J. and Khudanpur, S.},
  publisher = {ISCA},
  title = {Recurrent neural network based language model},
  year = 2010
}


@book{CaseBerg2001,
  author = {Casella, G. and Berger, R.},
  publisher = {{Duxbury Resource Center}},
  title = {Statistical Inference},
  year = 2001
}

@book{Boyd_Vandenberghe2004,
  title={Convex optimization},
  author={S. P. Boyd and and L. Vandenberghe},
  year={2004},
  publisher={Cambridge university press}
}

@article{Chang_Lin2011,
  title={LIBSVM: a library for support vector machines},
  author={C.-C. Chang and C.-L. Lin},
  journal={ACM transactions on intelligent systems and technology (TIST)},
  volume={2},
  number={3},
  pages={1--27},
  year={2011},
  publisher={Acm New York, NY, USA}
}

@article{Chen_Lin_Scholkopf2005,
  title={A tutorial on $\nu$-support vector machines},
  author={P.-H. Chen and C.-J. Lin and B. Sch{\"o}lkopf},
  journal={Applied Stochastic Models in Business and Industry},
  volume={21},
  number={2},
  pages={111--136},
  year={2005},
  publisher={Wiley Online Library}
}
@article{Bottou_Lin2007,
  title={Support vector machine solvers},
  author={L. Bottou and C.-J. Lin},
  journal={Large scale kernel machines},
  volume={3},
  number={1},
  pages={301--320},
  year={2007},
  publisher={MIT press Cambridge, MA}
}

@inproceedings{Hsieh2008,
  title={A dual coordinate descent method for large-scale linear SVM},
  author={C.-J. Hsieh and K.-W. Chang and C.-J. Lin and S. S. Keerthi and S. Sundararajan},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={408--415},
  year={2008}
}

@article{Shalev_Zhang2012,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization},
  author={S. Shalev-Shwartz and T. Zhang},
  journal={The Journal of Machine Learning Research},
  year={2012}
}

@inproceedings{Finn_Abbel_Levine2017,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={C. Finn and P. Abbeel and S. Levine},
  booktitle={International conference on machine learning},
  pages={1126--1135},
  year={2017},
  organization={PMLR}
}


@article{scholkopf2021toward,
  title={Toward causal representation learning},
  author={Sch{\"o}lkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  journal={Proceedings of the IEEE},
  volume={109},
  number={5},
  pages={612--634},
  year={2021},
  publisher={IEEE}
}

@article{bengio2013representation,
  title={Representation learning: A review and new perspectives},
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={8},
  pages={1798--1828},
  year={2013},
  publisher={IEEE}
}

@inproceedings{locatello2019challenging,
  title={Challenging common assumptions in the unsupervised learning of disentangled representations},
  author={Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Raetsch, Gunnar and Gelly, Sylvain and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
  booktitle={international conference on machine learning},
  pages={4114--4124},
  year={2019},
  organization={PMLR}
}

@article{tschannen2018recent,
  title={Recent advances in autoencoder-based representation learning},
  author={Tschannen, Michael and Bachem, Olivier and Lucic, Mario},
  journal={arXiv preprint arXiv:1812.05069},
  year={2018}
}



@inproceedings{hyvarinen2017nonlinear,
  title={Nonlinear ICA of temporally dependent stationary sources},
  author={Hyvarinen, Aapo and Morioka, Hiroshi},
  booktitle={Artificial Intelligence and Statistics},
  pages={460--469},
  year={2017},
  organization={PMLR}
}



@inproceedings{khemakhem2020variational,
  title={Variational autoencoders and nonlinear ica: A unifying framework},
  author={Khemakhem, Ilyes and Kingma, Diederik and Monti, Ricardo and Hyvarinen, Aapo},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2207--2217},
  year={2020},
  organization={PMLR}
}

@inproceedings{lippe2022citris,
  title={Citris: Causal identifiability from temporal intervened sequences},
  author={Lippe, Phillip and Magliacane, Sara and L{\"o}we, Sindy and Asano, Yuki M and Cohen, Taco and Gavves, Stratis},
  booktitle={International Conference on Machine Learning},
  pages={13557--13603},
  year={2022},
  organization={PMLR}
}

@article{scholkopf2012causal,
  title={On causal and anticausal learning},
  author={Sch{\"o}lkopf, Bernhard and Janzing, Dominik and Peters, Jonas and Sgouritsa, Eleni and Zhang, Kun and Mooij, Joris},
  journal={arXiv preprint arXiv:1206.6471},
  year={2012}
}

@article{lippe2023biscuit,
  title={BISCUIT: Causal Representation Learning from Binary Interactions},
  author={Lippe, Phillip and Magliacane, Sara and L{\"o}we, Sindy and Asano, Yuki M and Cohen, Taco and Gavves, Efstratios},
  journal={Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
  year={2023}
}

@inproceedings{ahuja2023interventional,
  title={Interventional causal representation learning},
  author={Ahuja, Kartik and Mahajan, Divyat and Wang, Yixin and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={372--407},
  year={2023},
  organization={PMLR}
}

@inproceedings{von2023nonparametric,
  title={Nonparametric Identifiability of Causal Representations from Unknown Interventions},
  author={von K{\"u}gelgen, Julius and Besserve, Michel and Liang, Wendong and Gresele, Luigi and Keki{\'c}, Armin and Bareinboim, Elias and Blei, David M and Sch{\"o}lkopf, Bernhard},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}

@inproceedings{Liang2023cca,
title={Causal Component Analysis},
author={Liang Wendong and Armin Keki\'c and Julius von K\"ugelgen and Simon Buchholz and Michel Besserve and Luigi Gresele and Bernhard Sch\"olkopf},
year={2023},
booktitle={Advances in Neural Information Processing Systems},
}

@article{moran2022identifiable,
  title={Identifiable Deep Generative Models via Sparse Decoding},
  author={Moran, G and Sridhar, D and Wang, Y and Blei, D},
  journal={Transactions on machine learning research},
  year={2022}
}

@article{perry2022causal,
  title={Causal discovery in heterogeneous environments under the sparse mechanism shift hypothesis},
  author={Perry, Ronan and Von K{\"u}gelgen, Julius and Sch{\"o}lkopf, Bernhard},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={10904--10917},
  year={2022}
}

@article{fumero2023leveraging,
  title={Leveraging sparse and shared feature activations for disentangled representation learning},
  author={Fumero, Marco and Wenzel, Florian and Zancato, Luca and Achille, Alessandro and Rodol{\`a}, Emanuele and Soatto, Stefano and Sch{\"o}lkopf, Bernhard and Locatello, Francesco},
  journal={arXiv preprint arXiv:2304.07939},
  year={2023}
}

@inproceedings{zhang2023identifiability,
title={Identifiability Guarantees for Causal Disentanglement from Soft Interventions},
author={Jiaqi Zhang and Kristjan Greenewald and Chandler Squires and Akash Srivastava and Karthikeyan Shanmugam and Caroline Uhler},
booktitle={Advances in Neural Information Processing Systems},
year={2023},
}

@inproceedings{ahuja2023multi,
  title={Multi-Domain Causal Representation Learning via Weak Distributional Invariances},
  author={Ahuja, Kartik and Mansouri, Amin and Wang, Yixin},
  booktitle={Causal Representation Learning Workshop at NeurIPS 2023},
  year={2023}
}

@inproceedings{squires2023linear,
  title={Linear Causal Disentanglement via Interventions}, 
  author={Chandler Squires and Anna Seigal and Salil Bhate and Caroline Uhler},
  year={2023},
  booktitle={40th International Conference on Machine Learning},
}

@inproceedings{buchholz2023learning,
  title={Learning Linear Causal Representations from Interventions under General Nonlinear Mixing},
  author={Buchholz, Simon and Rajendran, Goutham and Rosenfeld, Elan and Aragam, Bryon},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}

@inproceedings{ke2023learning,
  title={Learning Disentangled Causal Representations via Principal-Mechanism Analysis},
  author={Ke, Nan Rosemary and Bengio, Yoshua and Goyal, Anirudh},
  booktitle={Conference on Causal Learning and Reasoning},
  pages={368--396},
  year={2023},
  organization={PMLR}
}






%%%% CORRECTED VERSION SEE ABOVE
@inproceedings{kim_unsupervised_2011,
	title = {Unsupervised {Disaggregation} of {Low} {Frequency} {Power} {Measurements}},
	isbn = {978-0-89871-992-5 978-1-61197-281-8},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611972818.64},
	doi = {10.1137/1.9781611972818.64},
	language = {en},
	urldate = {2023-11-15},
	booktitle = {Proceedings of the 2011 {SIAM} {International} {Conference} on {Data} {Mining}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Kim, Hyungsul and Marwah, Manish and Arlitt, Martin and Lyon, Geoff and Han, Jiawei},
	month = apr,
	year = {2011},
	pages = {747--758},
}

@misc{noauthor_unsupervised_nodate,
	title = {Unsupervised {Disaggregation} of {Low} {Frequency} {Power} {Measurements}},
	url = {https://epubs.siam.org/doi/epdf/10.1137/1.9781611972818.64},
	language = {en},
	urldate = {2023-11-14},
	note = {ISBN: 9781611972818},
}

@inproceedings{trauble_disentangled_2021,
	title = {On {Disentangled} {Representations} {Learned} from {Correlated} {Data}},
	url = {https://proceedings.mlr.press/v139/trauble21a.html},
	abstract = {The focus of disentanglement approaches has been on identifying independent factors of variation in data. However, the causal variables underlying real-world observations are often not statistically independent. In this work, we bridge the gap to real-world scenarios by analyzing the behavior of the most prominent disentanglement approaches on correlated data in a large-scale empirical study (including 4260 models). We show and quantify that systematically induced correlations in the dataset are being learned and reflected in the latent representations, which has implications for downstream applications of disentanglement such as fairness. We also demonstrate how to resolve these latent correlations, either using weak supervision during training or by post-hoc correcting a pre-trained model with a small number of labels.},
	language = {en},
	urldate = {2023-11-13},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Träuble, Frederik and Creager, Elliot and Kilbertus, Niki and Locatello, Francesco and Dittadi, Andrea and Goyal, Anirudh and Schölkopf, Bernhard and Bauer, Stefan},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {10401--10412},
}

@misc{xia_causal-neural_2022,
	title = {The {Causal}-{Neural} {Connection}: {Expressiveness}, {Learnability}, and {Inference}},
	shorttitle = {The {Causal}-{Neural} {Connection}},
	url = {http://arxiv.org/abs/2107.00793},
	abstract = {One of the central elements of any causal inference is an object called structural causal model (SCM), which represents a collection of mechanisms and exogenous sources of random variation of the system under investigation (Pearl, 2000). An important property of many kinds of neural networks is universal approximability: the ability to approximate any function to arbitrary precision. Given this property, one may be tempted to surmise that a collection of neural nets is capable of learning any SCM by training on data generated by that SCM. In this paper, we show this is not the case by disentangling the notions of expressivity and learnability. Specifically, we show that the causal hierarchy theorem (Thm. 1, Bareinboim et al., 2020), which describes the limits of what can be learned from data, still holds for neural models. For instance, an arbitrarily complex and expressive neural net is unable to predict the effects of interventions given observational data alone. Given this result, we introduce a special type of SCM called a neural causal model (NCM), and formalize a new type of inductive bias to encode structural constraints necessary for performing causal inferences. Building on this new class of models, we focus on solving two canonical tasks found in the literature known as causal identification and estimation. Leveraging the neural toolbox, we develop an algorithm that is both sufficient and necessary to determine whether a causal effect can be learned from data (i.e., causal identifiability); it then estimates the effect whenever identifiability holds (causal estimation). Simulations corroborate the proposed approach.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Xia, Kevin and Lee, Kai-Zhan and Bengio, Yoshua and Bareinboim, Elias},
	month = oct,
	year = {2022},
	note = {arXiv:2107.00793 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{wang_desiderata_2022,
	title = {Desiderata for {Representation} {Learning}: {A} {Causal} {Perspective}},
	shorttitle = {Desiderata for {Representation} {Learning}},
	url = {http://arxiv.org/abs/2109.03795},
	abstract = {Representation learning constructs low-dimensional representations to summarize essential features of high-dimensional data. This learning problem is often approached by describing various desiderata associated with learned representations; e.g., that they be non-spurious, efficient, or disentangled. It can be challenging, however, to turn these intuitive desiderata into formal criteria that can be measured and enhanced based on observed data. In this paper, we take a causal perspective on representation learning, formalizing non-spuriousness and efficiency (in supervised representation learning) and disentanglement (in unsupervised representation learning) using counterfactual quantities and observable consequences of causal assertions. This yields computable metrics that can be used to assess the degree to which representations satisfy the desiderata of interest and learn non-spurious and disentangled representations from single observational datasets.},
	urldate = {2023-11-10},
	publisher = {arXiv},
	author = {Wang, Yixin and Jordan, Michael I.},
	month = feb,
	year = {2022},
	note = {arXiv:2109.03795 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@inproceedings{ahuja_interventional_2023,
	title = {Interventional {Causal} {Representation} {Learning}},
	url = {https://proceedings.mlr.press/v202/ahuja23a.html},
	abstract = {Causal representation learning seeks to extract high-level latent factors from low-level sensory data. Most existing methods rely on observational data and structural assumptions (e.g., conditional independence) to identify the latent factors. However, interventional data is prevalent across applications. Can interventional data facilitate causal representation learning? We explore this question in this paper. The key observation is that interventional data often carries geometric signatures of the latent factors’ support (i.e. what values each latent can possibly take). For example, when the latent factors are causally connected, interventions can break the dependency between the intervened latents’ support and their ancestors’. Leveraging this fact, we prove that the latent causal factors can be identified up to permutation and scaling given data from perfect do interventions. Moreover, we can achieve block affine identification, namely the estimated latent factors are only entangled with a few other latents if we have access to data from imperfect interventions. These results highlight the unique power of interventional data in causal representation learning; they can enable provable identification of latent factors without any assumptions about their distributions or dependency structure.},
	language = {en},
	urldate = {2023-11-10},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ahuja, Kartik and Mahajan, Divyat and Wang, Yixin and Bengio, Yoshua},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {372--407},
}

@misc{chen_isolating_2019,
	title = {Isolating {Sources} of {Disentanglement} in {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/1802.04942},
	abstract = {We decompose the evidence lower bound to show the existence of a term measuring the total correlation between latent variables. We use this to motivate our \${\textbackslash}beta\$-TCVAE (Total Correlation Variational Autoencoder), a refinement of the state-of-the-art \${\textbackslash}beta\$-VAE objective for learning disentangled representations, requiring no additional hyperparameters during training. We further propose a principled classifier-free measure of disentanglement called the mutual information gap (MIG). We perform extensive quantitative and qualitative experiments, in both restricted and non-restricted settings, and show a strong relation between total correlation and disentanglement, when the latent variables model is trained using our framework.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Chen, Ricky T. Q. and Li, Xuechen and Grosse, Roger and Duvenaud, David},
	month = apr,
	year = {2019},
	note = {arXiv:1802.04942 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{zimmermann_contrastive_2022,
	title = {Contrastive {Learning} {Inverts} the {Data} {Generating} {Process}},
	url = {http://arxiv.org/abs/2102.08850},
	abstract = {Contrastive learning has recently seen tremendous success in self-supervised learning. So far, however, it is largely unclear why the learned representations generalize so effectively to a large variety of downstream tasks. We here prove that feedforward models trained with objectives belonging to the commonly used InfoNCE family learn to implicitly invert the underlying generative model of the observed data. While the proofs make certain statistical assumptions about the generative model, we observe empirically that our findings hold even if these assumptions are severely violated. Our theory highlights a fundamental connection between contrastive learning, generative modeling, and nonlinear independent component analysis, thereby furthering our understanding of the learned representations as well as providing a theoretical foundation to derive more effective contrastive losses.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Zimmermann, Roland S. and Sharma, Yash and Schneider, Steffen and Bethge, Matthias and Brendel, Wieland},
	month = apr,
	year = {2022},
	note = {arXiv:2102.08850 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{bai_contrastively_nodate,
	title = {Contrastively {Disentangled} {Sequential} {Variational} {Autoencoder}},
	abstract = {Self-supervised disentangled representation learning is a critical task in sequence modeling. The learnt representations contribute to better model interpretability as well as the data generation, and improve the sample efﬁciency for downstream tasks. We propose a novel sequence representation learning method, named Contrastively Disentangled Sequential Variational Autoencoder (C-DSVAE), to extract and separate the static (time-invariant) and dynamic (time-variant) factors in the latent space. Different from previous sequential variational autoencoder methods, we use a novel evidence lower bound which maximizes the mutual information between the input and the latent factors, while penalizes the mutual information between the static and dynamic factors. We leverage contrastive estimations of the mutual information terms in training, together with simple yet effective augmentation techniques, to introduce additional inductive biases. Our experiments show that C-DSVAE signiﬁcantly outperforms the previous state-of-the-art methods on multiple metrics.},
	language = {en},
	author = {Bai, Junwen and Wang, Weiran and Gomes, Carla},
}

@misc{kopiczko_vera_2023,
	title = {{VeRA}: {Vector}-based {Random} {Matrix} {Adaptation}},
	shorttitle = {{VeRA}},
	url = {http://arxiv.org/abs/2310.11454},
	abstract = {Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous per-user or per-task adapted models. In this work, we present Vector-based Random Matrix Adaptation (VeRA), which reduces the number of trainable parameters by 10x compared to LoRA, yet maintains the same performance. It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead. We demonstrate its effectiveness on the GLUE and E2E benchmarks, and show its application in instruction-following with just 1.4M parameters using the Llama2 7B model.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Kopiczko, Dawid Jan and Blankevoort, Tijmen and Asano, Yuki Markus},
	month = oct,
	year = {2023},
	note = {arXiv:2310.11454 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{camps-valls_discovering_2023,
	title = {Discovering {Causal} {Relations} and {Equations} from {Data}},
	url = {http://arxiv.org/abs/2305.13341},
	abstract = {Physics is a field of science that has traditionally used the scientific method to answer questions about why natural phenomena occur and to make testable models that explain the phenomena. Discovering equations, laws and principles that are invariant, robust and causal explanations of the world has been fundamental in physical sciences throughout the centuries. Discoveries emerge from observing the world and, when possible, performing interventional studies in the system under study. With the advent of big data and the use of data-driven methods, causal and equation discovery fields have grown and made progress in computer science, physics, statistics, philosophy, and many applied fields. All these domains are intertwined and can be used to discover causal relations, physical laws, and equations from observational data. This paper reviews the concepts, methods, and relevant works on causal and equation discovery in the broad field of Physics and outlines the most important challenges and promising future lines of research. We also provide a taxonomy for observational causal and equation discovery, point out connections, and showcase a complete set of case studies in Earth and climate sciences, fluid dynamics and mechanics, and the neurosciences. This review demonstrates that discovering fundamental laws and causal relations by observing natural phenomena is being revolutionised with the efficient exploitation of observational data, modern machine learning algorithms and the interaction with domain knowledge. Exciting times are ahead with many challenges and opportunities to improve our understanding of complex systems.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Camps-Valls, Gustau and Gerhardus, Andreas and Ninad, Urmi and Varando, Gherardo and Martius, Georg and Balaguer-Ballester, Emili and Vinuesa, Ricardo and Diaz, Emiliano and Zanna, Laure and Runge, Jakob},
	month = may,
	year = {2023},
	note = {arXiv:2305.13341 [physics, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability, Statistics - Methodology},
}

@misc{camps-valls_discovering_2023-1,
	title = {Discovering {Causal} {Relations} and {Equations} from {Data}},
	url = {http://arxiv.org/abs/2305.13341},
	abstract = {Physics is a field of science that has traditionally used the scientific method to answer questions about why natural phenomena occur and to make testable models that explain the phenomena. Discovering equations, laws and principles that are invariant, robust and causal explanations of the world has been fundamental in physical sciences throughout the centuries. Discoveries emerge from observing the world and, when possible, performing interventional studies in the system under study. With the advent of big data and the use of data-driven methods, causal and equation discovery fields have grown and made progress in computer science, physics, statistics, philosophy, and many applied fields. All these domains are intertwined and can be used to discover causal relations, physical laws, and equations from observational data. This paper reviews the concepts, methods, and relevant works on causal and equation discovery in the broad field of Physics and outlines the most important challenges and promising future lines of research. We also provide a taxonomy for observational causal and equation discovery, point out connections, and showcase a complete set of case studies in Earth and climate sciences, fluid dynamics and mechanics, and the neurosciences. This review demonstrates that discovering fundamental laws and causal relations by observing natural phenomena is being revolutionised with the efficient exploitation of observational data, modern machine learning algorithms and the interaction with domain knowledge. Exciting times are ahead with many challenges and opportunities to improve our understanding of complex systems.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Camps-Valls, Gustau and Gerhardus, Andreas and Ninad, Urmi and Varando, Gherardo and Martius, Georg and Balaguer-Ballester, Emili and Vinuesa, Ricardo and Diaz, Emiliano and Zanna, Laure and Runge, Jakob},
	month = may,
	year = {2023},
	note = {arXiv:2305.13341 [physics, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability, Statistics - Methodology},
}

@misc{davidson_hyperspherical_2022,
	title = {Hyperspherical {Variational} {Auto}-{Encoders}},
	url = {http://arxiv.org/abs/1804.00891},
	abstract = {The Variational Auto-Encoder (VAE) is one of the most used unsupervised machine learning models. But although the default choice of a Gaussian distribution for both the prior and posterior represents a mathematically convenient distribution often leading to competitive results, we show that this parameterization fails to model data with a latent hyperspherical structure. To address this issue we propose using a von Mises-Fisher (vMF) distribution instead, leading to a hyperspherical latent space. Through a series of experiments we show how such a hyperspherical VAE, or \${\textbackslash}mathcal\{S\}\$-VAE, is more suitable for capturing data with a hyperspherical latent structure, while outperforming a normal, \${\textbackslash}mathcal\{N\}\$-VAE, in low dimensions on other data types. Code at http://github.com/nicola-decao/s-vae-tf and https://github.com/nicola-decao/s-vae-pytorch},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Davidson, Tim R. and Falorsi, Luca and De Cao, Nicola and Kipf, Thomas and Tomczak, Jakub M.},
	month = sep,
	year = {2022},
	note = {arXiv:1804.00891 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{mattei_miwae_2019,
	title = {{MIWAE}: {Deep} {Generative} {Modelling} and {Imputation} of {Incomplete} {Data}},
	shorttitle = {{MIWAE}},
	url = {http://arxiv.org/abs/1812.02633},
	abstract = {We consider the problem of handling missing data with deep latent variable models (DLVMs). First, we present a simple technique to train DLVMs when the training set contains missing-at-random data. Our approach, called MIWAE, is based on the importance-weighted autoencoder (IWAE), and maximises a potentially tight lower bound of the log-likelihood of the observed data. Compared to the original IWAE, our algorithm does not induce any additional computational overhead due to the missing data. We also develop Monte Carlo techniques for single and multiple imputation using a DLVM trained on an incomplete data set. We illustrate our approach by training a convolutional DLVM on a static binarisation of MNIST that contains 50\% of missing pixels. Leveraging multiple imputation, a convolutional network trained on these incomplete digits has a test performance similar to one trained on complete data. On various continuous and binary data sets, we also show that MIWAE provides accurate single imputations, and is highly competitive with state-of-the-art methods.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Mattei, Pierre-Alexandre and Frellsen, Jes},
	month = feb,
	year = {2019},
	note = {arXiv:1812.02633 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@article{rahimpour_non-intrusive_2017,
	title = {Non-{Intrusive} {Energy} {Disaggregation} {Using} {Non}-negative {Matrix} {Factorization} with {Sum}-to-k {Constraint}},
	volume = {32},
	issn = {0885-8950, 1558-0679},
	url = {http://arxiv.org/abs/1704.07308},
	doi = {10.1109/TPWRS.2017.2660246},
	abstract = {Energy disaggregation or Non-Intrusive Load Monitoring (NILM) addresses the issue of extracting device-level energy consumption information by monitoring the aggregated signal at one single measurement point without installing meters on each individual device. Energy disaggregation can be formulated as a source separation problem where the aggregated signal is expressed as linear combination of basis vectors in a matrix factorization framework. In this paper, an approach based on Sum-to-k constrained Non-negative Matrix Factorization (S2K-NMF) is proposed. By imposing the sum-to-k constraint and the non-negative constraint, S2K-NMF is able to effectively extract perceptually meaningful sources from complex mixtures. The strength of the proposed algorithm is demonstrated through two sets of experiments: Energy disaggregation in a residential smart home, and HVAC components energy monitoring in an industrial building testbed maintained at the Oak Ridge National Laboratory (ORNL). Extensive experimental results demonstrate the superior performance of S2K-NMF as compared to state-of-the-art decomposition-based disaggregation algorithms. The source code and our collected data (HVORUT) for studying NILM for HVAC units can be found at https://bitbucket.org/aicip/nonintrusive-load-monitoring.},
	number = {6},
	urldate = {2023-11-08},
	journal = {IEEE Transactions on Power Systems},
	author = {Rahimpour, Alireza and Qi, Hairong and Fugate, David and Kuruganti, Teja},
	month = nov,
	year = {2017},
	note = {arXiv:1704.07308 [cs]},
	keywords = {Computer Science - Computational Engineering, Finance, and Science},
	pages = {4430--4441},
}

@misc{kingma_auto-encoding_2022,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2022},
	note = {arXiv:1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{yeche_neighborhood_2021,
	title = {Neighborhood {Contrastive} {Learning} {Applied} to {Online} {Patient} {Monitoring}},
	url = {https://proceedings.mlr.press/v139/yeche21a.html},
	abstract = {Intensive care units (ICU) are increasingly looking towards machine learning for methods to provide online monitoring of critically ill patients. In machine learning, online monitoring is often formulated as a supervised learning problem. Recently, contrastive learning approaches have demonstrated promising improvements over competitive supervised benchmarks. These methods rely on well-understood data augmentation techniques developed for image data which do not apply to online monitoring. In this work, we overcome this limitation by supplementing time-series data augmentation techniques with a novel contrastive learning objective which we call neighborhood contrastive learning (NCL). Our objective explicitly groups together contiguous time segments from each patient while maintaining state-specific information. Our experiments demonstrate a marked improvement over existing work applying contrastive methods to medical time-series.},
	language = {en},
	urldate = {2023-11-08},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yèche, Hugo and Dresdner, Gideon and Locatello, Francesco and Hüser, Matthias and Rätsch, Gunnar},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {11964--11974},
}

@article{cha_orthogonality-enforced_nodate,
	title = {Orthogonality-{Enforced} {Latent} {Space} in {Autoencoders}: {An} {Approach} to {Learning} {Disentangled} {Representations}},
	abstract = {Noting the importance of factorizing (or disentangling) the latent space, we propose a novel, non-probabilistic disentangling framework for autoencoders, based on the principles of symmetry transformations that are independent of one another. To the best of our knowledge, this is the first deterministic model that is aiming to achieve disentanglement based on autoencoders using only a reconstruction loss without pairs of images or labels, by explicitly introducing inductive biases into a model architecture through Euler encoding. The proposed model is then compared with a number of state-of-the-art models, relevant to disentanglement, including symmetry-based models and generative models. Our evaluation using six different disentanglement metrics, including the unsupervised disentanglement metric we propose here in this paper, shows that the proposed model can offer better disentanglement, especially when variances of the features are different, where other methods may struggle. We believe that this model opens several opportunities for linear disentangled representation learning based on deterministic autoencoders.},
	language = {en},
	author = {Cha, Jaehoon and Thiyagalingam, Jeyan},
}


@article{yao2022temporally,
  title={Temporally disentangled representation learning},
  author={Yao, Weiran and Chen, Guangyi and Zhang, Kun},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={26492--26503},
  year={2022}
}

@inproceedings{yue2022ts2vec,
  title={Ts2vec: Towards universal representation of time series},
  author={Yue, Zhihan and Wang, Yujing and Duan, Juanyong and Yang, Tianmeng and Huang, Congrui and Tong, Yunhai and Xu, Bixiong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={8},
  pages={8980--8987},
  year={2022}
}


@article{ruff2021unifying,
  title={A unifying review of deep and shallow anomaly detection},
  author={Ruff, Lukas and Kauffmann, Jacob R and Vandermeulen, Robert A and Montavon, Gr{\'e}goire and Samek, Wojciech and Kloft, Marius and Dietterich, Thomas G and M{\"u}ller, Klaus-Robert},
  journal={Proceedings of the IEEE},
  volume={109},
  number={5},
  pages={756--795},
  year={2021},
  publisher={IEEE}
}

@misc{davidson_hyperspherical_2022-1,
	title = {Hyperspherical {Variational} {Auto}-{Encoders}},
	url = {http://arxiv.org/abs/1804.00891},
	abstract = {The Variational Auto-Encoder (VAE) is one of the most used unsupervised machine learning models. But although the default choice of a Gaussian distribution for both the prior and posterior represents a mathematically convenient distribution often leading to competitive results, we show that this parameterization fails to model data with a latent hyperspherical structure. To address this issue we propose using a von Mises-Fisher (vMF) distribution instead, leading to a hyperspherical latent space. Through a series of experiments we show how such a hyperspherical VAE, or \${\textbackslash}mathcal\{S\}\$-VAE, is more suitable for capturing data with a hyperspherical latent structure, while outperforming a normal, \${\textbackslash}mathcal\{N\}\$-VAE, in low dimensions on other data types. Code at http://github.com/nicola-decao/s-vae-tf and https://github.com/nicola-decao/s-vae-pytorch},
	urldate = {2023-11-07},
	publisher = {arXiv},
	author = {Davidson, Tim R. and Falorsi, Luca and De Cao, Nicola and Kipf, Thomas and Tomczak, Jakub M.},
	month = sep,
	year = {2022},
	note = {arXiv:1804.00891 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{domke_importance_2018,
	title = {Importance {Weighting} and {Variational} {Inference}},
	url = {http://arxiv.org/abs/1808.09034},
	abstract = {Recent work used importance sampling ideas for better variational bounds on likelihoods. We clarify the applicability of these ideas to pure probabilistic inference, by showing the resulting Importance Weighted Variational Inference (IWVI) technique is an instance of augmented variational inference, thus identifying the looseness in previous work. Experiments confirm IWVI's practicality for probabilistic inference. As a second contribution, we investigate inference with elliptical distributions, which improves accuracy in low dimensions, and convergence in high dimensions.},
	urldate = {2023-11-07},
	publisher = {arXiv},
	author = {Domke, Justin and Sheldon, Daniel},
	month = oct,
	year = {2018},
	note = {arXiv:1808.09034 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{alvarez_melis_towards_2018,
	title = {Towards {Robust} {Interpretability} with {Self}-{Explaining} {Neural} {Networks}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html},
	abstract = {Most recent work on interpretability of complex machine learning models has focused on estimating a-posteriori explanations for previously trained models around specific predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general -- explicitness, faithfulness, and stability -- and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.},
	urldate = {2023-11-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Alvarez Melis, David and Jaakkola, Tommi},
	year = {2018},
}

@inproceedings{alvarez_melis_towards_2018-1,
	title = {Towards {Robust} {Interpretability} with {Self}-{Explaining} {Neural} {Networks}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html},
	abstract = {Most recent work on interpretability of complex machine learning models has focused on estimating a-posteriori explanations for previously trained models around specific predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general -- explicitness, faithfulness, and stability -- and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.},
	urldate = {2023-11-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Alvarez Melis, David and Jaakkola, Tommi},
	year = {2018},
}

@misc{noauthor_carbon_nodate,
	title = {The carbon footprint of household energy use in the {United} {States}},
	url = {https://www.pnas.org/doi/10.1073/pnas.1922205117},
	language = {en},
	urldate = {2023-10-21},
	note = {ISBN: 9781922205117},
}

@article{booth_us_2010,
	title = {U.{S}. smart grid value at stake: {The} \$130 billion question},
	language = {en},
	author = {Booth, Adrian and Greene, Mike and Tai, Humayun},
	year = {2010},
}

@article{noauthor_chapter_nodate,
	title = {Chapter 5: {Increasing} {Efficiency} of {Building} {Systems} and {Technologies}},
	language = {en},
}

@misc{noauthor_neurips_nodate,
	title = {{NeurIPS} 2023 {Workshop} {UniReps} {Reviewers}},
	url = {https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/UniReps/Reviewers},
	abstract = {Welcome to the OpenReview homepage for NeurIPS 2023 Workshop UniReps Reviewers},
	language = {en},
	urldate = {2023-10-19},
	journal = {OpenReview},
}

@misc{wang_desiderata_2022-1,
	title = {Desiderata for {Representation} {Learning}: {A} {Causal} {Perspective}},
	shorttitle = {Desiderata for {Representation} {Learning}},
	url = {http://arxiv.org/abs/2109.03795},
	abstract = {Representation learning constructs low-dimensional representations to summarize essential features of high-dimensional data. This learning problem is often approached by describing various desiderata associated with learned representations; e.g., that they be non-spurious, efficient, or disentangled. It can be challenging, however, to turn these intuitive desiderata into formal criteria that can be measured and enhanced based on observed data. In this paper, we take a causal perspective on representation learning, formalizing non-spuriousness and efficiency (in supervised representation learning) and disentanglement (in unsupervised representation learning) using counterfactual quantities and observable consequences of causal assertions. This yields computable metrics that can be used to assess the degree to which representations satisfy the desiderata of interest and learn non-spurious and disentangled representations from single observational datasets.},
	urldate = {2023-10-19},
	publisher = {arXiv},
	author = {Wang, Yixin and Jordan, Michael I.},
	month = feb,
	year = {2022},
	note = {arXiv:2109.03795 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@misc{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://arxiv.org/abs/2002.05709},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	urldate = {2023-10-17},
	publisher = {arXiv},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {arXiv:2002.05709 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{oord_representation_2019,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	urldate = {2023-10-17},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	month = jan,
	year = {2019},
	note = {arXiv:1807.03748 [cs, stat]
version: 2},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{davidson_hyperspherical_2022-2,
	title = {Hyperspherical {Variational} {Auto}-{Encoders}},
	url = {http://arxiv.org/abs/1804.00891},
	doi = {10.48550/arXiv.1804.00891},
	abstract = {The Variational Auto-Encoder (VAE) is one of the most used unsupervised machine learning models. But although the default choice of a Gaussian distribution for both the prior and posterior represents a mathematically convenient distribution often leading to competitive results, we show that this parameterization fails to model data with a latent hyperspherical structure. To address this issue we propose using a von Mises-Fisher (vMF) distribution instead, leading to a hyperspherical latent space. Through a series of experiments we show how such a hyperspherical VAE, or \${\textbackslash}mathcal\{S\}\$-VAE, is more suitable for capturing data with a hyperspherical latent structure, while outperforming a normal, \${\textbackslash}mathcal\{N\}\$-VAE, in low dimensions on other data types. Code at http://github.com/nicola-decao/s-vae-tf and https://github.com/nicola-decao/s-vae-pytorch},
	urldate = {2023-10-13},
	publisher = {arXiv},
	author = {Davidson, Tim R. and Falorsi, Luca and De Cao, Nicola and Kipf, Thomas and Tomczak, Jakub M.},
	month = sep,
	year = {2022},
	note = {arXiv:1804.00891 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{jeon_gt-gan_2022,
	title = {{GT}-{GAN}: {General} {Purpose} {Time} {Series} {Synthesis} with {Generative} {Adversarial} {Networks}},
	shorttitle = {{GT}-{GAN}},
	url = {http://arxiv.org/abs/2210.02040},
	doi = {10.48550/arXiv.2210.02040},
	abstract = {Time series synthesis is an important research topic in the field of deep learning, which can be used for data augmentation. Time series data types can be broadly classified into regular or irregular. However, there are no existing generative models that show good performance for both types without any model changes. Therefore, we present a general purpose model capable of synthesizing regular and irregular time series data. To our knowledge, we are the first designing a general purpose time series synthesis model, which is one of the most challenging settings for time series synthesis. To this end, we design a generative adversarial network-based method, where many related techniques are carefully integrated into a single framework, ranging from neural ordinary/controlled differential equations to continuous time-flow processes. Our method outperforms all existing methods.},
	urldate = {2023-10-13},
	publisher = {arXiv},
	author = {Jeon, Jinsung and Kim, Jeonghak and Song, Haryong and Cho, Seunghyeon and Park, Noseong},
	month = oct,
	year = {2022},
	note = {arXiv:2210.02040 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{noauthor_monthly_2023,
	title = {Monthly {Energy} {Review} - {September} 2023},
	language = {en},
	year = {2023},
}

@article{makonin_rae_2018,
	title = {{RAE}: {The} {Rainforest} {Automation} {Energy} {Dataset} for {Smart} {Grid} {Meter} {Data} {Analysis}},
	volume = {3},
	issn = {2306-5729},
	shorttitle = {{RAE}},
	url = {http://arxiv.org/abs/1705.05767},
	doi = {10.3390/data3010008},
	abstract = {Datasets are important for researchers to build models and test how well their machine learning algorithms perform. This paper presents the Rainforest Automation Energy (RAE) dataset to help smart grid researchers test their algorithms which make use of smart meter data. This initial release of RAE contains 1Hz data (mains and sub-meters) from two a residential house. In addition to power data, environmental and sensor data from the house's thermostat is included. Sub-meter data from one of the houses includes heat pump and rental suite captures which is of interest to power utilities. We also show and energy breakdown of each house and show (by example) how RAE can be used to test non-intrusive load monitoring (NILM) algorithms.},
	number = {1},
	urldate = {2023-10-12},
	journal = {Data},
	author = {Makonin, Stephen and Wang, Z. Jane and Tumpach, Chris},
	month = feb,
	year = {2018},
	note = {arXiv:1705.05767 [cs]},
	keywords = {Computer Science - Other Computer Science},
	pages = {8},
}

@misc{wang_desiderata_2022-2,
	title = {Desiderata for {Representation} {Learning}: {A} {Causal} {Perspective}},
	shorttitle = {Desiderata for {Representation} {Learning}},
	url = {http://arxiv.org/abs/2109.03795},
	abstract = {Representation learning constructs low-dimensional representations to summarize essential features of high-dimensional data. This learning problem is often approached by describing various desiderata associated with learned representations; e.g., that they be non-spurious, efficient, or disentangled. It can be challenging, however, to turn these intuitive desiderata into formal criteria that can be measured and enhanced based on observed data. In this paper, we take a causal perspective on representation learning, formalizing non-spuriousness and efficiency (in supervised representation learning) and disentanglement (in unsupervised representation learning) using counterfactual quantities and observable consequences of causal assertions. This yields computable metrics that can be used to assess the degree to which representations satisfy the desiderata of interest and learn non-spurious and disentangled representations from single observational datasets.},
	urldate = {2023-10-12},
	publisher = {arXiv},
	author = {Wang, Yixin and Jordan, Michael I.},
	month = feb,
	year = {2022},
	note = {arXiv:2109.03795 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@misc{trauble_disentangled_2021-1,
	title = {On {Disentangled} {Representations} {Learned} {From} {Correlated} {Data}},
	url = {http://arxiv.org/abs/2006.07886},
	abstract = {The focus of disentanglement approaches has been on identifying independent factors of variation in data. However, the causal variables underlying real-world observations are often not statistically independent. In this work, we bridge the gap to real-world scenarios by analyzing the behavior of the most prominent disentanglement approaches on correlated data in a large-scale empirical study (including 4260 models). We show and quantify that systematically induced correlations in the dataset are being learned and reflected in the latent representations, which has implications for downstream applications of disentanglement such as fairness. We also demonstrate how to resolve these latent correlations, either using weak supervision during training or by post-hoc correcting a pre-trained model with a small number of labels.},
	urldate = {2023-10-12},
	publisher = {arXiv},
	author = {Träuble, Frederik and Creager, Elliot and Kilbertus, Niki and Locatello, Francesco and Dittadi, Andrea and Goyal, Anirudh and Schölkopf, Bernhard and Bauer, Stefan},
	month = jul,
	year = {2021},
	note = {arXiv:2006.07886 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{trauble_disentangled_nodate,
  title={On disentangled representations learned from correlated data},
  author={Tr{\"a}uble, Frederik and Creager, Elliot and Kilbertus, Niki and Locatello, Francesco and Dittadi, Andrea and Goyal, Anirudh and Sch{\"o}lkopf, Bernhard and Bauer, Stefan},
  booktitle={International Conference on Machine Learning},
  pages={10401--10412},
  year={2021},
  organization={PMLR}
}

@article{oublal_temporal_2023,
	series = {Workshop},
	title = {Temporal {Attention} {Bottleneck} is informative?  {Interpretability} through {Disentangled} {Generative} {Representations} for {Time} {Series} {Disaggregation}},
	abstract = {Generative models have garnered significant attention for their ability to address the challenge of source separation in disaggregation tasks. Energy Disaggregation holds promise for promoting energy conservation by allowing homeowners to gain comprehensive insights into their energy consumption solely through the interpretation of aggregated load curves. Nevertheless, the model’s ability to generalize and its interpretability remain two major challenges. To tackle these challenges, we deploy a generative model called TAB-VAE (Temporal Attention Bottleneck for Variational Auto-encoder), based on hierarchical architecture, addresses signature variability, and provides a robust, interpretable separation through the design of its informative representation of latent space. Our implementation and evaluation guidelines are available at https://github.com/ oublalkhalid/TAB-VAE.},
	language = {en},
	journal = {ICML 2023},
	author = {Oublal, Khalid and Ladjal, Saïd and Roueff, François and Benhaiem, David},
	year = {2023},
}

@article{higgins_learning_2017,
	title = {{LEARNING} {BASIC} {VISUAL} {CONCEPTS} {WITH} {A} {CONSTRAINED} {VARIATIONAL} {FRAMEWORK}},
	abstract = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artiﬁcial intelligence that is able to learn and reason in the same way that humans do. We introduce β-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modiﬁcation of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter β that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that β-VAE with appropriately tuned β {\textgreater} 1 qualitatively outperforms VAE (β = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also signiﬁcantly outperforms all baselines quantitatively. Unlike InfoGAN, β-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter β, which can be directly optimised through a hyperparameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.},
	language = {en},
	author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
	year = {2017},
}

@article{noauthor_notitle_nodate,
}

@misc{le_parisien_italie_2023,
	title = {Italie : un conducteur de {Ferrari} provoque un accident mortel lors d’un rallye de voitures de luxe},
	shorttitle = {Italie},
	url = {https://www.youtube.com/watch?v=c_R7uZCMCYU},
	abstract = {Deux touristes suisses sont morts carbonisés au volant d’une Ferrari louée à l’occasion d’un rallye de voitures de luxe, en Sardaigne. L’accident s’est produit sur une route à double sens, en Sardaigne,  
alors que le couple tentait un dépassement,  il a heurté cette Lamborghini, qui a ensuite percuté un camping-car. L'accident s'est produit sur une route qui comporte plusieurs tronçons où les dépassements sont interdits.},
	urldate = {2023-10-04},
	author = {{Le Parisien}},
	month = oct,
	year = {2023},
}

@inproceedings{chung_recurrent_2015,
	title = {A {Recurrent} {Latent} {Variable} {Model} for {Sequential} {Data}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper_files/paper/2015/hash/b618c3210e934362ac261db280128c22-Abstract.html},
	abstract = {In this paper, we explore the inclusion of latent random variables into the hidden state of a recurrent neural network (RNN) by combining the elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN) can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against other related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamics.},
	urldate = {2023-09-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron C and Bengio, Yoshua},
	year = {2015},
}

@misc{elfwing_sigmoid-weighted_2017,
	title = {Sigmoid-{Weighted} {Linear} {Units} for {Neural} {Network} {Function} {Approximation} in {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1702.03118},
	doi = {10.48550/arXiv.1702.03118},
	abstract = {In recent years, neural networks have enjoyed a renaissance as function approximators in reinforcement learning. Two decades after Tesauro's TD-Gammon achieved near top-level human performance in backgammon, the deep reinforcement learning algorithm DQN achieved human-level performance in many Atari 2600 games. The purpose of this study is twofold. First, we propose two activation functions for neural network function approximation in reinforcement learning: the sigmoid-weighted linear unit (SiLU) and its derivative function (dSiLU). The activation of the SiLU is computed by the sigmoid function multiplied by its input. Second, we suggest that the more traditional approach of using on-policy learning with eligibility traces, instead of experience replay, and softmax action selection with simple annealing can be competitive with DQN, without the need for a separate target network. We validate our proposed approach by, first, achieving new state-of-the-art results in both stochastic SZ-Tetris and Tetris with a small 10\${\textbackslash}times\$10 board, using TD(\${\textbackslash}lambda\$) learning and shallow dSiLU network agents, and, then, by outperforming DQN in the Atari 2600 domain by using a deep Sarsa(\${\textbackslash}lambda\$) agent with SiLU and dSiLU hidden units.},
	urldate = {2023-09-29},
	publisher = {arXiv},
	author = {Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
	month = nov,
	year = {2017},
	note = {arXiv:1702.03118 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{lea_temporal_2016,
	title = {Temporal {Convolutional} {Networks}: {A} {Unified} {Approach} to {Action} {Segmentation}},
	shorttitle = {Temporal {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1608.08242},
	abstract = {The dominant paradigm for video-based action segmentation is composed of two steps: first, for each frame, compute low-level features using Dense Trajectories or a Convolutional Neural Network that encode spatiotemporal information locally, and second, input these features into a classifier that captures high-level temporal relationships, such as a Recurrent Neural Network (RNN). While often effective, this decoupling requires specifying two separate models, each with their own complexities, and prevents capturing more nuanced long-range spatiotemporal relationships. We propose a unified approach, as demonstrated by our Temporal Convolutional Network (TCN), that hierarchically captures relationships at low-, intermediate-, and high-level time-scales. Our model achieves superior or competitive performance using video or sensor data on three public action segmentation datasets and can be trained in a fraction of the time it takes to train an RNN.},
	urldate = {2023-09-29},
	publisher = {arXiv},
	author = {Lea, Colin and Vidal, Rene and Reiter, Austin and Hager, Gregory D.},
	month = aug,
	year = {2016},
	note = {arXiv:1608.08242 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{yue_bert4nilm_2020,
	address = {New York, NY, USA},
	series = {{NILM}'20},
	title = {{BERT4NILM}: {A} {Bidirectional} {Transformer} {Model} for {Non}-{Intrusive} {Load} {Monitoring}},
	isbn = {978-1-4503-8191-8},
	shorttitle = {{BERT4NILM}},
	url = {https://dl.acm.org/doi/10.1145/3427771.3429390},
	doi = {10.1145/3427771.3429390},
	abstract = {Non-intrusive load monitoring (NILM) based energy disaggregation is the decomposition of a system's energy into the consumption of its individual appliances. Previous work on deep learning NILM algorithms has shown great potential in the field of energy management and smart grids. In this paper, we propose BERT4NILM, an architecture based on bidirectional encoder representations from transformers (BERT) and an improved objective function designed specifically for NILM learning. We adapt the bidirectional transformer architecture to the field of energy disaggregation and follow the pattern of sequence-to-sequence learning. With the improved loss function and masked training, BERT4NILM outperforms state-of-the-art models across various metrics on the two publicly available datasets UK-DALE and REDD.},
	urldate = {2023-09-29},
	booktitle = {Proceedings of the 5th {International} {Workshop} on {Non}-{Intrusive} {Load} {Monitoring}},
	publisher = {Association for Computing Machinery},
	author = {Yue, Zhenrui and Witzig, Camilo Requena and Jorde, Daniel and Jacobsen, Hans-Arno},
	month = nov,
	year = {2020},
	keywords = {Deep Learning, Energy Disaggregation, NILM, Neural Network, Non-Intrusive Load Monitoring, Transformer},
	pages = {89--93},
}

@article{kingma_adam_2014,
	title = {Adam: {A} method for stochastic optimization},
	journal = {arXiv preprint arXiv:1412.6980},
	author = {Kingma, Diederik P and Ba, Jimmy},
	year = {2014},
}

@article{yang_sequence_2021,
	title = {Sequence to {Point} {Learning} {Based} on an {Attention} {Neural} {Network} for {Nonintrusive} {Load} {Decomposition}},
	journal = {Electronics},
	author = {Yang, Mingzhi and Li, Xinchun and Liu, Yue},
	year = {2021},
}

@article{chen_convolutional_2018,
	title = {Convolutional sequence to sequence non-intrusive load monitoring},
	volume = {2018},
	number = {17},
	journal = {the Journal of Engineering},
	author = {Chen, Kunjin and Wang, Qin and He, Ziyu and Chen, Kunlong and Hu, Jun and He, Jinliang},
	year = {2018},
	note = {Publisher: Wiley Online Library},
	pages = {1860--1864},
}

@article{ciancetta_new_2021,
	title = {A {New} {Convolutional} {Neural} {Network}-{Based} {System} for {NILM} {Applications}},
	doi = {10.1109/TIM.2020.3035193},
	journal = {IEEE Transactions on Instrumentation and Measurement},
	author = {Ciancetta, Fabrizio and Bucci, Giovanni and Fiorucci, Edoardo and Mari, Simone and Fioravanti, Andrea},
	year = {2021},
}

@inproceedings{vahdat_nvae_2020,
	title = {{NVAE}: {A} {Deep} {Hierarchical} {Variational} {Autoencoder}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Vahdat, Arash and Kautz, Jan},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
}

@inproceedings{chen_isolating_2018,
	title = {Isolating {Sources} of {Disentanglement} in {Variational} {Autoencoders}},
	volume = {31},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Ricky T. Q. and Li, Xuechen and Grosse, Roger B and Duvenaud, David K},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
}

@article{kelly_uk-dale_2015,
	title = {The {UK}-{DALE} dataset, domestic appliance-level electricity demand and whole-house demand from five {UK} homes},
	volume = {2},
	journal = {Scientific data},
	author = {Kelly, Jack and Knottenbelt, William},
	year = {2015},
	note = {Publisher: Nature Publishing Group},
}


@inproceedings{valenti_exploiting_2018,
	title = {Exploiting the reactive power in deep neural models for non-intrusive load monitoring},
	booktitle = {2018 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Valenti, Michele and Bonfigli, Roberto and Principi, Emanuele and Squartini, Stefano},
	year = {2018},
}

@misc{zimmermann_contrastive_2022-1,
	title = {Contrastive {Learning} {Inverts} the {Data} {Generating} {Process}},
	url = {http://arxiv.org/abs/2102.08850},
	doi = {10.48550/arXiv.2102.08850},
	abstract = {Contrastive learning has recently seen tremendous success in self-supervised learning. So far, however, it is largely unclear why the learned representations generalize so effectively to a large variety of downstream tasks. We here prove that feedforward models trained with objectives belonging to the commonly used InfoNCE family learn to implicitly invert the underlying generative model of the observed data. While the proofs make certain statistical assumptions about the generative model, we observe empirically that our findings hold even if these assumptions are severely violated. Our theory highlights a fundamental connection between contrastive learning, generative modeling, and nonlinear independent component analysis, thereby furthering our understanding of the learned representations as well as providing a theoretical foundation to derive more effective contrastive losses.},
	urldate = {2023-09-29},
	publisher = {arXiv},
	author = {Zimmermann, Roland S. and Sharma, Yash and Schneider, Steffen and Bethge, Matthias and Brendel, Wieland},
	month = apr,
	year = {2022},
	note = {arXiv:2102.08850 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{zimmermann_contrastive_nodate,
	title = {Contrastive {Learning} {Inverts} the {Data} {Generating} {Process}},
	language = {en},
	author = {Zimmermann, Roland S and Sharma, Yash and Schneider, Steffen and Bethge, Matthias and Brendel, Wieland},
}

@misc{bengio_representation_2014,
	title = {Representation {Learning}: {A} {Review} and {New} {Perspectives}},
	shorttitle = {Representation {Learning}},
	url = {http://arxiv.org/abs/1206.5538},
	doi = {10.48550/arXiv.1206.5538},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	month = apr,
	year = {2014},
	note = {arXiv:1206.5538 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{wang_correlated_2023,
	title = {Correlated {Time} {Series} {Self}-{Supervised} {Representation} {Learning} via {Spatiotemporal} {Bootstrapping}},
	url = {http://arxiv.org/abs/2306.06994},
	abstract = {Correlated time series analysis plays an important role in many real-world industries. Learning an efficient representation of this large-scale data for further downstream tasks is necessary but challenging. In this paper, we propose a time-step-level representation learning framework for individual instances via bootstrapped spatiotemporal representation prediction. We evaluated the effectiveness and flexibility of our representation learning framework on correlated time series forecasting and cold-start transferring the forecasting model to new instances with limited data. A linear regression model trained on top of the learned representations demonstrates our model performs best in most cases. Especially compared to representation learning models, we reduce the RMSE, MAE, and MAPE by 37\%, 49\%, and 48\% on the PeMS-BAY dataset, respectively. Furthermore, in real-world metro passenger flow data, our framework demonstrates the ability to transfer to infer future information of new cold-start instances, with gains of 15\%, 19\%, and 18\%. The source code will be released under the GitHub https://github.com/bonaldli/Spatiotemporal-TS-Representation-Learning},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Wang, Luxuan and Bai, Lei and Li, Ziyue and Zhao, Rui and Tsung, Fugee},
	month = jun,
	year = {2023},
	note = {arXiv:2306.06994 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{yang_causalvae_2021,
	address = {Nashville, TN, USA},
	title = {{CausalVAE}: {Disentangled} {Representation} {Learning} via {Neural} {Structural} {Causal} {Models}},
	isbn = {978-1-66544-509-2},
	shorttitle = {{CausalVAE}},
	url = {https://ieeexplore.ieee.org/document/9578520/},
	doi = {10.1109/CVPR46437.2021.00947},
	abstract = {Learning disentanglement aims at ﬁnding a low dimensional representation which consists of multiple explanatory and generative factors of the observational data. The framework of variational autoencoder (VAE) is commonly used to disentangle independent factors from observations. However, in real scenarios, factors with semantics are not necessarily independent. Instead, there might be an underlying causal structure which renders these factors dependent. We thus propose a new VAE based framework named CausalVAE, which includes a Causal Layer to transform independent exogenous factors into causal endogenous ones that correspond to causally related concepts in data. We further analyze the model identiﬁabitily, showing that the proposed model learned from observations recovers the true one up to a certain degree. Experiments are conducted on various datasets, including synthetic and real word benchmark CelebA. Results show that the causal representations learned by CausalVAE are semantically interpretable, and their causal relationship as a Directed Acyclic Graph (DAG) is identiﬁed with good accuracy. Furthermore, we demonstrate that the proposed CausalVAE model is able to generate counterfactual data through “do-operation” to the causal factors.},
	language = {en},
	urldate = {2023-09-28},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yang, Mengyue and Liu, Furui and Chen, Zhitang and Shen, Xinwei and Hao, Jianye and Wang, Jun},
	month = jun,
	year = {2021},
	pages = {9588--9597},
}

@misc{scholkopf_towards_2021,
	title = {Towards {Causal} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2102.11107},
	abstract = {The two fields of machine learning and graphical causality arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Schölkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
	month = feb,
	year = {2021},
	note = {arXiv:2102.11107 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{shanmugam_elements_2018,
	title = {Elements of causal inference: foundations and learning algorithms},
	volume = {88},
	issn = {0094-9655, 1563-5163},
	shorttitle = {Elements of causal inference},
	url = {https://www.tandfonline.com/doi/full/10.1080/00949655.2018.1505197},
	doi = {10.1080/00949655.2018.1505197},
	language = {en},
	number = {16},
	urldate = {2023-09-27},
	journal = {Journal of Statistical Computation and Simulation},
	author = {Shanmugam, Ramalingam},
	month = nov,
	year = {2018},
	pages = {3248--3248},
}

@article{nalmpantis_time_2020,
	title = {On time series representations for multi-label {NILM}},
	volume = {32},
	issn = {0941-0643},
	url = {https://link.springer.com/epdf/10.1007/s00521-020-04916-5},
	doi = {10.1007/s00521-020-04916-5},
	abstract = {Given only the main power consumption of a household, a non-intrusive load monitoring (NILM) system identifies which appliances are operating. With the rise of Internet of things, running energy disaggregation models on the edge is more and more essential for privacy concerns and economic reasons. However, current NILM solutions use data-hungry deep learning models that can recognize only one device and are impossible to run on a device with limited resources. This research investigates in-depth multi-label NILM systems and suggests a novel framework which enables a cost-effective solution. It can be deployed on an embedded device, and thus, privacy can be preserved. The proposed system leverages dimensionality reduction using Signal2Vec, is evaluated on two popular public datasets and outperforms another state-of-the-art multi-label NILM system.},
	language = {en},
	number = {23},
	urldate = {2023-09-27},
	journal = {Neural Computing and Applications},
	author = {Nalmpantis, Christoforos and Vrakas, Dimitris},
	year = {2020},
}

@article{yang_semisupervised_2020,
	title = {Semisupervised {Multilabel} {Deep} {Learning} {Based} {Nonintrusive} {Load} {Monitoring} in {Smart} {Grids}},
	volume = {16},
	issn = {1551-3203, 1941-0050},
	url = {https://ieeexplore.ieee.org/document/8911216/},
	doi = {10.1109/TII.2019.2955470},
	abstract = {Nonintrusive load monitoring (NILM) is a technique that infers appliance-level energy consumption patterns and operation state changes based on feeder power signals. With the availability of ﬁne-grained electric load proﬁles, there has been increasing interest in using this approach for demand-side energy management in smart grids. NILM is a multilabel classiﬁcation problem due to the simultaneous operation of multiple appliances. Recently, deep learning based techniques have been shown to be a promising approach to solving this problem, but annotating the huge volume of load proﬁle data with multiple active appliances for learning is very challenging and impractical. In this article, a new semisupervised multilabel deep learning based framework is proposed to address this problem with the goal of mitigating the reliance on large labeled datasets. Speciﬁcally, a temporal convolutional neural network is used to automatically extract high-level load signatures for individual appliances. These signatures can be efﬁciently used to improve the feature representation capability of the framework. Case studies conducted on two open-access NILM datasets demonstrate the effectiveness and superiority of the proposed approach.},
	language = {en},
	number = {11},
	urldate = {2023-09-27},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Yang, Yandong and Zhong, Jing and Li, Wei and Gulliver, T. Aaron and Li, Shufang},
	month = nov,
	year = {2020},
	pages = {6892--6902},
}

@misc{noauthor_zimbra_nodate,
	title = {Zimbra: {Réception}},
	url = {https://z.imt.fr/zimbra/mail#1},
	urldate = {2023-09-27},
}

@misc{noauthor_zimbra_nodate-1,
	title = {Zimbra: {Réception}},
	url = {https://z.imt.fr/zimbra/mail#1},
	urldate = {2023-09-27},
}

@inproceedings{bai_contrastively_2021,
	title = {Contrastively {Disentangled} {Sequential} {Variational} {Autoencoder}},
	url = {https://openreview.net/forum?id=rWPxhfz2_S},
	abstract = {Self-supervised disentangled representation learning is a critical task in sequence modeling. The learnt representations contribute to better model interpretability as well as the data generation, and improve the sample efficiency for downstream tasks. We propose a novel sequence representation learning method, named Contrastively Disentangled Sequential Variational Autoencoder (C-DSVAE), to extract and separate the static (time-invariant) and dynamic (time-variant) factors in the latent space. Different from previous sequential variational autoencoder methods, we use a novel evidence lower bound which maximizes the mutual information between the input and the latent factors, while penalizes the mutual information between the static and dynamic factors. We leverage contrastive estimations of the mutual information terms in training, together with simple yet effective augmentation techniques, to introduce additional inductive biases. Our experiments show that C-DSVAE significantly outperforms the previous state-of-the-art methods on multiple metrics.},
	language = {en},
	urldate = {2023-09-27},
	author = {Bai, Junwen and Wang, Weiran and Gomes, Carla P.},
	month = nov,
	year = {2021},
}

@misc{carbonneau_measuring_2022,
	title = {Measuring {Disentanglement}: {A} {Review} of {Metrics}},
	shorttitle = {Measuring {Disentanglement}},
	url = {http://arxiv.org/abs/2012.09276},
	abstract = {Learning to disentangle and represent factors of variation in data is an important problem in AI. While many advances have been made to learn these representations, it is still unclear how to quantify disentanglement. While several metrics exist, little is known on their implicit assumptions, what they truly measure, and their limits. In consequence, it is difficult to interpret results when comparing different representations. In this work, we survey supervised disentanglement metrics and thoroughly analyze them. We propose a new taxonomy in which all metrics fall into one of three families: intervention-based, predictor-based and information-based. We conduct extensive experiments in which we isolate properties of disentangled representations, allowing stratified comparison along several axes. From our experiment results and analysis, we provide insights on relations between disentangled representation properties. Finally, we share guidelines on how to measure disentanglement.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Carbonneau, Marc-André and Zaidi, Julian and Boilard, Jonathan and Gagnon, Ghyslain},
	month = may,
	year = {2022},
	note = {arXiv:2012.09276 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2023-09-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
}

@inproceedings{apostolopoulou_deep_2022,
  title={Deep Attentive Variational Inference},
  author={Apostolopoulou, Ifigeneia and Char, Ian and Rosenfeld, Elan and Dubrawski, Artur},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@misc{franceschi_unsupervised_2020,
	title = {Unsupervised {Scalable} {Representation} {Learning} for {Multivariate} {Time} {Series}},
	url = {http://arxiv.org/abs/1901.10738},
	abstract = {Time series constitute a challenging data type for machine learning algorithms, due to their highly variable lengths and sparse labeling in practice. In this paper, we tackle this challenge by proposing an unsupervised method to learn universal embeddings of time series. Unlike previous works, it is scalable with respect to their length and we demonstrate the quality, transferability and practicability of the learned representations with thorough experiments and comparisons. To this end, we combine an encoder based on causal dilated convolutions with a novel triplet loss employing time-based negative sampling, obtaining general-purpose representations for variable length and multivariate time series.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Franceschi, Jean-Yves and Dieuleveut, Aymeric and Jaggi, Martin},
	month = jan,
	year = {2020},
	note = {arXiv:1901.10738 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{paul_psa-gan_2022,
	title = {{PSA}-{GAN}: {Progressive} {Self} {Attention} {GANs} for {Synthetic} {Time} {Series}},
	shorttitle = {{PSA}-{GAN}},
	url = {http://arxiv.org/abs/2108.00981},
	abstract = {Realistic synthetic time series data of sufficient length enables practical applications in time series modeling tasks, such as forecasting, but remains a challenge. In this paper we present PSA-GAN, a generative adversarial network (GAN) that generates long time series samples of high quality using progressive growing of GANs and self-attention. We show that PSA-GAN can be used to reduce the error in two downstream forecasting tasks over baselines that only use real data. We also introduce a Frechet-Inception Distance-like score, Context-FID, assessing the quality of synthetic time series samples. In our downstream tasks, we find that the lowest scoring models correspond to the best-performing ones. Therefore, Context-FID could be a useful tool to develop time series GAN models.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Paul, Jeha and Michael, Bohlke-Schneider and Pedro, Mercado and Shubham, Kapoor and Rajbir, Singh Nirwan and Valentin, Flunkert and Jan, Gasthaus and Tim, Januschowski},
	month = mar,
	year = {2022},
	note = {arXiv:2108.00981 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{zimmermann_contrastive_2022-2,
	title = {Contrastive {Learning} {Inverts} the {Data} {Generating} {Process}},
	url = {http://arxiv.org/abs/2102.08850},
	abstract = {Contrastive learning has recently seen tremendous success in self-supervised learning. So far, however, it is largely unclear why the learned representations generalize so effectively to a large variety of downstream tasks. We here prove that feedforward models trained with objectives belonging to the commonly used InfoNCE family learn to implicitly invert the underlying generative model of the observed data. While the proofs make certain statistical assumptions about the generative model, we observe empirically that our findings hold even if these assumptions are severely violated. Our theory highlights a fundamental connection between contrastive learning, generative modeling, and nonlinear independent component analysis, thereby furthering our understanding of the learned representations as well as providing a theoretical foundation to derive more effective contrastive losses.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Zimmermann, Roland S. and Sharma, Yash and Schneider, Steffen and Bethge, Matthias and Brendel, Wieland},
	month = apr,
	year = {2022},
	note = {arXiv:2102.08850 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{li_generative_2023,
	title = {Generative {Time} {Series} {Forecasting} with {Diffusion}, {Denoise}, and {Disentanglement}},
	url = {http://arxiv.org/abs/2301.03028},
	abstract = {Time series forecasting has been a widely explored task of great importance in many applications. However, it is common that real-world time series data are recorded in a short time period, which results in a big gap between the deep model and the limited and noisy time series. In this work, we propose to address the time series forecasting problem with generative modeling and propose a bidirectional variational auto-encoder (BVAE) equipped with diffusion, denoise, and disentanglement, namely D3VAE. Specifically, a coupled diffusion probabilistic model is proposed to augment the time series data without increasing the aleatoric uncertainty and implement a more tractable inference process with BVAE. To ensure the generated series move toward the true target, we further propose to adapt and integrate the multiscale denoising score matching into the diffusion process for time series forecasting. In addition, to enhance the interpretability and stability of the prediction, we treat the latent variable in a multivariate manner and disentangle them on top of minimizing total correlation. Extensive experiments on synthetic and real-world data show that D3VAE outperforms competitive algorithms with remarkable margins. Our implementation is available at https://github.com/PaddlePaddle/PaddleSpatial/tree/main/research/D3VAE.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Li, Yan and Lu, Xinjiang and Wang, Yaqing and Dou, Dejing},
	month = jan,
	year = {2023},
	note = {arXiv:2301.03028 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{li_generative_2023-1,
	title = {Generative {Time} {Series} {Forecasting} with {Diffusion}, {Denoise}, and {Disentanglement}},
	url = {http://arxiv.org/abs/2301.03028},
	abstract = {Time series forecasting has been a widely explored task of great importance in many applications. However, it is common that real-world time series data are recorded in a short time period, which results in a big gap between the deep model and the limited and noisy time series. In this work, we propose to address the time series forecasting problem with generative modeling and propose a bidirectional variational auto-encoder (BVAE) equipped with diffusion, denoise, and disentanglement, namely D3VAE. Specifically, a coupled diffusion probabilistic model is proposed to augment the time series data without increasing the aleatoric uncertainty and implement a more tractable inference process with BVAE. To ensure the generated series move toward the true target, we further propose to adapt and integrate the multiscale denoising score matching into the diffusion process for time series forecasting. In addition, to enhance the interpretability and stability of the prediction, we treat the latent variable in a multivariate manner and disentangle them on top of minimizing total correlation. Extensive experiments on synthetic and real-world data show that D3VAE outperforms competitive algorithms with remarkable margins. Our implementation is available at https://github.com/PaddlePaddle/PaddleSpatial/tree/main/research/D3VAE.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Li, Yan and Lu, Xinjiang and Wang, Yaqing and Dou, Dejing},
	month = jan,
	year = {2023},
	note = {arXiv:2301.03028 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{noauthor_zimbra_nodate-2,
	title = {Zimbra: {Réception}},
	url = {https://z.imt.fr/zimbra/mail#1},
	urldate = {2023-09-24},
}


@misc{maaloe_biva_2019,
	title = {{BIVA}: {A} {Very} {Deep} {Hierarchy} of {Latent} {Variables} for {Generative} {Modeling}},
	shorttitle = {{BIVA}},
	url = {http://arxiv.org/abs/1902.02102},
	abstract = {With the introduction of the variational autoencoder (VAE), probabilistic latent variable models have received renewed attention as powerful generative models. However, their performance in terms of test likelihood and quality of generated samples has been surpassed by autoregressive models without stochastic units. Furthermore, flow-based models have recently been shown to be an attractive alternative that scales well to high-dimensional data. In this paper we close the performance gap by constructing VAE models that can effectively utilize a deep hierarchy of stochastic variables and model complex covariance structures. We introduce the Bidirectional-Inference Variational Autoencoder (BIVA), characterized by a skip-connected generative model and an inference network formed by a bidirectional stochastic inference path. We show that BIVA reaches state-of-the-art test likelihoods, generates sharp and coherent natural images, and uses the hierarchy of latent variables to capture different aspects of the data distribution. We observe that BIVA, in contrast to recent results, can be used for anomaly detection. We attribute this to the hierarchy of latent variables which is able to extract high-level semantic features. Finally, we extend BIVA to semi-supervised classification tasks and show that it performs comparably to state-of-the-art results by generative adversarial networks.},
	urldate = {2023-09-24},
	publisher = {arXiv},
	author = {Maaløe, Lars and Fraccaro, Marco and Liévin, Valentin and Winther, Ole},
	month = nov,
	year = {2019},
	note = {arXiv:1902.02102 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{do_theory_2021,
	title = {Theory and {Evaluation} {Metrics} for {Learning} {Disentangled} {Representations}},
	url = {http://arxiv.org/abs/1908.09961},
	abstract = {We make two theoretical contributions to disentanglement learning by (a) defining precise semantics of disentangled representations, and (b) establishing robust metrics for evaluation. First, we characterize the concept "disentangled representations" used in supervised and unsupervised methods along three dimensions-informativeness, separability and interpretability - which can be expressed and quantified explicitly using information-theoretic constructs. This helps explain the behaviors of several well-known disentanglement learning models. We then propose robust metrics for measuring informativeness, separability and interpretability. Through a comprehensive suite of experiments, we show that our metrics correctly characterize the representations learned by different methods and are consistent with qualitative (visual) results. Thus, the metrics allow disentanglement learning methods to be compared on a fair ground. We also empirically uncovered new interesting properties of VAE-based methods and interpreted them with our formulation. These findings are promising and hopefully will encourage the design of more theoretically driven models for learning disentangled representations.},
	urldate = {2023-09-23},
	publisher = {arXiv},
	author = {Do, Kien and Tran, Truyen},
	month = mar,
	year = {2021},
	note = {arXiv:1908.09961 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}


@inproceedings{higgins_beta-vae_2016,
  title={beta-vae: Learning basic visual concepts with a constrained variational framework},
  author={Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  booktitle={International conference on learning representations},
  year={2016}
}

@misc{kim_disentangling_2019,
	title = {Disentangling by {Factorising}},
	url = {http://arxiv.org/abs/1802.05983},
	abstract = {We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon \${\textbackslash}beta\$-VAE by providing a better trade-off between disentanglement and reconstruction quality. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.},
	urldate = {2023-09-22},
	publisher = {arXiv},
	author = {Kim, Hyunjik and Mnih, Andriy},
	month = jul,
	year = {2019},
	note = {arXiv:1802.05983 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{kingma_auto-encoding_2022-1,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2023-09-22},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2022},
	note = {arXiv:1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bucci_new_2021,
	title = {A {New} {Convolutional} {Neural} {Network}-{Based} {System} for {NILM} {Applications}},
	doi = {10.1109/TIM.2020.3035193},
	journal = {IEEE Transactions on Instrumentation and Measurement},
	author = {Bucci, Giovanni and Fiorucci, Edoardo and Mari, Simone and Fioravanti, Andrea},
	year = {2021},
}

@article{koublal_xgen_2023,
	title = {{XGen}: {A} {Comprehensive} {Archive} and an {eXplainable} {Time} {Series} {Generation} {Framework} for {Energy}},
	url = {https://xgentimeseries.github.io},
	author = {Koublal, Ladjal S, Benhaiem D, le-borgne E and Roueff, F.},
	year = {2023},
}

@article{chen_isolating_2018-1,
	title = {Isolating sources of disentanglement in variational autoencoders},
	volume = {31},
	journal = {Advances in neural information processing systems},
	author = {Chen, Ricky TQ and Li, Xuechen and Grosse, Roger B and Duvenaud, David K},
	year = {2018},
}

@article{davidson_hyperspherical_2018,
	title = {Hyperspherical variational auto-encoders},
	journal = {arXiv preprint arXiv:1804.00891},
	author = {Davidson, Tim R and Falorsi, Luca and De Cao, Nicola and Kipf, Thomas and Tomczak, Jakub M},
	year = {2018},
}

@article{yang_sequence_2021-1,
	title = {Sequence to {Point} {Learning} {Based} on an {Attention} {Neural} {Network} for {Nonintrusive} {Load} {Decomposition}},
	journal = {Electronics},
	author = {Yang, Mingzhi and Li, Xinchun and Liu, Yue},
	year = {2021},
}

@article{kingma_adam_2014-1,
	title = {Adam: {A} method for stochastic optimization},
	journal = {arXiv preprint arXiv:1412.6980},
	author = {Kingma, Diederik P and Ba, Jimmy},
	year = {2014},
}

@article{chen_convolutional_2018-1,
	title = {Convolutional sequence to sequence non-intrusive load monitoring},
	volume = {2018},
	number = {17},
	journal = {the Journal of Engineering},
	author = {Chen, Kunjin and Wang, Qin and He, Ziyu and Chen, Kunlong and Hu, Jun and He, Jinliang},
	year = {2018},
	note = {Publisher: Wiley Online Library},
	pages = {1860--1864},
}

@inproceedings{vahdat_nvae_2020-1,
	title = {{NVAE}: {A} {Deep} {Hierarchical} {Variational} {Autoencoder}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Vahdat, Arash and Kautz, Jan},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
}

@inproceedings{chen_isolating_2018-2,
	title = {Isolating {Sources} of {Disentanglement} in {Variational} {Autoencoders}},
	volume = {31},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Ricky T. Q. and Li, Xuechen and Grosse, Roger B and Duvenaud, David K},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
}

@article{kelly_uk-dale_2015-1,
	title = {The {UK}-{DALE} dataset, domestic appliance-level electricity demand and whole-house demand from five {UK} homes},
	volume = {2},
	journal = {Scientific data},
	author = {Kelly, Jack and Knottenbelt, William},
	year = {2015},
	note = {Publisher: Nature Publishing Group},
}

@inproceedings{valenti_exploiting_2018-1,
	title = {Exploiting the reactive power in deep neural models for non-intrusive load monitoring},
	booktitle = {2018 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Valenti, Michele and Bonfigli, Roberto and Principi, Emanuele and Squartini, Stefano},
	year = {2018},
}

@inproceedings{kolter_redd_2011,
	title = {{REDD}: {A} public data set for energy disaggregation research},
	volume = {25},
	booktitle = {Workshop on data mining applications in sustainability ({SIGKDD}), {San} {Diego}, {CA}},
	author = {Kolter, J Zico and Johnson, Matthew J},
	year = {2011},
	note = {Issue: Citeseer},
}

@article{bardes_vicreg_2022,
	title = {{VICREG}: {VARIANCE}-{INVARIANCE}-{COVARIANCE} {RE}- {GULARIZATION} {FOR} {SELF}-{SUPERVISED} {LEARNING}},
	abstract = {Recent self-supervised methods for image representation learning maximize the agreement between embedding vectors produced by encoders fed with different views of the same image. The main challenge is to prevent a collapse in which the encoders produce constant or non-informative vectors. We introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with two regularizations terms applied to both embeddings separately: (1) a term that maintains the variance of each embedding dimension above a threshold, (2) a term that decorrelates each pair of variables. Unlike most other approaches to the same problem, VICReg does not require techniques such as: weight sharing between the branches, batch normalization, feature-wise normalization, output quantization, stop gradient, memory banks, etc., and achieves results on par with the state of the art on several downstream tasks. In addition, we show that our variance regularization term stabilizes the training of other methods and leads to performance improvements.},
	language = {en},
	author = {Bardes, Adrien and Ponce, Jean and LeCun, Yann},
	year = {2022},
}

@inproceedings{bardes_vicreg_2021,
	title = {{VICReg}: {Variance}-{Invariance}-{Covariance} {Regularization} for {Self}-{Supervised} {Learning}},
	shorttitle = {{VICReg}},
	url = {https://openreview.net/forum?id=xm6YD62D1Ub},
	abstract = {Recent self-supervised methods for image representation learning maximize the agreement between embedding vectors produced by encoders fed with different views of the same image. The main challenge is to prevent a collapse in which the encoders produce constant or non-informative vectors. We introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with two regularizations terms applied to both embeddings separately: (1) a term that maintains the variance of each embedding dimension above a threshold, (2) a term that decorrelates each pair of variables. Unlike most other approaches to the same problem, VICReg does not require techniques such as: weight sharing between the branches, batch normalization, feature-wise normalization, output quantization, stop gradient, memory banks, etc., and achieves results on par with the state of the art on several downstream tasks. In addition, we show that our variance regularization term stabilizes the training of other methods and leads to performance improvements.},
	language = {en},
	urldate = {2023-09-21},
	author = {Bardes, Adrien and Ponce, Jean and LeCun, Yann},
	month = oct,
	year = {2021},
}

@misc{roth_disentanglement_2023,
	title = {Disentanglement of {Correlated} {Factors} via {Hausdorff} {Factorized} {Support}},
	url = {http://arxiv.org/abs/2210.07347},
	abstract = {A grand goal in deep learning research is to learn representations capable of generalizing across distribution shifts. Disentanglement is one promising direction aimed at aligning a model's representation with the underlying factors generating the data (e.g. color or background). Existing disentanglement methods, however, rely on an often unrealistic assumption: that factors are statistically independent. In reality, factors (like object color and shape) are correlated. To address this limitation, we consider the use of a relaxed disentanglement criterion -- the Hausdorff Factorized Support (HFS) criterion -- that encourages only pairwise factorized {\textbackslash}emph\{support\}, rather than a factorial distribution, by minimizing a Hausdorff distance. This allows for arbitrary distributions of the factors over their support, including correlations between them. We show that the use of HFS consistently facilitates disentanglement and recovery of ground-truth factors across a variety of correlation settings and benchmarks, even under severe training correlations and correlation shifts, with in parts over \$+60{\textbackslash}\%\$ in relative improvement over existing disentanglement methods. In addition, we find that leveraging HFS for representation learning can even facilitate transfer to downstream tasks such as classification under distribution shifts. We hope our original approach and positive empirical results inspire further progress on the open problem of robust generalization. Code available at https://github.com/facebookresearch/disentangling-correlated-factors.},
	urldate = {2023-09-21},
	publisher = {arXiv},
	author = {Roth, Karsten and Ibrahim, Mark and Akata, Zeynep and Vincent, Pascal and Bouchacourt, Diane},
	month = feb,
	year = {2023},
	note = {arXiv:2210.07347 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}


@article{ren2021learning,
  title={Learning disentangled representation by exploiting pretrained generative models: A contrastive learning view},
  author={Ren, Xuanchi and Yang, Tao and Wang, Yuwang and Zeng, Wenjun},
  journal={arXiv preprint arXiv:2102.10543},
  year={2021}
}

@article{klindt_towards_2021,
  title={Towards nonlinear disentanglement in natural data with temporal sparse coding},
  author={Klindt, David and Schott, Lukas and Sharma, Yash and Ustyuzhaninov, Ivan and Brendel, Wieland and Bethge, Matthias and Paiton, Dylan},
  journal={arXiv preprint arXiv:2007.10930},
  year={2020}
}

@article{zhao2019deep,
  title={Deep temporal convolutional networks for short-term traffic flow forecasting},
  author={Zhao, Wentian and Gao, Yanyun and Ji, Tingxiang and Wan, Xili and Ye, Feng and Bai, Guangwei},
  journal={Ieee Access},
  volume={7},
  pages={114496--114507},
  year={2019},
  publisher={IEEE}
}

@article{murray2017electrical,
  title={An electrical load measurements dataset of United Kingdom households from a two-year longitudinal study},
  author={Murray, David and Stankovic, Lina and Stankovic, Vladimir},
  journal={Scientific data},
  volume={4},
  number={1},
  pages={1--12},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@book{ash2012information,
  title={Information theory},
  author={Ash, Robert B},
  year={2012},
  publisher={Courier Corporation}
}

@inproceedings{zhang_use_2022,
	address = {New Orleans, LA, USA},
	title = {Use {All} {The} {Labels}: {A} {Hierarchical} {Multi}-{Label} {Contrastive} {Learning} {Framework}},
	isbn = {978-1-66546-946-3},
	shorttitle = {Use {All} {The} {Labels}},
	url = {https://ieeexplore.ieee.org/document/9880213/},
	doi = {10.1109/CVPR52688.2022.01616},
	abstract = {Current contrastive learning frameworks focus on leveraging a single supervisory signal to learn representations, which limits the efficacy on unseen data and downstream tasks. In this paper, we present a hierarchical multi-label representation learning framework that can leverage all available labels and preserve the hierarchical relationship between classes. We introduce novel hierarchy preserving losses, which jointly apply a hierarchical penalty to the contrastive loss, and enforce the hierarchy constraint. The loss function is data driven and automatically adapts to arbitrary multi-label structures. Experiments on several datasets show that our relationship-preserving embedding performs well on a variety of tasks and outperform the baseline supervised and self-supervised approaches. Code is available at https://github.com/salesforce/ hierarchicalContrastiveLearning.},
	language = {en},
	urldate = {2023-09-20},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhang, Shu and Xu, Ran and Xiong, Caiming and Ramaiah, Chetan},
	month = jun,
	year = {2022},
	pages = {16639--16648},
}

@article{do_theory_2020,
	title = {{THEORY} {AND} {EVALUATION} {METRICS} {FOR} {LEARNING} {DISENTANGLED} {REPRESENTATIONS}},
	abstract = {We make two theoretical contributions to disentanglement learning by (a) deﬁning precise semantics of disentangled representations, and (b) establishing robust metrics for evaluation. First, we characterize the concept “disentangled representations” used in supervised and unsupervised methods along three dimensions–informativeness, separability and interpretability–which can be expressed and quantiﬁed explicitly using information-theoretic constructs. This helps explain the behaviors of several well-known disentanglement learning models. We then propose robust metrics for measuring informativeness, separability, and interpretability. Through a comprehensive suite of experiments, we show that our metrics correctly characterize the representations learned by different methods and are consistent with qualitative (visual) results. Thus, the metrics allow disentanglement learning methods to be compared on a fair ground. We also empirically uncovered new interesting properties of VAE-based methods and interpreted them with our formulation. These ﬁndings are promising and hopefully will encourage the design of more theoretically driven models for learning disentangled representations.},
	language = {en},
	author = {Do, Kien and Tran, Truyen},
	year = {2020},
}

@article{daudel_alpha-divergence_nodate,
	title = {Alpha-divergence {Variational} {Inference} {Meets} {Importance} {Weighted} {Auto}-{Encoders}: {Methodology} and {Asymptotics}},
	abstract = {Several algorithms involving the Variational R´enyi (VR) bound have been proposed to minimize an alpha-divergence between a target posterior distribution and a variational distribution. Despite promising empirical results, those algorithms resort to biased stochastic gradient descent procedures and thus lack theoretical guarantees. In this paper, we formalize and study the VR-IWAE bound, a generalization of the importance weighted auto-encoder (IWAE) bound. We show that the VR-IWAE bound enjoys several desirable properties and notably leads to the same stochastic gradient descent procedure as the VR bound in the reparameterized case, but this time by relying on unbiased gradient estimators. We then provide two complementary theoretical analyses of the VR-IWAE bound and thus of the standard IWAE bound. Those analyses shed light on the beneﬁts or lack thereof of these bounds. Lastly, we illustrate our theoretical claims over toy and real-data examples.},
	language = {en},
	author = {Daudel, Kamelia and Benton, Joe and Shi, Yuyang and Doucet, Arnaud},
}

@misc{pham_pcaae_2020,
	title = {{PCAAE}: {Principal} {Component} {Analysis} {Autoencoder} for organising the latent space of generative networks},
	shorttitle = {{PCAAE}},
	url = {http://arxiv.org/abs/2006.07827},
	abstract = {Autoencoders and generative models produce some of the most spectacular deep learning results to date. However, understanding and controlling the latent space of these models presents a considerable challenge. Drawing inspiration from principal component analysis and autoencoder, we propose the Principal Component Analysis Autoencoder (PCAAE). This is a novel autoencoder whose latent space verifies two properties. Firstly, the dimensions are organised in decreasing importance with respect to the data at hand. Secondly, the components of the latent space are statistically independent. We achieve this by progressively increasing the latent space during training, and with a covariance loss applied to the latent codes. The resulting autoencoder produces a latent space which separates the intrinsic attributes of the data into different components of the latent space, in a completely unsupervised manner. We also describe an extension of our approach to the case of powerful, pre-trained GANs. We show results on both synthetic examples of shapes and on a state-of-the-art GAN. For example, we are able to separate the color shade scale of hair and skin, pose of faces and the gender in the CelebA, without accessing any labels. We compare the PCAAE with other state-of-the-art approaches, in particular with respect to the ability to disentangle attributes in the latent space. We hope that this approach will contribute to better understanding of the intrinsic latent spaces of powerful deep generative models.},
	urldate = {2023-09-20},
	publisher = {arXiv},
	author = {Pham, Chi-Hieu and Ladjal, Saïd and Newson, Alasdair},
	month = jun,
	year = {2020},
	note = {arXiv:2006.07827 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}


@inproceedings{eastwood_framework_2018,
  title={A framework for the quantitative evaluation of disentangled representations},
  author={Eastwood, Cian and Williams, Christopher KI},
  booktitle={International conference on learning representations},
  year={2018}
}

@misc{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://arxiv.org/abs/2002.05709},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	urldate = {2023-09-20},
	publisher = {arXiv},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {arXiv:2002.05709 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_iclr-2023-self-supervised-attention-based-variational-autoencoder-for-appliance-usage_nodate,
	title = {{ICLR}-2023-{SELF}-{SUPERVISED}-{ATTENTION}-{BASED}-{VARIATIONAL}-{AUTOENCODER}-{FOR}-{APPLIANCE}-{USAGE}},
	url = {https://www.overleaf.com/project/64d55386b646ac8e003d85ac},
	abstract = {An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2023-09-19},
}

@misc{khosla_supervised_2021,
	title = {Supervised {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2004.11362},
	abstract = {Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches subsume or significantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4\% on the ImageNet dataset, which is 0.8\% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows benefits for robustness to natural corruptions and is more stable to hyperparameter settings such as optimizers and data augmentations. Our loss function is simple to implement, and reference TensorFlow code is released at https://t.ly/supcon.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
	month = mar,
	year = {2021},
	note = {arXiv:2004.11362 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{luo_time_2023,
	title = {Time {Series} {Contrastive} {Learning} with {Information}-{Aware} {Augmentations}},
	url = {http://arxiv.org/abs/2303.11911},
	abstract = {Various contrastive learning approaches have been proposed in recent years and achieve significant empirical success. While effective and prevalent, contrastive learning has been less explored for time series data. A key component of contrastive learning is to select appropriate augmentations imposing some priors to construct feasible positive samples, such that an encoder can be trained to learn robust and discriminative representations. Unlike image and language domains where ``desired'' augmented samples can be generated with the rule of thumb guided by prefabricated human priors, the ad-hoc manual selection of time series augmentations is hindered by their diverse and human-unrecognizable temporal structures. How to find the desired augmentations of time series data that are meaningful for given contrastive learning tasks and datasets remains an open question. In this work, we address the problem by encouraging both high {\textbackslash}textit\{fidelity\} and {\textbackslash}textit\{variety\} based upon information theory. A theoretical analysis leads to the criteria for selecting feasible data augmentations. On top of that, we propose a new contrastive learning approach with information-aware augmentations, InfoTS, that adaptively selects optimal augmentations for time series representation learning. Experiments on various datasets show highly competitive performance with up to 12.0{\textbackslash}\% reduction in MSE on forecasting tasks and up to 3.7{\textbackslash}\% relative improvement in accuracy on classification tasks over the leading baselines.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Luo, Dongsheng and Cheng, Wei and Wang, Yingheng and Xu, Dongkuan and Ni, Jingchao and Yu, Wenchao and Zhang, Xuchao and Liu, Yanchi and Chen, Yuncong and Chen, Haifeng and Zhang, Xiang},
	month = mar,
	year = {2023},
	note = {arXiv:2303.11911 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv:1406.2661 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{li_dual-stream_2021,
	title = {Dual-stream {Multiple} {Instance} {Learning} {Network} for {Whole} {Slide} {Image} {Classification} with {Self}-supervised {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2011.08939},
	abstract = {We address the challenging problem of whole slide image (WSI) classification. WSIs have very high resolutions and usually lack localized annotations. WSI classification can be cast as a multiple instance learning (MIL) problem when only slide-level labels are available. We propose a MIL-based method for WSI classification and tumor detection that does not require localized annotations. Our method has three major components. First, we introduce a novel MIL aggregator that models the relations of the instances in a dual-stream architecture with trainable distance measurement. Second, since WSIs can produce large or unbalanced bags that hinder the training of MIL models, we propose to use self-supervised contrastive learning to extract good representations for MIL and alleviate the issue of prohibitive memory cost for large bags. Third, we adopt a pyramidal fusion mechanism for multiscale WSI features, and further improve the accuracy of classification and localization. Our model is evaluated on two representative WSI datasets. The classification accuracy of our model compares favorably to fully-supervised methods, with less than 2\% accuracy gap across datasets. Our results also outperform all previous MIL-based methods. Additional benchmark results on standard MIL datasets further demonstrate the superior performance of our MIL aggregator on general MIL problems. GitHub repository: https://github.com/binli123/dsmil-wsi},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Li, Bin and Li, Yin and Eliceiri, Kevin W.},
	month = apr,
	year = {2021},
	note = {arXiv:2011.08939 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{zbontar_barlow_2021,
	title = {Barlow {Twins}: {Self}-{Supervised} {Learning} via {Redundancy} {Reduction}},
	shorttitle = {Barlow {Twins}},
	url = {http://arxiv.org/abs/2103.03230},
	abstract = {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stéphane},
	month = jun,
	year = {2021},
	note = {arXiv:2103.03230 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
}

@article{woo_cost_2022,
  title={CoST: Contrastive learning of disentangled seasonal-trend representations for time series forecasting},
  author={Woo, Gerald and Liu, Chenghao and Sahoo, Doyen and Kumar, Akshat and Hoi, Steven},
  journal={arXiv preprint arXiv:2202.01575},
  year={2022}
}

@inproceedings{liu2022multivariate,
  title={Multivariate Time-series Imputation with Disentangled Temporal Representations},
  author={LIU, SHUAI and Li, Xiucheng and Cong, Gao and Chen, Yile and JIANG, YUE},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@misc{vahdat_nvae_2021,
	title = {{NVAE}: {A} {Deep} {Hierarchical} {Variational} {Autoencoder}},
	shorttitle = {{NVAE}},
	url = {http://arxiv.org/abs/2007.03898},
	abstract = {Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256\${\textbackslash}times\$256 pixels. The source code is available at https://github.com/NVlabs/NVAE .},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Vahdat, Arash and Kautz, Jan},
	month = jan,
	year = {2021},
	note = {arXiv:2007.03898 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kumar_variational_2018,
  title={Variational inference of disentangled latent concepts from unlabeled observations},
  author={Kumar, Abhishek and Sattigeri, Prasanna and Balakrishnan, Avinash},
  journal={arXiv preprint arXiv:1711.00848},
  year={2017}
}

@article{liu_multivariate_2023-1,
	title = {{MULTIVARIATE} {TIME}-{SERIES} {IMPUTATION} {WITH} {DIS}- {ENTANGLED} {TEMPORAL} {REPRESENTATIONS}},
	abstract = {Multivariate time series often faces the problem of missing value. Many time series imputation methods have been developed in literature. However, they all rely on an entangled representation to model dynamics of time series, which may fail to fully exploit the multiple factors (e.g., periodic patterns) presented in the data. Moreover, the entangled representations usually have no semantic meaning, and thus they often lack interpretability. In addition, many recent models are proposed to deal with the whole time series to identify temporal dynamics, but they are not scalable to long time series. Different from existing approaches, we propose TIDER, a novel matrix factorization-based method with disentangled temporal representations that account for multiple factors, namely trend, seasonality, and local bias, to model complex dynamics. The learned disentanglement makes the imputation process more reliable and offers explainability for imputation results. Moreover, TIDER is scalable to long time series. Empirical results show that our method outperforms existing approaches on three typical real-world datasets, especially on long time series, reducing mean absolute error by up to 50\%. It also scales well to long datasets on which existing deep learning based methods struggle. Disentanglement validation experiments further highlight the robustness and accuracy of our model.},
	language = {en},
	author = {Liu, Shuai and Li, Xiucheng and Cong, Gao and Chen, Yile and Jiang, Yue},
	year = {2023},
}

@misc{bouchacourt_multi-level_2017,
	title = {Multi-{Level} {Variational} {Autoencoder}: {Learning} {Disentangled} {Representations} from {Grouped} {Observations}},
	shorttitle = {Multi-{Level} {Variational} {Autoencoder}},
	url = {http://arxiv.org/abs/1705.08841},
	abstract = {We would like to learn a representation of the data which decomposes an observation into factors of variation which we can independently control. Specifically, we want to use minimal supervision to learn a latent representation that reflects the semantics behind a specific grouping of the data, where within a group the samples share a common factor of variation. For example, consider a collection of face images grouped by identity. We wish to anchor the semantics of the grouping into a relevant and disentangled representation that we can easily exploit. However, existing deep probabilistic models often assume that the observations are independent and identically distributed. We present the Multi-Level Variational Autoencoder (ML-VAE), a new deep probabilistic model for learning a disentangled representation of a set of grouped observations. The ML-VAE separates the latent representation into semantically meaningful parts by working both at the group level and the observation level, while retaining efficient test-time inference. Quantitative and qualitative evaluations show that the ML-VAE model (i) learns a semantically meaningful disentanglement of grouped data, (ii) enables manipulation of the latent representation, and (iii) generalises to unseen groups.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Bouchacourt, Diane and Tomioka, Ryota and Nowozin, Sebastian},
	month = may,
	year = {2017},
	note = {arXiv:1705.08841 [cs, stat]},
	keywords = {Computer Science - Machine Learning, ML-VAE, Statistics - Machine Learning},
}


@article{jutten2004advances,
  title={Advances in blind source separation (BSS) and independent component analysis (ICA) for nonlinear mixtures},
  author={Jutten, Christian and Karhunen, Juha},
  journal={International journal of neural systems},
  volume={14},
  number={05},
  pages={267--292},
  year={2004},
  publisher={World Scientific}
}

@book{peters2017elements,
  title={Elements of causal inference: foundations and learning algorithms},
  author={Peters, Jonas and Janzing, Dominik and Sch{\"o}lkopf, Bernhard},
  year={2017},
  publisher={The MIT Press}
}


@inproceedings{haber2018learning,
  title={Learning across scales---multiscale methods for convolution neural networks},
  author={Haber, Eldad and Ruthotto, Lars and Holtham, Elliot and Jun, Seong-Hwan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}


@article{autoformer,
  title={Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting},
  author={Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={22419--22430},
  year={2021}
}


@inproceedings{chen2023learning,
  title={Learning a sparse transformer network for effective image deraining},
  author={Chen, Xiang and Li, Hao and Li, Mingqiang and Pan, Jinshan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5896--5905},
  year={2023}
}

@inproceedings{fedformer,
  title={Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting},
  author={Zhou, Tian and Ma, Ziqing and Wen, Qingsong and Wang, Xue and Sun, Liang and Jin, Rong},
  booktitle={International conference on machine learning},
  pages={27268--27286},
  year={2022},
  organization={PMLR}
}

@inproceedings{timesnet,
  title={Timesnet: Temporal 2d-variation modeling for general time series analysis},
  author={Wu, Haixu and Hu, Tengge and Liu, Yong and Zhou, Hang and Wang, Jianmin and Long, Mingsheng},
  booktitle={The eleventh international conference on learning representations},
  year={2022}
}


@inproceedings{Dlinear,
  title={Are transformers effective for time series forecasting?},
  author={Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  number={9},
  volume={34},
  pages={11121--11128},
  year={2023}
}
@article{bai2021contrastively,
  title={Contrastively disentangled sequential variational autoencoder},
  author={Bai, Junwen and Wang, Weiran and Gomes, Carla P},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={10105--10118},
  year={2021}
}

@incollection{jutten2010nonlinear,
  title={Nonlinear mixtures},
  author={Jutten, Christian and Babaie-Zadeh, Massoud and Karhunen, Juha},
  booktitle={Handbook of Blind Source Separation},
  pages={549--592},
  year={2010},
  publisher={Elsevier}
}

@article{hyvarinen2013independent,
  title={Independent component analysis: recent advances},
  author={Hyv{\"a}rinen, Aapo},
  journal={Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume={371},
  number={1984},
  pages={20110534},
  year={2013},
  publisher={The Royal Society Publishing}
}

@article{comon1994independent,
  title={Independent component analysis, a new concept?},
  author={Comon, Pierre},
  journal={Signal processing},
  volume={36},
  number={3},
  pages={287--314},
  year={1994},
  publisher={Elsevier}
}


@inproceedings{brady2023provably,
  author = {
    Brady, Jack and
    Zimmermann, Roland S. and
    Sharma, Yash and
    Sch{\"o}lkopf, Bernhard and
    von K{\"u}gelgen, Julius
    Brendel, Wieland and
  },
  title = {
    Provably Learning
    Object-Centric Representations
  },
  year = {2023},
  booktitle = {
    Proceedings of the 40th International
    Conference on Machine Learning
  },
  articleno = {126},
  numpages = {25},
  location = {Honolulu, Hawaii, USA},
  series = {ICML'23}
}



%%%%% COST

@article{scholkopf2021toward,
  title={Toward causal representation learning},
  author={Sch{\"o}lkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  journal={Proceedings of the IEEE},
  volume={109},
  number={5},
  pages={612--634},
  year={2021},
  publisher={IEEE}
}

% ------ Start: Time Series Decomopsition ------
@book{hyndman2018forecasting,
  title={Forecasting: principles and practice},
  author={Hyndman, Rob J and Athanasopoulos, George},
  year={2018},
  publisher={OTexts}
}

@misc{wen2018robuststl,
      title={RobustSTL: A Robust Seasonal-Trend Decomposition Algorithm for Long Time Series}, 
      author={Qingsong Wen and Jingkun Gao and Xiaomin Song and Liang Sun and Huan Xu and Shenghuo Zhu},
      year={2018},
      eprint={1812.01767},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wen2019robusttrend,
    title={RobustTrend: A Huber Loss with a Combined First and Second Order Difference Regularization for Time Series Trend Filtering}, 
    author={Qingsong Wen and Jingkun Gao and Xiaomin Song and Liang Sun and Jian Tan},
    year={2019},
    eprint={1906.03751},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{wen2020fast,
    author = {Wen, Qingsong and Zhang, Zhe and Li, Yan and Sun, Liang},
    title = {Fast RobustSTL: Efficient and Robust Seasonal-Trend Decomposition for Time Series with Complex Patterns},
    year = {2020},
    isbn = {9781450379984},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3394486.3403271},
    doi = {10.1145/3394486.3403271},
    booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
    pages = {2203–2213},
    numpages = {11},
    keywords = {time series, generalized ADMM, seasonal-trend decomposition, multiple seasonality},
    location = {Virtual Event, CA, USA},
    series = {KDD '20}
}

@INPROCEEDINGS{yang2021multiscale,
  author={Yang, Linxiao and Wen, Qingsong and Yang, Bo and Sun, Liang},
  booktitle={ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={A Robust and Efficient Multi-Scale Seasonal-Trend Decomposition}, 
  year={2021},
  volume={},
  number={},
  pages={5085-5089},
  doi={10.1109/ICASSP39728.2021.9413939}}

@article{godfrey2017decomposition,
   title={Neural Decomposition of Time-Series Data for Effective Generalization},
   ISSN={2162-2388},
   url={http://dx.doi.org/10.1109/TNNLS.2017.2709324},
   DOI={10.1109/tnnls.2017.2709324},
   journal={IEEE Transactions on Neural Networks and Learning Systems},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Godfrey, Luke B. and Gashler, Michael S.},
   year={2017},
   pages={1–13}
}


% ------ End: Time Series Decomopsition ------

% ------ Start: Time series representation learning ------
@misc{yue2021ts2vec,
  title={TS2Vec: Towards Universal Representation of Time Series}, 
  author={Zhihan Yue and Yujing Wang and Juanyong Duan and Tianmeng Yang and Congrui Huang and Yunhai Tong and Bixiong Xu},
  year={2021},
  eprint={2106.10466},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}
@inproceedings{
tonekaboni2021unsupervised,
title={Unsupervised Representation Learning for Time Series with Temporal Neighborhood Coding},
author={Sana Tonekaboni and Danny Eytan and Anna Goldenberg},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=8qDwejCuCN}
}
@inproceedings{eldele2021time,
  title     = {Time-Series Representation Learning via Temporal and Contextual Contrasting},
  author    = {Eldele, Emadeldeen and Ragab, Mohamed and Chen, Zhenghua and Wu, Min and Kwoh, Chee Keong and Li, Xiaoli and Guan, Cuntai},
  booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, {IJCAI-21}},
  pages     = {2352--2359},
  year      = {2021},
}
@misc{franceschi2020unsupervised,
      title={Unsupervised Scalable Representation Learning for Multivariate Time Series}, 
      author={Jean-Yves Franceschi and Aymeric Dieuleveut and Martin Jaggi},
      year={2020},
      eprint={1901.10738},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{zerveas2021transformer,
author = {Zerveas, George and Jayaraman, Srideepika and Patel, Dhaval and Bhamidipaty, Anuradha and Eickhoff, Carsten},
title = {A Transformer-Based Framework for Multivariate Time Series Representation Learning},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467401},
doi = {10.1145/3447548.3467401},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
pages = {2114–2124},
numpages = {11},
series = {KDD '21}
}
% ------ End: Time series representation learning ------

% ------ Start: General representation learning ------
@article{bengio2013representation,
  title={Representation learning: A review and new perspectives},
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={8},
  pages={1798--1828},
  year={2013},
  publisher={IEEE}
}

@misc{oord2019representation,
      title={Representation Learning with Contrastive Predictive Coding}, 
      author={Aaron van den Oord and Yazhe Li and Oriol Vinyals},
      year={2019},
      eprint={1807.03748},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{he2020momentum,
      title={Momentum Contrast for Unsupervised Visual Representation Learning}, 
      author={Kaiming He and Haoqi Fan and Yuxin Wu and Saining Xie and Ross Girshick},
      year={2020},
      eprint={1911.05722},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
% ------ End: General representation learning ------

% ------ Start: Deep forecasting ------
@article{wen2017multi,
  title={A multi-horizon quantile recurrent forecaster},
  author={Wen, Ruofeng and Torkkola, Kari and Narayanaswamy, Balakrishnan and Madeka, Dhruv},
  journal={arXiv preprint arXiv:1711.11053},
  year={2017}
}

@inproceedings{lai2018modeling,
  title={Modeling long-and short-term temporal patterns with deep neural networks},
  author={Lai, Guokun and Chang, Wei-Cheng and Yang, Yiming and Liu, Hanxiao},
  booktitle={The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval},
  pages={95--104},
  year={2018}
}

@misc{salinas2019deepar,
      title={DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks}, 
      author={David Salinas and Valentin Flunkert and Jan Gasthaus},
      year={2019},
      eprint={1704.04110},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{rangapuram2018deep,
  title={Deep state space models for time series forecasting},
  author={Rangapuram, Syama Sundar and Seeger, Matthias W and Gasthaus, Jan and Stella, Lorenzo and Wang, Yuyang and Januschowski, Tim},
  journal={Advances in neural information processing systems},
  volume={31},
  pages={7785--7794},
  year={2018}
}

@inproceedings{wang2019deep,
  title={Deep factors for forecasting},
  author={Wang, Yuyang and Smola, Alex and Maddix, Danielle and Gasthaus, Jan and Foster, Dean and Januschowski, Tim},
  booktitle={International conference on machine learning},
  pages={6607--6617},
  year={2019},
  organization={PMLR}
}

@misc{li2020enhancing,
      title={Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting}, 
      author={Shiyang Li and Xiaoyong Jin and Yao Xuan and Xiyou Zhou and Wenhu Chen and Yu-Xiang Wang and Xifeng Yan},
      year={2020},
      eprint={1907.00235},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{zhou2021informer,
  title={Informer: Beyond efficient transformer for long sequence time-series forecasting},
  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  booktitle={Proceedings of AAAI},
  year={2021}
}

@misc{oreshkin2020nbeats,
      title={N-BEATS: Neural basis expansion analysis for interpretable time series forecasting}, 
      author={Boris N. Oreshkin and Dmitri Carpov and Nicolas Chapados and Yoshua Bengio},
      year={2020},
      eprint={1905.10437},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
% ------ End: Deep forecasting ------

@article{cuaresma2004forecasting,
  title={Forecasting electricity spot-prices using linear univariate time-series models},
  author={Cuaresma, Jes{\'u}s Crespo and Hlouskova, Jaroslava and Kossmeier, Stephan and Obersteiner, Michael},
  journal={Applied Energy},
  volume={77},
  number={1},
  pages={87--106},
  year={2004},
  publisher={Elsevier}
}

@article{carbonneau2008application,
  title={Application of machine learning techniques for supply chain demand forecasting},
  author={Carbonneau, Real and Laframboise, Kevin and Vahidov, Rustam},
  journal={European Journal of Operational Research},
  volume={184},
  number={3},
  pages={1140--1154},
  year={2008},
  publisher={Elsevier}
}

@article{kim2003financial,
  title={Financial time series forecasting using support vector machines},
  author={Kim, Kyoung-jae},
  journal={Neurocomputing},
  volume={55},
  number={1-2},
  pages={307--319},
  year={2003},
  publisher={Elsevier}
}

@inproceedings{laptev2017time,
  title={Time-series extreme event forecasting with neural networks at uber},
  author={Laptev, Nikolay and Yosinski, Jason and Li, Li Erran and Smyl, Slawek},
  booktitle={International conference on machine learning},
  volume={34},
  pages={1--5},
  year={2017}
}

@article{bai2018empirical,
  title={An empirical evaluation of generic convolutional and recurrent networks for sequence modeling},
  author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  journal={arXiv preprint arXiv:1803.01271},
  year={2018}
}

@article{makridakis2020m5,
  title={The M5 accuracy competition: Results, findings and conclusions},
  author={Makridakis, S and Spiliotis, E and Assimakopoulos, V},
  journal={Int J Forecast},
  year={2020}
}

@inproceedings{khurana2016cognito,
  title={Cognito: Automated feature engineering for supervised learning},
  author={Khurana, Udayan and Turaga, Deepak and Samulowitz, Horst and Parthasrathy, Srinivasan},
  booktitle={2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)},
  pages={1304--1307},
  year={2016},
  organization={IEEE}
}

@article{qiu2018multivariate,
  title={Multivariate Bayesian Structural Time Series Model.},
  author={Qiu, Jinwen and Jammalamadaka, S Rao and Ning, Ning},
  journal={J. Mach. Learn. Res.},
  volume={19},
  number={1},
  pages={2744--2776},
  year={2018}
}

@book{scott20154,
  title={4. Bayesian Variable Selection for Nowcasting Economic Time Series},
  author={Scott, Steven L and Varian, Hal R},
  year={2015},
  publisher={University of Chicago Press}
}

@inproceedings{parascandolo2018learning,
  title={Learning independent causal mechanisms},
  author={Parascandolo, Giambattista and Kilbertus, Niki and Rojas-Carulla, Mateo and Sch{\"o}lkopf, Bernhard},
  booktitle={International Conference on Machine Learning},
  pages={4036--4044},
  year={2018},
  organization={PMLR}
}

@book{peters2017elements,
  title={Elements of causal inference: foundations and learning algorithms},
  author={Peters, Jonas and Janzing, Dominik and Sch{\"o}lkopf, Bernhard},
  year={2017},
  publisher={The MIT Press}
}

@article{mitrovic2020representation,
  title={Representation learning via invariant causal mechanisms},
  author={Mitrovic, Jovana and McWilliams, Brian and Walker, Jacob and Buesing, Lars and Blundell, Charles},
  journal={arXiv preprint arXiv:2010.07922},
  year={2020}
}

@article{von2021self,
  title={Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style},
  author={von K{\"u}gelgen, Julius and Sharma, Yash and Gresele, Luigi and Brendel, Wieland and Sch{\"o}lkopf, Bernhard and Besserve, Michel and Locatello, Francesco},
  journal={arXiv preprint arXiv:2106.04619},
  year={2021}
}

@article{hyndman2008automatic,
  title={Automatic time series forecasting: the forecast package for R},
  author={Hyndman, Rob J and Khandakar, Yeasmin},
  journal={Journal of statistical software},
  volume={27},
  number={1},
  pages={1--22},
  year={2008}
}

@book{shumway2000time,
  title={Time series analysis and its applications},
  author={Shumway, Robert H and Stoffer, David S and Stoffer, David S},
  volume={3},
  year={2000},
  publisher={Springer}
}

@article{van2008visualizing,
  title={Visualizing data using t-SNE.},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={11},
  year={2008}
}

@article{de2011forecasting,
  title={Forecasting time series with complex seasonal patterns using exponential smoothing},
  author={De Livera, Alysha M and Hyndman, Rob J and Snyder, Ralph D},
  journal={Journal of the American statistical association},
  volume={106},
  number={496},
  pages={1513--1527},
  year={2011},
  publisher={Taylor \& Francis}
}

@article{cordeiro2009forecasting,
  title={Forecasting time series with BOOT. EXPOS procedure},
  author={Cordeiro, Clara and Neves, M},
  journal={REVSTAT-Statistical Journal},
  volume={7},
  number={2},
  pages={135--149},
  year={2009}
}

@inproceedings{Pearl2012TheDR,
  title={The Do-Calculus Revisited},
  author={Judea Pearl},
  booktitle={UAI},
  year={2012}
}

@misc{he2021fastmoe,
    title={FastMoE: A Fast Mixture-of-Expert Training System}, 
    author={Jiaao He and Jiezhong Qiu and Aohan Zeng and Zhilin Yang and Jidong Zhai and Jie Tang},
    year={2021},
    eprint={2103.13262},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}



######NIIIIIII 




@inproceedings{
Engelcke2020GENESIS:,
title={GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations},
author={Martin Engelcke and Adam R. Kosiorek and Oiwi Parker Jones and Ingmar Posner},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BkxfaTVFwH}
}

@inproceedings{
Lin2020SPACE:,
title={SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition},
author={Zhixuan Lin and Yi-Fu Wu and Skand Vishwanath Peri and Weihao Sun and Gautam Singh and Fei Deng and Jindong Jiang and Sungjin Ahn},
booktitle={International Conference on Learning Representations},
year={2020}
}

@inproceedings{
jia2023improving,
title={Improving Object-centric Learning with Query Optimization},
author={Baoxiong Jia and Yu Liu and Siyuan Huang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=_-FN9mJsgg}
}

@inproceedings{
seitzer2023bridging,
title={Bridging the Gap to Real-World Object-Centric Learning},
author={Maximilian Seitzer and Max Horn and Andrii Zadaianchuk and Dominik Zietlow and Tianjun Xiao and Carl-Johann Simon-Gabriel and Tong He and Zheng Zhang and Bernhard Sch{\"o}lkopf and Thomas Brox and Francesco Locatello},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023}
}

@article{vankovTrainingNeuralNetworks2020,
  title = {Training Neural Networks to Encode Symbols Enables Combinatorial Generalization},
  author = {Vankov, Ivan I. and Bowers, Jeffrey S.},
  year = {2020},
  month = feb,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {375},
  number = {1791},
  pages = {20190309},
  publisher = {{Royal Society}},
  doi = {10.1098/rstb.2019.0309},
  urldate = {2022-12-08},
  keywords = {combinatorial generalization,neural networks,symbols},
  file = {C:\Users\thadd\OneDrive\Dokumente\zotero\vankovTrainingNeuralNetworks2020.pdf}
}



@article{greff2020binding,
  title={On the binding problem in artificial neural networks},
  author={Greff, Klaus and Van Steenkiste, Sjoerd and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:2012.05208},
  year={2020}
}

@article{lake_ullman_tenenbaum_gershman_2017, title={Building machines that learn and think like people}, volume={40}, DOI={10.1017/S0140525X16001837}, journal={Behavioral and Brain Sciences}, publisher={Cambridge University Press}, author={Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.}, year={2017}, pages={e253}}



@inproceedings{lachapelle2021disentanglement,
 author = {Lachapelle, S{\'e}bastien and Rodriguez, Pau and Sharma, Yash and Everett, Katie E and Le Priol, R{\'e}mi and Lacoste, Alexandre and Lacoste-Julien, Simon},
 booktitle = {First Conference on Causal Learning and Reasoning},
 title = {Disentanglement via Mechanism Sparsity Regularization: A New Principle for Nonlinear ICA},
 year = {2021}
}



@inproceedings{gresele2020incomplete,
 author = {Luigi Gresele and
Paul K. Rubenstein and
Arash Mehrjou and
Francesco Locatello and
Bernhard Sch{\"{o}}lkopf},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/uai/GreseleRMLS19.bib},
 booktitle = {Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial
Intelligence, {UAI} 2019, Tel Aviv, Israel, July 22-25, 2019},
 pages = {217--227},
 series = {Proceedings of Machine Learning Research},
 title = {The Incomplete Rosetta Stone problem: Identifiability results for
Multi-view Nonlinear {ICA}},
 volume = {115},
 year = {2019}
}

@inproceedings{shu2019weakly,
 author = {Rui Shu and
Yining Chen and
Abhishek Kumar and
Stefano Ermon and
Ben Poole},
 booktitle = {{ICLR}},
 title = {Weakly Supervised Disentanglement with Guarantees},
 year = {2020}
}




@inproceedings{khemakhem2020ice,
 author = {Ilyes Khemakhem and
Ricardo Pio Monti and
Diederik P. Kingma and
Aapo Hyv{\"{a}}rinen},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/KhemakhemMKH20.bib},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
 title = {ICE-BeeM: Identifiable Conditional Energy-Based Deep Models Based
on Nonlinear {ICA}},
 year = {2020}
}


@article{Hyvrinen2023NonlinearIC,
  title={Nonlinear Independent Component Analysis for Principled Disentanglement in Unsupervised Deep Learning},
  author={Aapo Hyv{\"a}rinen and Ilyes Khemakhem and Hiroshi Morioka},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.16535},
}


@inproceedings{Brady2023ProvablyLO,
  title = 	 {Provably Learning Object-Centric Representations},
  author =       {Brady, Jack and Zimmermann, Roland S. and Sharma, Yash and Sch\"{o}lkopf, Bernhard and Von K\"{u}gelgen, Julius and Brendel, Wieland},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {3038--3062},
  year = 	 {2023},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR}
}


@inproceedings{Halva2021,
 author = {Hermanni H{\"{a}}lv{\"{a}} and
Sylvain Le Corff and
Luc Leh{\'{e}}ricy and
Jonathan So and
Yongjie Zhu and
Elisabeth Gassiat and
Aapo Hyv{\"{a}}rinen},
 booktitle = {NeurIPS},
 pages = {1624--1633},
 title = {Disentangling Identifiable Features from Noisy Data with Structured
Nonlinear {ICA}},
 year = {2021}
}



@article{
moran2021identifiable,
title={Identifiable Deep Generative Models via Sparse Decoding},
author={Gemma Elyse Moran and Dhanya Sridhar and Yixin Wang and David Blei},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
note={}
}


@inproceedings{
assouel2022objectcentric,
title={Object-centric Compositional Imagination for Visual Abstract Reasoning},
author={Rim Assouel and Pau Rodriguez and Perouz Taslakian and David Vazquez and Yoshua Bengio},
booktitle={ICLR2022 Workshop on the Elements of Reasoning: Objects, Structure and Causality},
year={2022}
}

@article{ellis2023dreamcoder,
  title={DreamCoder: growing generalizable, interpretable knowledge with wake--sleep Bayesian program learning},
  author={Ellis, Kevin and Wong, Lionel and Nye, Maxwell and Sable-Meyer, Mathias and Cary, Luc and Anaya Pozo, Lore and Hewitt, Luke and Solar-Lezama, Armando and Tenenbaum, Joshua B},
  journal={Philosophical Transactions of the Royal Society A},
  volume={381},
  number={2251},
  pages={20220050},
  year={2023},
  publisher={The Royal Society}
}

@article{cemgil2020autoencoding,
  title={The autoencoding variational autoencoder},
  author={Cemgil, Taylan and Ghaisas, Sumedh and Dvijotham, Krishnamurthy and Gowal, Sven and Kohli, Pushmeet},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15077--15087},
  year={2020}
}

@article{sinha2021consistency,
  title={Consistency regularization for variational auto-encoders},
  author={Sinha, Samarth and Dieng, Adji Bousso},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12943--12954},
  year={2021}
}

@inproceedings{
leeb2021assays,
title={Exploring the Latent Space of Autoencoders with Interventional Assays},
author={Felix Leeb and Stefan Bauer and Michel Besserve and Bernhard Sch{\"o}lkopf},
booktitle={Advances in Neural Information Processing Systems},
year={2022}
}

@article{lachapelle2023additive,
  title={Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation},
  author={Lachapelle, S{\'e}bastien and Mahajan, Divyat and Mitliagkas, Ioannis and Lacoste-Julien, Simon},
  journal={arXiv preprint arXiv:2307.02598},
  year={2023}
}

@inproceedings{zhao2022toward,
  title={Toward Compositional Generalization in Object-Oriented World Modeling},
  author={Zhao, Linfeng and Kong, Lingzhi and Walters, Robin and Wong, Lawson LS},
  booktitle={International Conference on Machine Learning},
  pages={26841--26864},
  year={2022},
  organization={PMLR}
}

@article{wiedemer2023compositional,
  title={Compositional Generalization from First Principles},
  author={Wiedemer, Thadd{\"a}us and Mayilvahanan, Prasanna and Bethge, Matthias and Brendel, Wieland},
  journal={arXiv preprint arXiv:2307.05596},
  year={2023}
}



@article{zhao2018bias,
  title={Bias and generalization in deep generative models: An empirical study},
  author={Zhao, Shengjia and Ren, Hongyu and Yuan, Arianna and Song, Jiaming and Goodman, Noah and Ermon, Stefano},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{Dittadi2021GeneralizationAR,
  title={Generalization and Robustness Implications in Object-Centric Learning},
  author={Andrea Dittadi and Samuele Papa and Michele De Vita and Bernhard Sch{\"o}lkopf and Ole Winther and Francesco Locatello},
  booktitle={International Conference on Machine Learning},
  year={2021}
}


@article{Liang2023CausalCA,
  title={Causal Component Analysis},
  author={Wendong Liang and Armin Keki'c and Julius von K{\"u}gelgen and Simon Buchholz and Michel Besserve and Luigi Gresele and Bernhard Sch{\"o}lkopf},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.17225},
}

@inproceedings{
horan2021when,
title={When Is Unsupervised Disentanglement Possible?},
author={Daniella Horan and Eitan Richardson and Yair Weiss},
booktitle={Advances in Neural Information Processing Systems},
year={2021}
}



@inproceedings{Buchholz2022FunctionCF,
 author = {Simon Buchholz and
Michel Besserve and
Bernhard Sch{\"{o}}lkopf},
 booktitle = {NeurIPS},
 title = {Function Classes for Identifiable Nonlinear Independent Component
Analysis},
 year = {2022}
}

@article{rezende2018taming,
  title={Taming vaes},
  author={Rezende, Danilo Jimenez and Viola, Fabio},
  journal={arXiv preprint arXiv:1810.00597},
  year={2018}
}

@inproceedings{Zheng2022OnTI,
 author = {Yujia Zheng and
Ignavier Ng and
Kun Zhang},
 booktitle = {NeurIPS},
 title = {On the Identifiability of Nonlinear {ICA:} Sparsity and Beyond},
 year = {2022}
}


@inproceedings{trauble2021disentangled,
  title={On disentangled representations learned from correlated data},
  author={Tr{\"a}uble, Frederik and Creager, Elliot and Kilbertus, Niki and Locatello, Francesco and Dittadi, Andrea and Goyal, Anirudh and Sch{\"o}lkopf, Bernhard and Bauer, Stefan},
  booktitle={International Conference on Machine Learning},
  pages={10401--10412},
  year={2021},
  organization={PMLR}
}


@inproceedings{montero022lost,
 author = {Montero, Milton and Bowers, Jeffrey and Ponte Costa, Rui  and Ludwig, Casimir and Malhotra, Gaurav},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {10136--10149},
 publisher = {Curran Associates, Inc.},
 title = {Lost in Latent Space: Examining failures of disentangled models at combinatorial generalisation},
 volume = {35},
 year = {2022}
}

@inproceedings{
schott2022visual,
title={Visual Representation Learning Does Not Generalize Strongly Within the Same Domain},
author={Lukas Schott and Julius Von K{\"u}gelgen and Frederik Tr{\"a}uble and Peter Vincent Gehler and Chris Russell and Matthias Bethge and Bernhard Sch{\"o}lkopf and Francesco Locatello and Wieland Brendel},
booktitle={International Conference on Learning Representations},
year={2022}
}

@inproceedings{
montero2021the,
title={The role of Disentanglement in Generalisation},
author={Milton Llera Montero and Casimir JH Ludwig and Rui Ponte Costa and Gaurav Malhotra and Jeffrey Bowers},
booktitle={International Conference on Learning Representations},
year={2021}
}

@article{KurthNelson2022ReplayAC,
  title={Replay and compositional computation},
  author={Zeb Kurth-Nelson and Timothy Edward John Behrens and Greg Wayne and Kevin J. Miller and Lennart Luettgau and Raymond Dolan and Yunzhe Liu and Philipp Schwartenbeck},
  journal={Neuron},
  year={2022},
  volume={111},
  pages={454-469}
}

@article {Bakermans2023,
	author = {Jacob J.W. Bakermans and Joseph Warren and James C.R. Whittington and Timothy E.J. Behrens},
	title = {Constructing future behaviour in the hippocampal formation through composition and replay},
	elocation-id = {2023.04.07.536053},
	year = {2023},
	doi = {10.1101/2023.04.07.536053},
	publisher = {Cold Spring Harbor Laboratory},
	eprint = {https://www.biorxiv.org/content/early/2023/04/07/2023.04.07.536053.full.pdf},
	journal = {bioRxiv}
}

@article {Schwartenbeck2021.06.06.447249,
	author = {Philipp Schwartenbeck and Alon Baram and Yunzhe Liu and Shirley Mark and Timothy Muller and Raymond Dolan and Matthew Botvinick and Zeb Kurth-Nelson and Timothy Behrens},
	title = {Generative replay for compositional visual understanding in the prefrontal-hippocampal circuit},
	elocation-id = {2021.06.06.447249},
	year = {2021},
	doi = {10.1101/2021.06.06.447249},
	publisher = {Cold Spring Harbor Laboratory},
	eprint = {https://www.biorxiv.org/content/early/2021/06/06/2021.06.06.447249.full.pdf},
	journal = {bioRxiv}
}

@misc{multiobjectdatasets19,
  title={Multi-Object Datasets},
  author={Kabra, Rishabh and Burgess, Chris and Matthey, Loic and
          Kaufman, Raphael Lopez and Greff, Klaus and Reynolds, Malcolm and
          Lerchner, Alexander},
  howpublished={https://github.com/deepmind/multi-object-datasets/},
  year={2019}
}

@misc{kuhnle2017shapeworld,
      title={ShapeWorld - A new test methodology for multimodal language understanding}, 
      author={Alexander Kuhnle and Ann Copestake},
      year={2017},
      eprint={1704.04517},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{greff2019multi,
 author = {Klaus Greff and
Rapha{\"{e}}l Lopez Kaufman and
Rishabh Kabra and
Nick Watters and
Chris Burgess and
Daniel Zoran and
Loic Matthey and
Matthew M. Botvinick and
Alexander Lerchner},
 booktitle = {{ICML}},
 pages = {2424--2433},
 series = {Proceedings of Machine Learning Research},
 title = {Multi-Object Representation Learning with Iterative Variational Inference},
 volume = {97},
 year = {2019}
}

@inproceedings{
singh2021illiterate,
title={Illiterate {DALL}-E Learns to Compose},
author={Gautam Singh and Fei Deng and Sungjin Ahn},
booktitle={International Conference on Learning Representations},
year={2022}
}

@article{elsayed2022savi++,
  title={Savi++: Towards end-to-end object-centric learning from real-world videos},
  author={Elsayed, Gamaleldin and Mahendran, Aravindh and van Steenkiste, Sjoerd and Greff, Klaus and Mozer, Michael C and Kipf, Thomas},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={28940--28954},
  year={2022}
}

@article{fradyLearningGeneralizationCompositional2023,
  title={Learning and generalization of compositional representations of visual scenes},
  author={Frady, E Paxon and Kent, Spencer and Tran, Quinn and Kanerva, Pentti and Olshausen, Bruno A and Sommer, Friedrich T},
  journal={arXiv preprint arXiv:2303.13691},
  year={2023}
}

@inproceedings{dongFirstStepsUnderstanding2022,
  title = {First {{Steps Toward Understanding}} the {{Extrapolation}} of {{Nonlinear Models}} to {{Unseen Domains}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Dong, Kefan and Ma, Tengyu},
  year = {2022},
  month = sep,
  urldate = {2023-08-30},
  langid = {english}
}

@inproceedings{kipf2019contrastive,
title = {Contrastive Learning of Structured World Models},
author = {Thomas Kipf and Elise van der Pol and Max Welling},
booktitle = {International Conference on Learning Representations},
year = {2020}
}

@article{madanWhenHowCNNs2021,
  title={When and how CNNs generalize to out-of-distribution category-viewpoint combinations},
  author={Madan, Spandan and Henry, Timothy and Dozier, Jamell and Ho, Helen and Bhandari, Nishchal and Sasaki, Tomotake and Durand, Fr{\'e}do and Pfister, Hanspeter and Boix, Xavier},
  journal={arXiv preprint arXiv:2007.08032},
  year={2020}
}

@misc{burgess2018understanding,
      title={Understanding disentangling in $\beta$-VAE}, 
      author={Christopher P. Burgess and Irina Higgins and Arka Pal and Loic Matthey and Nick Watters and Guillaume Desjardins and Alexander Lerchner},
      year={2018},
      eprint={1804.03599},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{burgessMONetUnsupervisedScene2019,
  title = {{{MONet}}: {{Unsupervised Scene Decomposition}} and {{Representation}}},
  shorttitle = {{{MONet}}},
  author = {Burgess, Christopher P. and Matthey, Loic and Watters, Nicholas and Kabra, Rishabh and Higgins, Irina and Botvinick, Matt and Lerchner, Alexander},
  year = {2019},
  month = jan,
  number = {arXiv:1901.11390},
  eprint = {1901.11390},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1901.11390},
  urldate = {2023-05-16},
  archiveprefix = {arxiv},
}

@misc{spriteworld19,
    author = {Nicholas Watters and Loic Matthey and Sebastian Borgeaud and Rishabh Kabra and Alexander Lerchner},
    title = {Spriteworld: A Flexible, Configurable Reinforcement Learning Environment},
    howpublished = {https://github.com/deepmind/spriteworld/},
    year={2019}
}

@inproceedings{
loshchilov2019decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019}
}

@article{Tenenbaum2011HowTG,
 author = {Joshua B. Tenenbaum and Charles Kemp and Thomas L. Griffiths and Noah D. Goodman},
 journal = {Science},
 pages = {1279 - 1285},
 title = {How to Grow a Mind: Statistics, Structure, and Abstraction},
 volume = {331},
 year = {2011}
}


@article{goyal2020inductive,
  title={Inductive biases for deep learning of higher-level cognition},
  author={Goyal, Anirudh and Bengio, Yoshua},
  journal={Proceedings of the Royal Society A},
  volume={478},
  number={2266},
  pages={20210068},
  year={2022},
  publisher={The Royal Society}
}

@article{BEHRENS2018490,
title = {What Is a Cognitive Map? Organizing Knowledge for Flexible Behavior},
journal = {Neuron},
volume = {100},
number = {2},
pages = {490-509},
year = {2018},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2018.10.002},
author = {Timothy E.J. Behrens and Timothy H. Muller and James C.R. Whittington and Shirley Mark and Alon B. Baram and Kimberly L. Stachenfeld and Zeb Kurth-Nelson}}

@article{Battaglia2018RelationalIB,
 author = {Peter W. Battaglia and Jessica B. Hamrick and Victor Bapst and Alvaro Sanchez-Gonzalez and Vin{\'i}cius Flores Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Çaglar G{\"u}lçehre and H. Francis Song and Andrew J. Ballard and Justin Gilmer and George E. Dahl and Ashish Vaswani and Kelsey R. Allen and Charlie Nash and Victoria Langston and Chris Dyer and Nicolas Manfred Otto Heess and Daan Wierstra and Pushmeet Kohli and Matthew M. Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},
 journal = {ArXiv},
 title = {Relational inductive biases, deep learning, and graph networks},
 volume = {abs/1806.01261},
 year = {2018}
}

@article{Fodor1988ConnectionismAC,
  title={Connectionism and cognitive architecture: A critical analysis},
  author={Jerry A. Fodor and Zenon W. Pylyshyn},
  journal={Cognition},
  year={1988},
  volume={28},
  pages={3-71}
}

@article{yuan2023compositional,
  title={Compositional Scene Representation Learning via Reconstruction: A Survey},
  author={Yuan, Jinyang and Chen, Tonglin and Li, Bin and Xue, Xiangyang},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2023},
  publisher={IEEE}
}

@inproceedings{Paszke2019PyTorchAI,
 author = {Adam Paszke and
Sam Gross and
Francisco Massa and
Adam Lerer and
James Bradbury and
Gregory Chanan and
Trevor Killeen and
Zeming Lin and
Natalia Gimelshein and
Luca Antiga and
Alban Desmaison and
Andreas K{\"{o}}pf and
Edward Z. Yang and
Zachary DeVito and
Martin Raison and
Alykhan Tejani and
Sasank Chilamkurthy and
Benoit Steiner and
Lu Fang and
Junjie Bai and
Soumith Chintala},
 booktitle = {NeurIPS},
 pages = {8024--8035},
 title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
 year = {2019}
}